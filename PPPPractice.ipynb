{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86daf8f0-8404-4aff-8921-edc959667000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write a PySpark script to find the details of students whose first names end with the letter ‘h’ and have exactly six alphabets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b4516fb-b93a-44d9-8617-2f15cba1dc44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(1, 'Ramesh', 'Babu', 3.3, '2024-07-01', 'Commerce'),\n",
    "        (2, 'Sharath', 'Kumar', 3.9, '2022-07-01', 'AI'),\n",
    "        (3, 'Venkatesh', 'Raj', 3.2, '2023-07-20', 'MBBS'),\n",
    "        (4, 'Joseph', 'Brown', 3.7, '2023-07-01', 'Aerospace'),\n",
    "        (5, 'Sourabh', 'saxena', 3.4, '2022-06-15', 'Engineering'),\n",
    "        (6, 'Leah', 'Smith', 3.7, '2021-07-22', 'Masters'),\n",
    "        (7, 'Prakash', 'Raja', 3.8, '2022-07-05', 'Scientist'),        \n",
    "        (8, 'Rakesh', 'Kumar', 3.9, '2020-07-01', 'Data Scientist'),       \n",
    "        (9, 'Kamalesh', 'Rao', 3.6, '2023-07-01', 'Social'),\n",
    "        (10, 'Kamath', 'mohan', 3.7, '2023-07-01', 'Science'),\n",
    "        (11, 'Naagesh', 'Babu', 3.5, '2024-07-01', 'Commerce'),\n",
    "        (12, 'Praharsh', 'Mohan', 3.4, '2023-07-01', 'Science'),\n",
    "        (13, 'Danush', 'Vedanth', 3.3, '2021-07-01', 'Social'),\n",
    "        (14, 'Kanniah', 'kanth', 3.2, '2023-07-01', 'Masters')\n",
    "       ]\n",
    "\n",
    "# Define schema\n",
    "columns = [\"student_id\", \"first_name\", \"last_name\", \"gpa\", \"enrollment_date\", \"major\"]\n",
    "\n",
    "# Create DataFrame\n",
    "students_df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Show DataFrame\n",
    "display(students_df)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f0e15e7-8db7-4cd2-8eb8-a55d507be960",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "st_df = students_df.filter((col(\"first_name\").endswith(\"h\")) & (length(col(\"first_name\")) == 6))\n",
    "display(st_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18afc94b-aa68-4865-bcc9-5c9d1c175bb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "st_df = students_df.filter((col('first_name').rlike(r'^[a-zA-Z]{6}h$'))).orderBy(col('first_name'))\n",
    "display(st_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75e83bc7-dbca-4ef3-af17-25e06d543c6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_col = students_df.select(students_df.columns[:3])\n",
    "df_bal = students_df.select(students_df.columns[3:6])\n",
    "display(df_col)\n",
    "display(df_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "851a754a-c669-4946-bf16-9a7441f9506b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "     (\"xyz\", \"blank\", \"1234\", None),\n",
    "     (None, \"abs@gmail.com\", \"blank\", \"blank\"),\n",
    "     (\"abc\", None, None, \"india\"),\n",
    "     (\"def\", \"xyz@gmail.com\", \"5678\", \"sweden\"),\n",
    "     (None, None, None, None)\n",
    "]\n",
    "\n",
    " # Define schema\n",
    "schema = [\"name\", \"email\", \"phoneNo\", \"country\"]\n",
    "\n",
    " # Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f98baf8-307b-4961-9bb2-7dc7f263a910",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from functools import reduce\n",
    "\n",
    "def nullval(*cols):\n",
    "    return reduce(lambda x,y: when(x.isNull(), y).otherwise(x), cols)\n",
    "\n",
    "df_null = df.select([nullval(col(c)).alias(c) for c in df.columns])\n",
    "display(df_null)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def filnull(*args):\n",
    "    fil_val = filter(lambda x: x is not None, args)\n",
    "    return reduce(lambda x,y: when(x.isNull(), y).otherwise(x), fil_val)\n",
    "\n",
    "df_null = df.select([filnull(col(c)).alias(c) for c in df.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d577f947-51db-4d29-bd4e-5c1c56fdfa00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = [\"Name\", \"SkillSet\"]\n",
    "    \n",
    "data = ([\"ABC\", ['.Net', 'Git', 'C#']],\n",
    "        [\"XYZ\", ['Wordpress', 'PHP']],\n",
    "        [\"IJK\", ['Python', 'MongoDB', 'Git']],\n",
    "        [\"DEF\", ['SSIS', 'SSAS', 'Power BI', 'SQL Server', 'Data Warehouse']],\n",
    "        [\"PQR\", ['Azure']])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a691bb68-405e-473a-a296-f9964d43b359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    df = df.withColumn(f\"skill_{i}\", get(col(\"SkillSet\"),i))\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba71b7c6-c5c5-4cd3-801e-bfed37ba64f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "df_split = df.withColumn(\"skill1\", col(\"skillset\")[0])\\\n",
    "    .withColumn(\"skill2\", (col(\"skillset\")[1])\\\n",
    "    .withColumn(\"skill3\", col(\"skillset\")[2])\\\n",
    "    .withColumn(\"skill4\", col(\"skillset\")[3])\n",
    "    #.withColumn(\"skill5\", col(\"skillset\")[4])\n",
    "\n",
    "display(df_split)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e31afef-ef11-47cb-9931-e95ab27a896b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bda414e4-21a3-408d-b0bd-9751d01bba6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "d21 = df.withColumn(\"Skill1\", df['SkillSet'][0]) \\\n",
    "        .withColumn(\"Skill2\", df['SkillSet'][1]) \\\n",
    "        .withColumn(\"Skill3\", df['SkillSet'][2]) \\\n",
    "        .withColumn(\"Skill4\", df['SkillSet'][3]) \\\n",
    "        .withColumn(\"Skill5\", df['SkillSet'][4])\n",
    "display(d21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b4c9f7c-f87f-47a2-a984-9ce3a27b8609",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import get, col\n",
    "\n",
    "d21 = (\n",
    "    df.withColumn(\"Skill1\", get(col(\"SkillSet\"), 0))\n",
    "      .withColumn(\"Skill2\", get(col(\"SkillSet\"), 1))\n",
    "      .withColumn(\"Skill3\", get(col(\"SkillSet\"), 2))\n",
    "      .withColumn(\"Skill4\", get(col(\"SkillSet\"), 3))\n",
    "      .withColumn(\"Skill5\", get(col(\"SkillSet\"), 4))\n",
    ")\n",
    "display(d21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c9902e5-d28a-423d-8266-38071ee69b21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    df_split = df_split.withColumn(f\"skill_{i}\", df_split['skillset[i]'])\n",
    "display(df_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19a80a7d-9ce3-4605-bc42-ddf28ee8b29a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "  df_split = df.withColumn(f\"skill_{i}\", get(df[SkillSet], i))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbbffc91-c0ca-45d6-b6ef-92ebc2a59ff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import get, col\n",
    "\n",
    "for i in range(5):\n",
    "    df = df.withColumn(\n",
    "        f\"skill_{i}\",\n",
    "        get(col(\"SkillSet\"), i)\n",
    "    )\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f193dbd1-b2b2-45d3-ae7a-15ea50a701ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Sample Data\n",
    "data = [\n",
    "    (1, \"ITC\", 59000, \"2024-01-15\"),\n",
    "    (2, \"BEML\", 68000, \"2023-12-10\"),\n",
    "    (3, \"HCL\", 53500, \"2022-06-25\"),\n",
    "    (4, \"AIRTEL\", 77800, \"2021-09-30\"),\n",
    "    (5, \"ACT\", 5550, \"2024-05-15\"),\n",
    "    (6, \"TATA\", 95600, \"2023-09-15\"),\n",
    "    (7, \"BEML\", 87500, \"2025-02-05\"),\n",
    "    (8, \"AIRTEL\", 95600, \"2021-06-20\"),\n",
    "    (9, \"ACT\", 65000, \"2024-02-04\"),\n",
    "    (10, \"ITC\", 36700, \"2022-09-08\"),\n",
    "    (11, \"TATA\", 175600, \"2023-06-15\"),\n",
    "    (12, \"ITC\", 98700, \"2022-12-18\"),\n",
    "    (13, \"BEML\", 99550, \"2023-01-22\"),\n",
    "    (14, \"AIRTEL\", 395800, \"2020-02-23\")\n",
    "]\n",
    "\n",
    "# Define Schema\n",
    "columns = [\"ID\", \"Company\", \"Salary\", \"JoiningDate\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = df.withColumn(\"JoiningDate\", col(\"JoiningDate\").cast(DateType()))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4563961-d9df-4af4-8066-5823089af2c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_max = df.agg(max(\"Salary\"))\n",
    "df_max.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c166fa8-6b78-45fa-a4dc-fbc1de0f6825",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "df_max = df.agg(\n",
    "    max(\"Salary\").alias(\"max_salary\")\n",
    ")\n",
    "display(df_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ba13250-ec67-446d-b4a4-a65565f8052a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data = [(\"Adarsh\", \"Chennai\", \"Cochin\", \"Hyderabad\"),\n",
    "        (\"Akash\", \"Coimbatore\", \"Mumbai\", \"Chennai\"),\n",
    "        (\"Senthil\", \"Salem\", \"Bangalore\", None),\n",
    "        (\"Kalyan\", \"Delhi\", \"Bangalore\", \"Noida\"),\n",
    "        (\"Sohile\", \"Mumbai\", \"Pune\", \"Cochin\"),\n",
    "        (\"Gouthami\", \"Chennai\", \"Mumbai\", None),\n",
    "        (\"Hemanth\", \"Delhi\", \"Noida\", \"Kolkata\")\n",
    "        ]\n",
    " \n",
    "columns = [\"Name\", \"pLocation\", \"sLocation\", \"aLocation\"]\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "df.printSchema()\n",
    "display(df)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeb4ad1e-cbea-4e0c-86e4-9bcf692689a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, array, array_contains, size, element_at\n",
    "from pyspark.sql.types import IntegerType, StringType, ArrayType, StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85585a18-b1cb-4b70-9948-c40c578e72c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, size\n",
    "\n",
    "df_arr = df.select(\n",
    "    \"Name\",\n",
    "    array(\"pLocation\", \"sLocation\", \"aLocation\").alias(\"alllocation\")\n",
    ").withColumn(\n",
    "    \"Size\",\n",
    "    size(\"alllocation\")\n",
    ")\n",
    "display(df_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e1db9fc-6f8b-4276-b7b1-322f4d07e184",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_arr = df.select(\"Name\", array(\"pLocation\", \"sLocation\", \"aLocation\").alias(\"alllocation\").withColumn(\"Size\", size(\"alllocaton\")))\n",
    "display(df_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eb2684a-afa1-4759-885f-ac5a3f313e9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%sql\n",
    "SELECT filter(array(1, 2, 3, 5, 7, 8, 9), x -> x % 2 == 1) AS LAMDA;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bbab244-d029-4dd5-8dff-7bc55eab5761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [(1, \"Anand\"),\n",
    "        (2, \"Baskar\"),\n",
    "        (3, \"Catherin\"),\n",
    "        (4, \"Dravid\"),\n",
    "        (5, \"Swetha\"),\n",
    "        (6, \"Akash\"),\n",
    "        (7, \"Senthil\"),\n",
    "        (8, \"Praveen\")]\n",
    "columns = [\"ID\", \"Name\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdf78cd5-46c5-4246-bef7-b5dff324b12b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import random\n",
    "\n",
    "prod = ['TV', 'AC', 'Washing Machine', 'Refrigerator']\n",
    "\n",
    "price =[10000, 20000, 30000]\n",
    "\n",
    "@udf(StringType())\n",
    "def cre_df():\n",
    "    return random.choice(prod)\n",
    "\n",
    "@udf(IntegerType())\n",
    "def cre_price():\n",
    "    return random.choice(price)\n",
    "\n",
    "df_fin = df.withColumn(\"Product\", cre_df())\\\n",
    "    .withColumn(\"Price\", cre_price())\n",
    "\n",
    "display(df_fin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07e8bb3d-149e-437e-94db-7f8905edd1f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"department_id\", StringType(), True),\n",
    "    StructField(\"product_volume\", DoubleType(), True),\n",
    "    StructField(\"Salary\", IntegerType(), True),\n",
    "    StructField(\"start_time\", DateType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"profile_id\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"index\", BooleanType(), True),\n",
    "    StructField(\"target\", StringType(), True),\n",
    "    StructField(\"impact\", IntegerType(), True),\n",
    "    StructField(\"impact1\", IntegerType(), True),\n",
    "    StructField(\"impact2\", StringType(), True),\n",
    "    StructField(\"start_date\", LongType(), True),\n",
    "    StructField(\"end_date\", LongType(), True),\n",
    "    StructField(\"update_date\", LongType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"Age\": random.randint(1, 100),  # Random integer for Age\n",
    "        \"department_id\": ''.join(random.choices(string.ascii_uppercase + string.digits, k=10)),  # Random alphanumeric string\n",
    "        \"product_volume\": random.uniform(1, 100),  # Random float for product volume\n",
    "        \"Salary\": random.randint(200, 500),  # Random integer for salary\n",
    "        \"start_time\": datetime.date.today(),  # Today's date\n",
    "        \"source\": f\"RESTAPI_{i}\",  # Custom string with index\n",
    "        \"profile_id\": ''.join(random.choices(string.ascii_uppercase + string.digits, k=10)),  # Random alphanumeric string\n",
    "        \"category\": random.choice([\"standard\", \"upper\", \"medium\", \"premium\", \"lower\"]),  # Random choice from a list\n",
    "        \"index\": random.choice([True, False]),  # Boolean value\n",
    "        \"target\": \"Azure\",  # Static value\n",
    "        \"impact\": random.choice([0]), # integer column has all rows with \"0\"\n",
    "        \"impact1\": random.choice([1]), # integer column has all rows with \"1\"\n",
    "        \"impact2\": random.choice([\"\"]), # column has empty values\n",
    "        \"start_date\": int((datetime.datetime.now() - datetime.timedelta(days=random.randint(0, 365))).timestamp()),  # LongType\n",
    "        \"end_date\": int((datetime.datetime.now() - datetime.timedelta(days=random.randint(0, 365))).timestamp()),  # LongType\n",
    "        \"update_date\": int((datetime.datetime.now() - datetime.timedelta(days=random.randint(0, 365))).timestamp()),  # LongType\n",
    "    }\n",
    "    for i in range(20)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d537e28c-9a9b-4637-838f-388ae9e7216b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "import datetime\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, IntegerType, StringType, DoubleType,\n",
    "    DateType, BooleanType, LongType\n",
    ")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"department_id\", StringType(), True),\n",
    "    StructField(\"product_volume\", DoubleType(), True),\n",
    "    StructField(\"Salary\", IntegerType(), True),\n",
    "    StructField(\"start_time\", DateType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"profile_id\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"index\", BooleanType(), True),\n",
    "    StructField(\"target\", StringType(), True),\n",
    "    StructField(\"impact\", IntegerType(), True),\n",
    "    StructField(\"impact1\", IntegerType(), True),\n",
    "    StructField(\"impact2\", StringType(), True),\n",
    "    StructField(\"start_date\", LongType(), True),\n",
    "    StructField(\"end_date\", LongType(), True),\n",
    "    StructField(\"update_date\", LongType(), True)\n",
    "])\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"Age\": random.randint(1, 100),\n",
    "        \"department_id\": ''.join(random.choices(string.ascii_uppercase + string.digits, k=10)),\n",
    "        \"product_volume\": random.uniform(1, 100),\n",
    "        \"Salary\": random.randint(200, 500),\n",
    "        \"start_time\": datetime.date.today(),\n",
    "        \"source\": f\"RESTAPI_{i}\",\n",
    "        \"profile_id\": ''.join(random.choices(string.ascii_uppercase + string.digits, k=10)),\n",
    "        \"category\": random.choice([\"standard\", \"upper\", \"medium\", \"premium\", \"lower\"]),\n",
    "        \"index\": random.choice([True, False]),\n",
    "        \"target\": \"Azure\",\n",
    "        \"impact\": random.choice([0]),\n",
    "        \"impact1\": random.choice([1]),\n",
    "        \"impact2\": random.choice([\"\"]),\n",
    "        \"start_date\": int((datetime.datetime.now() - datetime.timedelta(days=random.randint(0, 365))).timestamp()),\n",
    "        \"end_date\": int((datetime.datetime.now() - datetime.timedelta(days=random.randint(0, 365))).timestamp()),\n",
    "        \"update_date\": int((datetime.datetime.now() - datetime.timedelta(days=random.randint(0, 365))).timestamp()),\n",
    "    }\n",
    "    for i in range(20)\n",
    "]\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddd71a22-e5ae-4a4f-8a4d-307a35217f22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data = [15, 25, 36, 44, 57, 65, 89, 95, 9]\n",
    "\n",
    "df4 = spark.createDataFrame(([(x,) for x in data]), [\"Numbers\"])\n",
    "display(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26a41508-c45e-421e-82ec-3c527404fccd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "root_files = dbutils.fs.ls(\"/\")\n",
    "for file in root_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db123299-0a9b-43bd-b954-0776c97f3b2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "687219e4-570d-4d6f-8581-f4deab7b7d6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x = 10\n",
    "multiply = lambda x : x*2\n",
    "print(multiply(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a4c5a64-bf91-44b5-989c-7e14ffff6f8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5050494734172901,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "PPPPractice",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
