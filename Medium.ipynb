{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a6b6175-bd2c-4ca4-8e0f-22c48d2ec68e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"PySpark_Mastery\").getOrCreate()\n",
    "\n",
    "# Employee Dataset\n",
    "employee_data = [\n",
    "    (1, \"John\", \"Doe\", 28, \"Engineering\", 75000, \"2020-01-15\", [\"Python\", \"Spark\", \"SQL\"], \"john.doe@company.com\", \"New York\"),\n",
    "    (2, \"Jane\", \"Smith\", 32, \"Marketing\", 65000, \"2019-03-20\", [\"Excel\", \"PowerBI\", \"SQL\"], \"jane.smith@company.com\", \"Los Angeles\"),\n",
    "    (3, \"Mike\", \"Johnson\", 45, \"Engineering\", 95000, \"2018-07-10\", [\"Java\", \"Scala\", \"Spark\"], \"mike.johnson@company.com\", \"New York\"),\n",
    "    (4, \"Sarah\", \"Wilson\", 29, \"Sales\", 55000, \"2021-02-28\", [\"Salesforce\", \"Excel\"], \"sarah.wilson@company.com\", \"Chicago\"),\n",
    "    (5, \"David\", \"Brown\", 35, \"Engineering\", 85000, \"2019-11-05\", [\"Python\", \"AWS\", \"Docker\"], \"david.brown@company.com\", \"Seattle\"),\n",
    "    (6, \"Lisa\", \"Davis\", 27, \"Marketing\", 60000, \"2020-09-12\", [\"Google Ads\", \"Analytics\"], None, \"Los Angeles\"),\n",
    "    (7, \"Tom\", \"Miller\", 41, \"Sales\", 70000, \"2017-05-18\", [\"CRM\", \"Excel\"], \"tom.miller@company.com\", \"Chicago\"),\n",
    "    (8, \"Anna\", \"Garcia\", 33, \"Engineering\", 80000, \"2020-12-01\", [\"React\", \"Node.js\", \"MongoDB\"], \"anna.garcia@company.com\", \"Austin\")\n",
    "]\n",
    "\n",
    "employee_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"hire_date\", StringType(), True),\n",
    "    StructField(\"skills\", ArrayType(StringType()), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_employees = spark.createDataFrame(employee_data, employee_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "223c65ee-7e2c-4db4-a665-cd9a8a6b9775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sales Dataset\n",
    "sales_data = [\n",
    "    (101, 1, \"2023-01-15\", \"Electronics\", \"Laptop\", 1200, 1, \"Online\"),\n",
    "    (102, 2, \"2023-01-16\", \"Clothing\", \"T-Shirt\", 25, 3, \"Store\"),\n",
    "    (103, 1, \"2023-01-17\", \"Electronics\", \"Mouse\", 30, 2, \"Online\"),\n",
    "    (104, 3, \"2023-01-18\", \"Books\", \"Python Guide\", 45, 1, \"Online\"),\n",
    "    (105, 4, \"2023-01-19\", \"Electronics\", \"Keyboard\", 80, 1, \"Store\"),\n",
    "    (106, 2, \"2023-01-20\", \"Clothing\", \"Jeans\", 60, 2, \"Online\"),\n",
    "    (107, 5, \"2023-01-21\", \"Books\", \"Data Science\", 55, 1, \"Online\"),\n",
    "    (108, 1, \"2023-01-22\", \"Electronics\", \"Monitor\", 300, 1, \"Store\")\n",
    "]\n",
    "\n",
    "sales_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"price\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"channel\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_sales = spark.createDataFrame(sales_data, sales_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c78927a-677c-472d-825d-facc23313dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Department Dataset\n",
    "dept_data = [\n",
    "    (\"Engineering\", \"Tech Tower\", \"Alice Johnson\", 50),\n",
    "    (\"Marketing\", \"Business Center\", \"Bob Smith\", 25),\n",
    "    (\"Sales\", \"Sales Plaza\", \"Carol White\", 30),\n",
    "    (\"HR\", \"Admin Building\", \"David Lee\", 15)\n",
    "]\n",
    "\n",
    "dept_schema = StructType([\n",
    "    StructField(\"dept_name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"manager\", StringType(), True),\n",
    "    StructField(\"budget\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_departments = spark.createDataFrame(dept_data, dept_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aae7557b-be33-4c1d-9d7e-a079dd67291e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_employees_enhanced = df_employees.withColumn(\n",
    "    \"full_name\", \n",
    "    concat(col(\"first_name\"), lit(\" \"), col(\"last_name\"))\n",
    ")\n",
    "\n",
    "# Calculate years of experience (assuming current year is 2024)\n",
    "# df_employees_enhanced = df_employees_enhanced.withColumn(\n",
    "#     \"years_experience\",\n",
    "#     year(lit(\"2024-01-01\")) - year(to_date(col(\"hire_date\"), \"yyyy-MM-dd\"))\n",
    "# )\n",
    "# df_employees_enhanced.display()\n",
    "\n",
    "# Calculate years of experience (assuming current year is 2024)\n",
    "df_employees_enhanced = df_employees_enhanced.withColumn(\n",
    "    \"years_experience\",\n",
    "    year(current_date()) - year(to_date(col(\"hire_date\"), \"yyyy-MM-dd\"))\n",
    ")\n",
    "\n",
    "df_employees_enhanced.display()\n",
    "\n",
    "\n",
    "# Create salary categories\n",
    "df_employees_enhanced = df_employees_enhanced.withColumn(\n",
    "    \"salary_category\",\n",
    "    when(col(\"salary\") >= 80000, \"High\")\n",
    "    .when(col(\"salary\") >= 60000, \"Medium\")\n",
    "    .otherwise(\"Entry Level\")\n",
    ")\n",
    "\n",
    "df_employees_enhanced.select(\"full_name\", \"years_experience\", \"salary_category\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d25ab723-962d-46bd-a4ed-e0278860523e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "completely_unique = df_employees.dropDuplicates()\n",
    "print(f\"Original count: {df_employees.count()}\")\n",
    "print(f\"After removing duplicates: {completely_unique.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1591a3c3-79d3-4a3f-9625-cb9225212cc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employees_no_email = df_employees.filter(col(\"email\").isNull())\n",
    "print(f\"Employees without email: {employees_no_email.count()}\")\n",
    "\n",
    "# Find complete records (no nulls in critical fields)\n",
    "complete_records = df_employees.filter(\n",
    "    col(\"email\").isNotNull() & \n",
    "    col(\"salary\").isNotNull() & \n",
    "    col(\"department\").isNotNull()\n",
    ")\n",
    "complete_records.display()\n",
    "# Data quality report\n",
    "total_records = df_employees.count()\n",
    "null_emails = df_employees.filter(col(\"email\").isNull()).count()\n",
    "print(f\"Data Quality Report:\")\n",
    "print(f\"Total Records: {total_records}\")\n",
    "print(f\"Missing Emails: {null_emails} ({null_emails/total_records*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47f686cb-d357-4673-a8a7-2c48901a188e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Department-wise salary statistics\n",
    "dept_stats = df_employees.groupBy(\"department\").agg(\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    max(\"salary\").alias(\"max_salary\"),\n",
    "    min(\"salary\").alias(\"min_salary\"),\n",
    "    count(\"*\").alias(\"employee_count\"),\n",
    "    stddev(\"salary\").alias(\"salary_stddev\")\n",
    ")\n",
    "dept_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d52f693-f823-47a6-92fe-533c192e104f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate standard deviation of salary by department\n",
    "dept_salary_stddev = df_employees.groupBy(\"department\").agg(\n",
    "    stddev(\"salary\").alias(\"salary_stddev\")\n",
    ")\n",
    "display(dept_salary_stddev)\n",
    "# This computes the spread (standard deviation) of salaries for each department.\n",
    "# Useful for understanding salary variability within departments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d6b7eec-be3d-44d0-b2a3-2bba76b0397b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " fillna() and dropna() — The Null Handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c31fb44-6196-422e-b672-c2cb886d2af8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employees.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f35bbd19-d828-4030-b2c7-c317aa7977f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fill missing emails with a default patt\n",
    "df_filled_data = df_employees.fillna({\"email\": \"sugan@gmail.com\", \"city\" : \"Rochester\"})\n",
    "df_filled_data.display()\n",
    "\n",
    "df_filled_data = df_employees.dropna()\n",
    "df_filled_data.display()\n",
    "\n",
    "df_specific = df_employees.dropna(subset = [\"email\", \"department\"])\n",
    "df_specific.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b46d4b1a-754a-4852-80a4-4754ef91f2fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parse_skill = df_employees.withColumn(\"skill1\", col(\"skills\").getItem(0)).withColumn(\"skill2\", col(\"skills\").getItem(1))\n",
    "parse_skill.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0398c0e-1aba-4efa-8838-b51785c87678",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parse_skill = df_employees.withColumn(\"skill1\", col(\"skills\").getItem(0)).withColumn(\"skill2\", col(\"skills\").getItem(1)).withColumn(\"skill3\", col(\"skills\").getItem(2))\n",
    "parse_skill.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17a9aa11-dc5c-4b0c-9a88-cc31212a6891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, get\n",
    "\n",
    "parse_skill = (\n",
    "    df_employees\n",
    "    .withColumn(\"skill1\", get(col(\"skills\"), 0))\n",
    "    .withColumn(\"skill2\", get(col(\"skills\"), 1))\n",
    "    .withColumn(\"skill3\", get(col(\"skills\"), 2))\n",
    ")\n",
    "display(parse_skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4700a6f7-41eb-4951-bc26-c3da220ba13b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get all skills by department\n",
    "skills_by_dept = df_employees.groupBy(\"department\").agg(\n",
    "    # collect_set(explode(col(\"skills\"))).alias(\"unique_skills\"),\n",
    "    collect_list(\"first_name\").alias(\"employee_names\")\n",
    ")\n",
    "skills_by_dept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a47fa449-663b-4d9e-993b-377e4780e448",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explode skills to analyze individual skill distribution\n",
    "skills_exploded = df_employees.select(\n",
    "    \"first_name\",\n",
    "    \"last_name\", \n",
    "    \"department\",\n",
    "    explode(col(\"skills\")).alias(\"individual_skill\")\n",
    ")\n",
    "skills_exploded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bda304a-d7c7-45b1-b62a-9c1ba45c8475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze skill popularity\n",
    "skill_popularity = skills_exploded.groupBy(\"individual_skill\").agg(\n",
    "    count(\"*\").alias(\"skill_count\"),\n",
    "    collect_list(\"department\").alias(\"departments_using\")\n",
    ").orderBy(col(\"skill_count\").desc())\n",
    "skill_popularity.show(truncate=False)\n",
    "# Find employees with specific skills\n",
    "python_experts = skills_exploded.filter(col(\"individual_skill\") == \"Python\")\n",
    "python_experts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f97bef3-647d-4532-8879-14b73659a1ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert string dates to proper date types\n",
    "df_with_dates = df_employees.withColumn(\n",
    "    \"hire_date_formatted\", \n",
    "    to_date(col(\"hire_date\"), \"yyyy-MM-dd\")\n",
    ").withColumn(\n",
    "    \"current_date\", \n",
    "    current_date()\n",
    ").withColumn(\n",
    "    \"days_employed\",\n",
    "    datediff(current_date(), to_date(col(\"hire_date\"), \"yyyy-MM-dd\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff57baa-ddce-4247-8571-e2baf02c1c46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract date components\n",
    "df_date_analysis = df_with_dates.withColumn(\n",
    "    \"hire_year\", year(col(\"hire_date_formatted\"))\n",
    ").withColumn(\n",
    "    \"hire_month\", month(col(\"hire_date_formatted\"))\n",
    ").withColumn(\n",
    "    \"hire_quarter\", quarter(col(\"hire_date_formatted\"))\n",
    ")\n",
    "# Analyze hiring patterns\n",
    "hiring_trends = df_date_analysis.groupBy(\"hire_year\", \"hire_quarter\").agg(\n",
    "    count(\"*\").alias(\"hires_count\")\n",
    ").orderBy(\"hire_year\", \"hire_quarter\")\n",
    "hiring_trends.show()\n",
    "# Format dates for reporting\n",
    "df_formatted_dates = df_with_dates.withColumn(\n",
    "    \"hire_date_display\",\n",
    "    date_format(col(\"hire_date_formatted\"), \"MMM dd, yyyy\")\n",
    ")\n",
    "df_formatted_dates.select(\"first_name\", \"hire_date_display\", \"days_employed\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d581ed4-0d02-4e1f-ad96-db3adcc57fec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract information using regex\n",
    "df_regex = df_employees.withColumn(\n",
    "    \"email_username\",\n",
    "    regexp_extract(col(\"email\"), \"([^@]+)@\", 1)\n",
    ").withColumn(\n",
    "    \"has_tech_skills\",\n",
    "    when(\n",
    "        regexp_extract(concat_ws(\",\", col(\"skills\")), \"(Python|Java|SQL|Spark)\", 1) != \"\",\n",
    "        True\n",
    "    ).otherwise(False))\n",
    "df_regex.select(\"first_name\", \"email_username\", \"has_tech_skills\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea0d65c9-441b-4dbf-8452-5fb83413fd2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean and standardize data\n",
    "df_cleaned = df_employees.withColumn(\n",
    "    \"phone_cleaned\",\n",
    "    regexp_replace(col(\"first_name\"), \"[^a-zA-Z]\", \"\")  # Remove non-alphabetic characters\n",
    ").withColumn(\n",
    "    \"city_standardized\",\n",
    "    regexp_replace(col(\"city\"), \"\\\\s+\", \" \")  # Replace multiple spaces with single space\n",
    ")\n",
    "df_regex.select(\"first_name\", \"email_username\", \"has_tech_skills\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "256a7a7a-d96c-44b6-8691-a3c3ad1a7c7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Detect skew by checking partition sizes\n",
    "def check_skew(df):\n",
    "    sizes = df.rdd.mapPartitions(lambda x: [len(x)]).collect()\n",
    "    max_size = max(sizes)\n",
    "    min_size = min(sizes)\n",
    "    skew_ratio = max_size / min_size if min_size > 0 else float('inf')\n",
    "    return skew_ratio > 3  # threshold for skew detection\n",
    "\n",
    "# Mitigate skew with salting\n",
    "from pyspark.sql.functions import rand, floor, concat, lit\n",
    "\n",
    "def salted_join(df1, df2, join_key):\n",
    "    # Add salt to smaller table\n",
    "    salt_df = df2.withColumn(\"salt\", floor(rand() * 10).cast(\"int\")) \\\n",
    "                 .withColumn(\"join_key_salt\", concat(join_key, lit(\"_\"), col(\"salt\")))\n",
    "    \n",
    "    # Add salt to larger table\n",
    "    large_salt_df = df1.withColumn(\"salt\", floor(rand() * 10).cast(\"int\")) \\\n",
    "                      .withColumn(\"join_key_salt\", concat(join_key, lit(\"_\"), col(\"salt\")))\n",
    "    \n",
    "    return large_salt_df.join(salt_df, large_salt_df.join_key_salt == salt_df.join_key_salt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c717836e-4342-4aa3-8ebd-cbe7477248ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1.1 Create a Reusable SparkSession Builder\n",
    "Instead of repeating .builder…config() everywhere, define a single reusable function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "431c2407-b831-437e-b541-79df64297b1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark_session_utils.py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def get_spark(app_name=\"pyspark-app\",\n",
    "              shuffle_partitions=200,\n",
    "              extra_conf=None):\n",
    "    builder = (SparkSession.builder\n",
    "               .appName(app_name)\n",
    "               .config(\"spark.sql.shuffle.partitions\", shuffle_partitions))\n",
    "\n",
    "    if extra_conf:\n",
    "        for k, v in extra_conf.items():\n",
    "            builder = builder.config(k, v)\n",
    "\n",
    "    return builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99fc6801-cba6-4c97-99d1-a07c5f4678d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1.2 Schema-Enforced, Safe Readers\n",
    "\n",
    "Fail fast if upstream data changes — don’t corrupt downstream outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b82eb540-5835-4ab6-9207-7ea45ebc722f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fb7cf2f-16f1-4e38-8f22-777c1c197881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# io_utils.py\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "def read_csv_with_schema(spark, path, schema, header=True, delimiter=\",\"):\n",
    "    return (spark.read.format(\"csv\")\n",
    "            .option(\"header\", str(header).lower())\n",
    "            .option(\"delimiter\", delimiter)\n",
    "            .schema(schema)\n",
    "            .load(path))\n",
    "\n",
    "def read_parquet(spark, path):\n",
    "    return spark.read.parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93649d79-3618-420c-814e-8a1a99aa2ffd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Part 2 — Standardizing & Cleaning Data\n",
    "Real-world data is messy.\n",
    "\n",
    "Before transformations, you must standardize formats.\n",
    "\n",
    "2.1 Clean String Columns (Trim, Lowercase, Replace Blanks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af79efc3-c116-431c-a21e-9d8019ef60b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# cleaning_utils.py\n",
    "from pyspark.sql.functions import col, trim, lower, when, lit\n",
    "\n",
    "def clean_string_columns(df, cols):\n",
    "    result = df\n",
    "    for c in cols:\n",
    "        result = (result\n",
    "                  .withColumn(c, trim(col(c)))\n",
    "                  .withColumn(c, lower(col(c)))\n",
    "                  .withColumn(c, when(col(c) == \"\", lit(None)).otherwise(col(c))))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "141a102a-49cf-4f37-9401-d4a17ae22bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Part 3 — Deduplication: Keeping the Latest Record\n",
    "\n",
    "When multiple records exist for the same entity, keep the latest version.\n",
    "\n",
    "3.1 Latest Record Per Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0a8e59f-c648-4e97-a39d-fdd58d84df22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dedup_utils.py\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, desc, col\n",
    "\n",
    "def deduplicate_latest(df, key_cols, order_col):\n",
    "    w = Window.partitionBy(*key_cols).orderBy(desc(order_col))\n",
    "    ranked = df.withColumn(\"_rn\", row_number().over(w))\n",
    "    return ranked.filter(col(\"_rn\") == 1).drop(\"_rn\")\n",
    "\n",
    "# Why this matters\n",
    "# Essential for SCD1 behavior\n",
    "# Handles CDC datasets cleanly\n",
    "# Avoids noisy duplicates in analytical tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "092ceb7d-6f68-4fe3-bbee-6c1bface0f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Part 4 — Incremental Loads (Most Common ETL Pattern)\n",
    "\n",
    "Production pipelines shouldn’t process all data every day.\n",
    "\n",
    "4.1 Load Only New Data Using Max Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a4a887c-2844-4917-81c9-8d93bbaab0f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# incremental_utils.py\n",
    "from pyspark.sql.functions import col, max as spark_max\n",
    "\n",
    "def get_max_date(spark, path, date_col):\n",
    "    try:\n",
    "        df = spark.read.parquet(path)\n",
    "        row = df.select(spark_max(col(date_col)).alias(\"max_date\")).first()\n",
    "        return row[\"max_date\"] if row[\"max_date\"] else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def incremental_load(source_df, target_path, date_col):\n",
    "    spark = source_df.sparkSession\n",
    "    max_date = get_max_date(spark, target_path, date_col)\n",
    "\n",
    "    if max_date:\n",
    "        incr_df = source_df.filter(col(date_col) > max_date)\n",
    "    else:\n",
    "        incr_df = source_df\n",
    "\n",
    "    (incr_df\n",
    "        .write\n",
    "        .mode(\"append\")\n",
    "        .partitionBy(date_col)\n",
    "        .parquet(target_path))\n",
    "    \n",
    "# Why this matters\n",
    "# Massive performance improvements\n",
    "# Ideal for daily/hourly pipelines\n",
    "# Makes pipelines cheaper and faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b0ebf69-38d5-4912-b476-aec8fc146976",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Part 5 — Aggregations & Metrics\n",
    "Every analytics pipeline uses aggregation.\n",
    "\n",
    "5.1 Reusable Aggregation Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "240763e7-c02e-4580-b33f-763a629adc93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# agg_utils.py\n",
    "from pyspark.sql.functions import sum as spark_sum, count, avg\n",
    "\n",
    "def aggregate_metrics(df, group_cols, value_col):\n",
    "    agg_df = (df.groupBy(*group_cols)\n",
    "                .agg(\n",
    "                    count(\"*\").alias(\"row_count\"),\n",
    "                    spark_sum(value_col).alias(\"sum_\" + value_col),\n",
    "                    avg(value_col).alias(\"avg_\" + value_col)\n",
    "                ))\n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70c8ebe3-0383-4d7b-ac23-5e987ab68551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Part 6 — Join Optimization with Optional Broadcast\n",
    "Joins are the most expensive part of Spark processing.\n",
    "\n",
    "6.1 Join Helper with Toggle for Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "587fe2ab-66f9-4eff-8ed2-9e9791a39d9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# join_utils.py\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "def join_with_optional_broadcast(left, right, join_cols, how=\"inner\", broadcast_right=False):\n",
    "    right_df = broadcast(right) if broadcast_right else right\n",
    "    cond = [left[c] == right_df[c] for c in join_cols]\n",
    "    return left.join(right_df, cond, how)\n",
    "\n",
    "# Why this matters\n",
    "# Dramatically speeds up joins with small dimension tables\n",
    "# Allows config-based tuning instead of rewriting code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a392c28-1dcb-40be-970d-0ee87da4a4f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Part 7 — Parquet Compaction (Fix Small File Problems)\n",
    "Small files kill Spark performance. Compaction solves it.\n",
    "\n",
    "7.1 Repartition & Compact Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d350444-c80d-4432-8288-86a9e390e171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# compaction_utils.py\n",
    "def compact_parquet(input_path, output_path, num_partitions=50, partition_cols=None):\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    df = spark.read.parquet(input_path)\n",
    "\n",
    "    if partition_cols:\n",
    "        df = df.repartition(num_partitions, *partition_cols)\n",
    "        writer = df.write.partitionBy(*partition_cols)\n",
    "    else:\n",
    "        df = df.repartition(num_partitions)\n",
    "        writer = df.write\n",
    "\n",
    "    writer.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "# Why this matters\n",
    "# Faster joins and reads\n",
    "# Lower cluster compute needs\n",
    "# # Smoother downstream analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62ef0acf-08d9-4632-bcfa-cbb7395c5abf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Part 8 — Data Quality Checks\n",
    "\n",
    "Before writing out results, always verify data quality.\n",
    "\n",
    "8.1 Column-Level Quality Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b22d9c4f-72cb-4bc4-9e29-a24f2115c95b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dq_utils.py\n",
    "from pyspark.sql.functions import col, count, countDistinct, min as spark_min, max as spark_max\n",
    "\n",
    "def column_quality_report(df, cols):\n",
    "    total_rows = df.count()\n",
    "\n",
    "    exprs = []\n",
    "    for c in cols:\n",
    "        exprs.append(count(when(col(c).isNull(), c)).alias(f\"{c}_nulls\"))\n",
    "        exprs.append(countDistinct(col(c)).alias(f\"{c}_distinct\"))\n",
    "        exprs.append(spark_min(col(c)).alias(f\"{c}_min\"))\n",
    "        exprs.append(spark_max(col(c)).alias(f\"{c}_max\"))\n",
    "\n",
    "    print(f\"Total rows: {total_rows}\")\n",
    "    return df.select(*exprs)\n",
    "\n",
    "# Why this matters\n",
    "# Catch upstream ingestion issues\n",
    "# Validate dataset health at scale\n",
    "# Prevent bad data from landing in downstream tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dc7f7a4-2f29-4977-aff4-594888241766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Part 9 — Standard Writer with Dynamic Partition Overwrite\n",
    "\n",
    "Writes must be safe, idempotent, and consistent.\n",
    "\n",
    "9.1 Reusable Partitioned Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4062d58-596f-4ecf-9d69-fc69f1683ee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# writer_utils.py\n",
    "def write_partitioned(df, output_path, partition_cols, mode=\"overwrite\", dynamic_overwrite=True):\n",
    "    spark = df.sparkSession\n",
    "    if dynamic_overwrite and mode == \"overwrite\":\n",
    "        spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "    (df.write\n",
    "       .mode(mode)\n",
    "       .partitionBy(*partition_cols)\n",
    "       .parquet(output_path))\n",
    "    \n",
    "# Why this matters\n",
    "# Only overwrites partitions you actually processed\n",
    "# Makes reruns and backfills safe\n",
    "# Essential for production reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e6f4726-d2d2-4478-bf57-f5e7798a9984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from spark_session_utils import get_spark\n",
    "from io_utils import read_csv_with_schema\n",
    "from cleaning_utils import clean_string_columns\n",
    "from dedup_utils import deduplicate_latest\n",
    "from writer_utils import write_partitioned\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "spark = get_spark(app_name=\"customer-events\")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True),\n",
    "    StructField(\"event_date\", StringType(), False)\n",
    "])\n",
    "\n",
    "df = read_csv_with_schema(spark, \"/data/raw/customer_events/\", schema)\n",
    "\n",
    "df_clean = clean_string_columns(df, [\"customer_id\", \"event_type\"])\n",
    "\n",
    "df_dedup = deduplicate_latest(df_clean,\n",
    "                              key_cols=[\"customer_id\", \"event_date\"],\n",
    "                              order_col=\"event_date\")\n",
    "\n",
    "write_partitioned(df_dedup,\n",
    "                  output_path=\"/data/curated/customer_events/\",\n",
    "                  partition_cols=[\"event_date\"])\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "491dd479-580c-4d5d-b0b7-8ca1bc52b92a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Medium",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
