{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a6b6175-bd2c-4ca4-8e0f-22c48d2ec68e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"PySpark_Mastery\").getOrCreate()\n",
    "\n",
    "# Employee Dataset\n",
    "employee_data = [\n",
    "    (1, \"John\", \"Doe\", 28, \"Engineering\", 75000, \"2020-01-15\", [\"Python\", \"Spark\", \"SQL\"], \"john.doe@company.com\", \"New York\"),\n",
    "    (2, \"Jane\", \"Smith\", 32, \"Marketing\", 65000, \"2019-03-20\", [\"Excel\", \"PowerBI\", \"SQL\"], \"jane.smith@company.com\", \"Los Angeles\"),\n",
    "    (3, \"Mike\", \"Johnson\", 45, \"Engineering\", 95000, \"2018-07-10\", [\"Java\", \"Scala\", \"Spark\"], \"mike.johnson@company.com\", \"New York\"),\n",
    "    (4, \"Sarah\", \"Wilson\", 29, \"Sales\", 55000, \"2021-02-28\", [\"Salesforce\", \"Excel\"], \"sarah.wilson@company.com\", \"Chicago\"),\n",
    "    (5, \"David\", \"Brown\", 35, \"Engineering\", 85000, \"2019-11-05\", [\"Python\", \"AWS\", \"Docker\"], \"david.brown@company.com\", \"Seattle\"),\n",
    "    (6, \"Lisa\", \"Davis\", 27, \"Marketing\", 60000, \"2020-09-12\", [\"Google Ads\", \"Analytics\"], None, \"Los Angeles\"),\n",
    "    (7, \"Tom\", \"Miller\", 41, \"Sales\", 70000, \"2017-05-18\", [\"CRM\", \"Excel\"], \"tom.miller@company.com\", \"Chicago\"),\n",
    "    (8, \"Anna\", \"Garcia\", 33, \"Engineering\", 80000, \"2020-12-01\", [\"React\", \"Node.js\", \"MongoDB\"], \"anna.garcia@company.com\", \"Austin\")\n",
    "]\n",
    "\n",
    "employee_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"hire_date\", StringType(), True),\n",
    "    StructField(\"skills\", ArrayType(StringType()), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_employees = spark.createDataFrame(employee_data, employee_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "223c65ee-7e2c-4db4-a665-cd9a8a6b9775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sales Dataset\n",
    "sales_data = [\n",
    "    (101, 1, \"2023-01-15\", \"Electronics\", \"Laptop\", 1200, 1, \"Online\"),\n",
    "    (102, 2, \"2023-01-16\", \"Clothing\", \"T-Shirt\", 25, 3, \"Store\"),\n",
    "    (103, 1, \"2023-01-17\", \"Electronics\", \"Mouse\", 30, 2, \"Online\"),\n",
    "    (104, 3, \"2023-01-18\", \"Books\", \"Python Guide\", 45, 1, \"Online\"),\n",
    "    (105, 4, \"2023-01-19\", \"Electronics\", \"Keyboard\", 80, 1, \"Store\"),\n",
    "    (106, 2, \"2023-01-20\", \"Clothing\", \"Jeans\", 60, 2, \"Online\"),\n",
    "    (107, 5, \"2023-01-21\", \"Books\", \"Data Science\", 55, 1, \"Online\"),\n",
    "    (108, 1, \"2023-01-22\", \"Electronics\", \"Monitor\", 300, 1, \"Store\")\n",
    "]\n",
    "\n",
    "sales_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"price\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"channel\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_sales = spark.createDataFrame(sales_data, sales_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c78927a-677c-472d-825d-facc23313dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Department Dataset\n",
    "dept_data = [\n",
    "    (\"Engineering\", \"Tech Tower\", \"Alice Johnson\", 50),\n",
    "    (\"Marketing\", \"Business Center\", \"Bob Smith\", 25),\n",
    "    (\"Sales\", \"Sales Plaza\", \"Carol White\", 30),\n",
    "    (\"HR\", \"Admin Building\", \"David Lee\", 15)\n",
    "]\n",
    "\n",
    "dept_schema = StructType([\n",
    "    StructField(\"dept_name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"manager\", StringType(), True),\n",
    "    StructField(\"budget\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_departments = spark.createDataFrame(dept_data, dept_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aae7557b-be33-4c1d-9d7e-a079dd67291e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_employees_enhanced = df_employees.withColumn(\n",
    "    \"full_name\", \n",
    "    concat(col(\"first_name\"), lit(\" \"), col(\"last_name\"))\n",
    ")\n",
    "\n",
    "# Calculate years of experience (assuming current year is 2024)\n",
    "# df_employees_enhanced = df_employees_enhanced.withColumn(\n",
    "#     \"years_experience\",\n",
    "#     year(lit(\"2024-01-01\")) - year(to_date(col(\"hire_date\"), \"yyyy-MM-dd\"))\n",
    "# )\n",
    "# df_employees_enhanced.display()\n",
    "\n",
    "# Calculate years of experience (assuming current year is 2024)\n",
    "df_employees_enhanced = df_employees_enhanced.withColumn(\n",
    "    \"years_experience\",\n",
    "    year(current_date()) - year(to_date(col(\"hire_date\"), \"yyyy-MM-dd\"))\n",
    ")\n",
    "\n",
    "df_employees_enhanced.display()\n",
    "\n",
    "\n",
    "# Create salary categories\n",
    "df_employees_enhanced = df_employees_enhanced.withColumn(\n",
    "    \"salary_category\",\n",
    "    when(col(\"salary\") >= 80000, \"High\")\n",
    "    .when(col(\"salary\") >= 60000, \"Medium\")\n",
    "    .otherwise(\"Entry Level\")\n",
    ")\n",
    "\n",
    "df_employees_enhanced.select(\"full_name\", \"years_experience\", \"salary_category\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d25ab723-962d-46bd-a4ed-e0278860523e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "completely_unique = df_employees.dropDuplicates()\n",
    "print(f\"Original count: {df_employees.count()}\")\n",
    "print(f\"After removing duplicates: {completely_unique.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1591a3c3-79d3-4a3f-9625-cb9225212cc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employees_no_email = df_employees.filter(col(\"email\").isNull())\n",
    "print(f\"Employees without email: {employees_no_email.count()}\")\n",
    "\n",
    "# Find complete records (no nulls in critical fields)\n",
    "complete_records = df_employees.filter(\n",
    "    col(\"email\").isNotNull() & \n",
    "    col(\"salary\").isNotNull() & \n",
    "    col(\"department\").isNotNull()\n",
    ")\n",
    "complete_records.display()\n",
    "# Data quality report\n",
    "total_records = df_employees.count()\n",
    "null_emails = df_employees.filter(col(\"email\").isNull()).count()\n",
    "print(f\"Data Quality Report:\")\n",
    "print(f\"Total Records: {total_records}\")\n",
    "print(f\"Missing Emails: {null_emails} ({null_emails/total_records*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47f686cb-d357-4673-a8a7-2c48901a188e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Department-wise salary statistics\n",
    "dept_stats = df_employees.groupBy(\"department\").agg(\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    max(\"salary\").alias(\"max_salary\"),\n",
    "    min(\"salary\").alias(\"min_salary\"),\n",
    "    count(\"*\").alias(\"employee_count\"),\n",
    "    stddev(\"salary\").alias(\"salary_stddev\")\n",
    ")\n",
    "dept_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d52f693-f823-47a6-92fe-533c192e104f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate standard deviation of salary by department\n",
    "dept_salary_stddev = df_employees.groupBy(\"department\").agg(\n",
    "    stddev(\"salary\").alias(\"salary_stddev\")\n",
    ")\n",
    "display(dept_salary_stddev)\n",
    "# This computes the spread (standard deviation) of salaries for each department.\n",
    "# Useful for understanding salary variability within departments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d6b7eec-be3d-44d0-b2a3-2bba76b0397b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    " fillna() and dropna() â€” The Null Handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c31fb44-6196-422e-b672-c2cb886d2af8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_employees.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f35bbd19-d828-4030-b2c7-c317aa7977f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fill missing emails with a default patt\n",
    "df_filled_data = df_employees.fillna({\"email\": \"sugan@gmail.com\", \"city\" : \"Rochester\"})\n",
    "df_filled_data.display()\n",
    "\n",
    "df_filled_data = df_employees.dropna()\n",
    "df_filled_data.display()\n",
    "\n",
    "df_specific = df_employees.dropna(subset = [\"email\", \"department\"])\n",
    "df_specific.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b46d4b1a-754a-4852-80a4-4754ef91f2fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parse_skill = df_employees.withColumn(\"skill1\", col(\"skills\").getItem(0)).withColumn(\"skill2\", col(\"skills\").getItem(1))\n",
    "parse_skill.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0398c0e-1aba-4efa-8838-b51785c87678",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parse_skill = df_employees.withColumn(\"skill1\", col(\"skills\").getItem(0)).withColumn(\"skill2\", col(\"skills\").getItem(1)).withColumn(\"skill3\", col(\"skills\").getItem(2))\n",
    "parse_skill.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17a9aa11-dc5c-4b0c-9a88-cc31212a6891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, get\n",
    "\n",
    "parse_skill = (\n",
    "    df_employees\n",
    "    .withColumn(\"skill1\", get(col(\"skills\"), 0))\n",
    "    .withColumn(\"skill2\", get(col(\"skills\"), 1))\n",
    "    .withColumn(\"skill3\", get(col(\"skills\"), 2))\n",
    ")\n",
    "display(parse_skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4700a6f7-41eb-4951-bc26-c3da220ba13b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get all skills by department\n",
    "skills_by_dept = df_employees.groupBy(\"department\").agg(\n",
    "    # collect_set(explode(col(\"skills\"))).alias(\"unique_skills\"),\n",
    "    collect_list(\"first_name\").alias(\"employee_names\")\n",
    ")\n",
    "skills_by_dept.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a47fa449-663b-4d9e-993b-377e4780e448",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Explode skills to analyze individual skill distribution\n",
    "skills_exploded = df_employees.select(\n",
    "    \"first_name\",\n",
    "    \"last_name\", \n",
    "    \"department\",\n",
    "    explode(col(\"skills\")).alias(\"individual_skill\")\n",
    ")\n",
    "skills_exploded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bda304a-d7c7-45b1-b62a-9c1ba45c8475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze skill popularity\n",
    "skill_popularity = skills_exploded.groupBy(\"individual_skill\").agg(\n",
    "    count(\"*\").alias(\"skill_count\"),\n",
    "    collect_list(\"department\").alias(\"departments_using\")\n",
    ").orderBy(col(\"skill_count\").desc())\n",
    "skill_popularity.show(truncate=False)\n",
    "# Find employees with specific skills\n",
    "python_experts = skills_exploded.filter(col(\"individual_skill\") == \"Python\")\n",
    "python_experts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f97bef3-647d-4532-8879-14b73659a1ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert string dates to proper date types\n",
    "df_with_dates = df_employees.withColumn(\n",
    "    \"hire_date_formatted\", \n",
    "    to_date(col(\"hire_date\"), \"yyyy-MM-dd\")\n",
    ").withColumn(\n",
    "    \"current_date\", \n",
    "    current_date()\n",
    ").withColumn(\n",
    "    \"days_employed\",\n",
    "    datediff(current_date(), to_date(col(\"hire_date\"), \"yyyy-MM-dd\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff57baa-ddce-4247-8571-e2baf02c1c46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract date components\n",
    "df_date_analysis = df_with_dates.withColumn(\n",
    "    \"hire_year\", year(col(\"hire_date_formatted\"))\n",
    ").withColumn(\n",
    "    \"hire_month\", month(col(\"hire_date_formatted\"))\n",
    ").withColumn(\n",
    "    \"hire_quarter\", quarter(col(\"hire_date_formatted\"))\n",
    ")\n",
    "# Analyze hiring patterns\n",
    "hiring_trends = df_date_analysis.groupBy(\"hire_year\", \"hire_quarter\").agg(\n",
    "    count(\"*\").alias(\"hires_count\")\n",
    ").orderBy(\"hire_year\", \"hire_quarter\")\n",
    "hiring_trends.show()\n",
    "# Format dates for reporting\n",
    "df_formatted_dates = df_with_dates.withColumn(\n",
    "    \"hire_date_display\",\n",
    "    date_format(col(\"hire_date_formatted\"), \"MMM dd, yyyy\")\n",
    ")\n",
    "df_formatted_dates.select(\"first_name\", \"hire_date_display\", \"days_employed\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d581ed4-0d02-4e1f-ad96-db3adcc57fec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extract information using regex\n",
    "df_regex = df_employees.withColumn(\n",
    "    \"email_username\",\n",
    "    regexp_extract(col(\"email\"), \"([^@]+)@\", 1)\n",
    ").withColumn(\n",
    "    \"has_tech_skills\",\n",
    "    when(\n",
    "        regexp_extract(concat_ws(\",\", col(\"skills\")), \"(Python|Java|SQL|Spark)\", 1) != \"\",\n",
    "        True\n",
    "    ).otherwise(False))\n",
    "df_regex.select(\"first_name\", \"email_username\", \"has_tech_skills\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea0d65c9-441b-4dbf-8452-5fb83413fd2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean and standardize data\n",
    "df_cleaned = df_employees.withColumn(\n",
    "    \"phone_cleaned\",\n",
    "    regexp_replace(col(\"first_name\"), \"[^a-zA-Z]\", \"\")  # Remove non-alphabetic characters\n",
    ").withColumn(\n",
    "    \"city_standardized\",\n",
    "    regexp_replace(col(\"city\"), \"\\\\s+\", \" \")  # Replace multiple spaces with single space\n",
    ")\n",
    "df_regex.select(\"first_name\", \"email_username\", \"has_tech_skills\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "256a7a7a-d96c-44b6-8691-a3c3ad1a7c7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Detect skew by checking partition sizes\n",
    "def check_skew(df):\n",
    "    sizes = df.rdd.mapPartitions(lambda x: [len(x)]).collect()\n",
    "    max_size = max(sizes)\n",
    "    min_size = min(sizes)\n",
    "    skew_ratio = max_size / min_size if min_size > 0 else float('inf')\n",
    "    return skew_ratio > 3  # threshold for skew detection\n",
    "\n",
    "# Mitigate skew with salting\n",
    "from pyspark.sql.functions import rand, floor, concat, lit\n",
    "\n",
    "def salted_join(df1, df2, join_key):\n",
    "    # Add salt to smaller table\n",
    "    salt_df = df2.withColumn(\"salt\", floor(rand() * 10).cast(\"int\")) \\\n",
    "                 .withColumn(\"join_key_salt\", concat(join_key, lit(\"_\"), col(\"salt\")))\n",
    "    \n",
    "    # Add salt to larger table\n",
    "    large_salt_df = df1.withColumn(\"salt\", floor(rand() * 10).cast(\"int\")) \\\n",
    "                      .withColumn(\"join_key_salt\", concat(join_key, lit(\"_\"), col(\"salt\")))\n",
    "    \n",
    "    return large_salt_df.join(salt_df, large_salt_df.join_key_salt == salt_df.join_key_salt)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Medium",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
