{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7433bafa-b3c7-487c-a128-5d3d5f9436f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "348b8dea-94ea-42ed-8753-84dcbf1c46ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65250447-9948-4316-a89b-a0d7cece6c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Generate 5000 patient records\n",
    "num_records = 5000\n",
    "data = {\n",
    "    \"PatientID\": [f\"P{str(i).zfill(4)}\" for i in range(1, num_records + 1)],\n",
    "    \"Name\": [fake.name() for _ in range(num_records)],\n",
    "    \"SSN\": [fake.ssn() for _ in range(num_records)],\n",
    "    \"Address\": [fake.address().replace('\\n', ', ') for _ in range(num_records)],\n",
    "    \"Mail\": [fake.email() for _ in range(num_records)],\n",
    "    \"PhoneNumber\": [fake.phone_number() for _ in range(num_records)]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "patients_df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "file_path = \"sample_patients_data.csv\"\n",
    "patients_df.to_csv(file_path, index=False)\n",
    "\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c25f61f-ca6a-4ef5-8898-66855fcf2dd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import datetime\n",
    "from faker import Faker\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Increase Faker's seed for reproducibility\n",
    "Faker.seed(42)\n",
    "\n",
    "# Generate 50,000 patient records\n",
    "num_records = 50000\n",
    "data_large = {\n",
    "    \"PatientID\": [f\"HOSP{str(i).zfill(6)}\" for i in range(1, num_records + 1)],\n",
    "    \"FirstName\": [fake.first_name() for _ in range(num_records)],\n",
    "    \"LastName\": [fake.last_name() for _ in range(num_records)],\n",
    "    \"MiddleName\": [fake.random_letter().upper() for _ in range(num_records)],\n",
    "    \"SSN\": [fake.ssn() for _ in range(num_records)],\n",
    "    \"PhoneNumber\": [fake.phone_number() for _ in range(num_records)],\n",
    "    \"Gender\": [random.choice([\"Male\", \"Female\"]) for _ in range(num_records)],\n",
    "    \"DOB\": [fake.date_of_birth(minimum_age=0, maximum_age=100) for _ in range(num_records)],\n",
    "    \"Address\": [fake.address().replace('\\n', ', ') for _ in range(num_records)],\n",
    "    \"ModifiedDate\": [fake.date_this_decade(before_today=True, after_today=False) for _ in range(num_records)]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "large_patients_df = pd.DataFrame(data_large)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "large_file_path = \"/Volumes/prod/bronze/projectdata/patients.csv\"\n",
    "large_patients_df.to_csv(large_file_path, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a16f1b1-aff5-4ff6-8f07-743eda6100f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_patients = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(large_file_path)\n",
    "df_patients.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98d50785-e885-49a9-b506-b0f9fa012fdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define possible encounter types and a range of CPT codes\n",
    "encounter_types = [\"Inpatient\", \"Outpatient\", \"Emergency\", \"Telemedicine\", \"Routine Checkup\"]\n",
    "cpt_codes = [str(random.randint(10000, 99999)) for _ in range(1000)]  # Example CPT codes\n",
    "\n",
    "# Generate 100,000 encounter records\n",
    "num_encounters = 100000\n",
    "encounter_data = {\n",
    "    \"EncounterID\": [f\"ENC{str(i).zfill(6)}\" for i in range(1, num_encounters + 1)],\n",
    "    \"PatientID\": [f\"HOSP{str(random.randint(1, 50000)).zfill(6)}\" for _ in range(num_encounters)],\n",
    "    \"EncounterDate\": [fake.date_this_decade(before_today=True, after_today=False) for _ in range(num_encounters)],\n",
    "    \"EncounterType\": [random.choice(encounter_types) for _ in range(num_encounters)],\n",
    "    \"ProviderID\": [f\"PROV{str(random.randint(1, 50)).zfill(4)}\" for _ in range(num_encounters)],\n",
    "    \"DepartmentID\": [f\"DEPT{str(random.randint(1, 10)).zfill(3)}\" for _ in range(num_encounters)],\n",
    "    \"ProcedureCode\": [random.choice(cpt_codes) for _ in range(num_encounters)],\n",
    "    \"InsertedDate\": [fake.date_this_decade(before_today=True, after_today=False) for _ in range(num_encounters)],\n",
    "    \"ModifiedDate\": [fake.date_this_decade(before_today=True, after_today=False) for _ in range(num_encounters)]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "encounters_df = pd.DataFrame(encounter_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "encounters_file_path = \"/Volumes/prod/bronze/projectdata/encounters\"\n",
    "encounters_df.to_csv(encounters_file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bdfb2e4-f364-469f-a40d-08b10fd1aafd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_encounters = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(encounters_file_path)\n",
    "df_encounters.display()\n",
    "# Define possible payor types\n",
    "# payor_types = [\"Medicare\", \"Medicaid\", \"Private Insurance\",\"self_pay\",\"other\"]\n",
    "\n",
    "# Generate 100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51bb96d4-47e9-439e-a71a-0b6b744fc837",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define department data\n",
    "departments = {\n",
    "    \"DeptID\": [f\"DEPT{str(i).zfill(3)}\" for i in range(1, 21)],\n",
    "    \"Name\": [\n",
    "        \"Emergency\", \"Cardiology\", \"Neurology\", \"Oncology\", \"Pediatrics\", \n",
    "        \"Orthopedics\", \"Dermatology\", \"Gastroenterology\", \"Urology\", \n",
    "        \"Radiology\", \"Anesthesiology\", \"Pathology\", \"Surgery\", \n",
    "        \"Pulmonology\", \"Nephrology\", \"Ophthalmology\", \"Gynecology\", \n",
    "        \"Psychiatry\", \"Endocrinology\", \"Rheumatology\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "departments_df = pd.DataFrame(departments)\n",
    "\n",
    "# Save to CSV\n",
    "departments_path = '/Volumes/prod/bronze/projectdata/departments'\n",
    "departments_df.to_csv(departments_path, index=False)\n",
    "\n",
    "print(\"Department dataset created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb9c686c-1505-4671-a97c-2de78d5b351a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_departments = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(departments_path)\n",
    "df_departments.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "530e9b0c-22ed-4526-ac7b-259b1fca7c28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Parameters for data generation\n",
    "num_encounters = 10000  # Number of encounter records per hospital\n",
    "encounter_types = [\"Inpatient\", \"Outpatient\", \"Emergency\", \"Telemedicine\", \"Routine Checkup\"]\n",
    "cpt_codes = [str(random.randint(10000, 99999)) for _ in range(1000)]  # Sample CPT codes\n",
    "\n",
    "# Generate Hospital 1 encounter data\n",
    "hospital1_encounter_data = {\n",
    "    \"EncounterID\": [f\"ENC{str(i).zfill(6)}\" for i in range(1, num_encounters + 1)],\n",
    "    \"PatientID\": [f\"HOSP1-{str(random.randint(1, 5000)).zfill(6)}\" for _ in range(num_encounters)],\n",
    "    \"EncounterDate\": [fake.date_this_decade(before_today=True, after_today=False) for _ in range(num_encounters)],\n",
    "    \"EncounterType\": [random.choice(encounter_types) for _ in range(num_encounters)],\n",
    "    \"ProviderID\": [f\"PROV{str(random.randint(1, 500)).zfill(4)}\" for _ in range(num_encounters)],\n",
    "    \"DepartmentID\": [f\"DEPT{str(random.randint(1, 20)).zfill(3)}\" for _ in range(num_encounters)],\n",
    "    \"ProcedureCode\": [random.choice(cpt_codes) for _ in range(num_encounters)],\n",
    "    \"InsertedDate\": [fake.date_this_decade(before_today=True, after_today=False) for _ in range(num_encounters)],\n",
    "    \"ModifiedDate\": [fake.date_this_decade(before_today=True, after_today=False) for _ in range(num_encounters)]\n",
    "}\n",
    "\n",
    "hospital1_encounters_df = pd.DataFrame(hospital1_encounter_data)\n",
    "hospital1_encounters_path ='/Volumes/prod/bronze/projectdata/encounter_data.csv'\n",
    "\n",
    "hospital1_encounters_df.to_csv(hospital1_encounters_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e9e59a0-627a-4bc8-b737-9122091d131e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_encounters = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(hospital1_encounters_path)\n",
    "df_encounters.display()\n",
    "df_encounters.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "118fc419-8516-45f3-8125-61ac9d72f531",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Parameters for data generation\n",
    "num_encounters = 10000  # Number of encounter records per hospital\n",
    "encounter_types = [\"Inpatient\", \"Outpatient\", \"Emergency\", \"Telemedicine\", \"Routine Checkup\"]\n",
    "cpt_codes = [str(random.randint(10000, 99999)) for _ in range(1000)]  # Sample CPT codes\n",
    "\n",
    "# Generate Hospital 1 encounter data\n",
    "hospital1_encounter_data = {\n",
    "    \"EncounterID\": [f\"ENC{str(i).zfill(6)}\" for i in range(1, num_encounters + 1)],\n",
    "    \"PatientID\": [f\"HOSP1-{str(random.randint(1, 5000)).zfill(6)}\" for _ in range(num_encounters)],\n",
    "    \"EncounterDate\": [fake.date_this_decade(before_today=True, after_today=False) for _ in range(num_encounters)],\n",
    "    \"EncounterType\": [random.choice(encounter_types) for _ in range(num_encounters)],\n",
    "    \"ProviderID\": [f\"PROV{str(random.randint(1, 500)).zfill(4)}\" for _ in range(num_encounters)],\n",
    "    \"DepartmentID\": [f\"DEPT{str(random.randint(1, 20)).zfill(3)}\" for _ in range(num_encounters)],\n",
    "    \"ProcedureCode\": [random.choice(cpt_codes) for _ in range(num_encounters)],\n",
    "    \"InsertedDate\": [fake.date_this_decade(before_today=True, after_today=False) for _ in range(num_encounters)],\n",
    "    \"ModifiedDate\": [fake.date_this_decade(before_today=True, after_today=False) for _ in range(num_encounters)]\n",
    "}\n",
    "\n",
    "# Generate Hospital 2 encounter data\n",
    "hospital2_encounter_data = {\n",
    "    \"EncounterID\": [f\"ENC{str(i).zfill(6)}\" for i in range(1, num_encounters + 1)],\n",
    "    \"PatientID\": [f\"HOSP1-{str(random.randint(1, 5000)).zfill(6)}\" for _ in range(num_encounters)],\n",
    "    \"EncounterDate\": [fake.date_this_decade(before_today=True, after_today=False) for _ in range(num_encounters)],\n",
    "    \"EncounterType\": [random.choice(encounter_types) for _ in range(num_encounters)],\n",
    "    \"ProviderID\": [f\"PROV{str(random.randint(1, 500)).zfill(4)}\" for _ in range(num_encounters)],\n",
    "    \"DepartmentID\": [f\"DEPT{str(random.randint(1, 20)).zfill(3)}\" for _ in range(num_encounters)],\n",
    "    \"ProcedureCode\": [random.choice(cpt_codes) for _ in range(num_encounters)],\n",
    "    \"InsertedDate\": [fake.date_this_decade(before_today=True, after_today=False) for _ in range(num_encounters)],\n",
    "    \"ModifiedDate\": [fake.date_this_decade(before_today=True, after_today=False) for _ in range(num_encounters)]\n",
    "}\n",
    "\n",
    "# Create DataFrames\n",
    "hospital1_encounters_df = pd.DataFrame(hospital1_encounter_data)\n",
    "hospital2_encounters_df = pd.DataFrame(hospital2_encounter_data)\n",
    "\n",
    "# Save to CSV files\n",
    "hospital1_encounters_df.to_csv(\"hospital1_encounter_data.csv\", index=False)\n",
    "hospital2_encounters_df.to_csv(\"hospital2_encounter_data.csv\", index=False)\n",
    "\n",
    "print(\"Hospital 1 and Hospital 2 encounter datasets created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce71943e-db87-4569-9d0e-50801af8886e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Parameters for data generation\n",
    "num_transactions = 10000  # Number of transaction records per hospital\n",
    "amount_types = [\"Co-pay\", \"Insurance\", \"Self-pay\", \"Medicaid\", \"Medicare\"]\n",
    "visit_types = [\"Routine\", \"Follow-up\", \"Emergency\", \"Consultation\"]\n",
    "line_of_business = [\"Commercial\", \"Medicaid\", \"Medicare\", \"Self-Pay\"]\n",
    "icd_codes = [f\"I{random.randint(10, 99)}.{random.randint(0, 9)}\" for _ in range(100)]  # Sample ICD codes\n",
    "cpt_codes = [str(random.randint(10000, 99999)) for _ in range(1000)]  # Sample CPT codes\n",
    "\n",
    "# Generate Hospital 1 transaction data\n",
    "hospital1_transaction_data = {\n",
    "    \"TransactionID\": [f\"TRANS{str(i).zfill(6)}\" for i in range(1, num_transactions + 1)],\n",
    "    \"EncounterID\": [f\"ENC{str(random.randint(1, 10000)).zfill(6)}\" for _ in range(num_transactions)],\n",
    "    \"PatientID\": [f\"HOSP1-{str(random.randint(1, 5000)).zfill(6)}\" for _ in range(num_transactions)],\n",
    "    \"ProviderID\": [f\"PROV{str(random.randint(1, 500)).zfill(4)}\" for _ in range(num_transactions)],\n",
    "    \"DeptID\": [f\"DEPT{str(random.randint(1, 20)).zfill(3)}\" for _ in range(num_transactions)],\n",
    "    \"VisitDate\": [fake.date_this_year(before_today=True, after_today=False) for _ in range(num_transactions)],\n",
    "    \"ServiceDate\": [fake.date_this_year(before_today=True, after_today=False) for _ in range(num_transactions)],\n",
    "    \"PaidDate\": [fake.date_this_year(before_today=True, after_today=False) for _ in range(num_transactions)],\n",
    "    \"VisitType\": [random.choice(visit_types) for _ in range(num_transactions)],\n",
    "    \"Amount\": [round(random.uniform(50, 1000), 2) for _ in range(num_transactions)],\n",
    "    \"AmountType\": [random.choice(amount_types) for _ in range(num_transactions)],\n",
    "    \"PaidAmount\": [round(random.uniform(20, 800), 2) for _ in range(num_transactions)],\n",
    "    \"ClaimID\": [f\"CLAIM{str(random.randint(100000, 999999))}\" for _ in range(num_transactions)],\n",
    "    \"PayorID\": [f\"PAYOR{str(random.randint(1000, 9999))}\" for _ in range(num_transactions)],\n",
    "    \"ProcedureCode\": [random.choice(cpt_codes) for _ in range(num_transactions)],\n",
    "    \"ICDCode\": [random.choice(icd_codes) for _ in range(num_transactions)],\n",
    "    \"LineOfBusiness\": [random.choice(line_of_business) for _ in range(num_transactions)],\n",
    "    \"MedicaidID\": [f\"MEDI{str(random.randint(10000, 99999))}\" for _ in range(num_transactions)],\n",
    "    \"MedicareID\": [f\"MCARE{str(random.randint(10000, 99999))}\" for _ in range(num_transactions)],\n",
    "    \"InsertDate\": [fake.date_this_decade(before_today=True, after_today=False) for _ in range(num_transactions)],\n",
    "    \"ModifiedDate\": [fake.date_this_decade(before_today=True, after_today=False) for _ in range(num_transactions)]\n",
    "}\n",
    "\n",
    "# # Generate Hospital 2 transaction data\n",
    "# hospital2_transaction_data = {\n",
    "#     \"TransactionID\": [f\"TRANS{str(i).zfill(6)}\" for i in range(1, num_transactions + 1)],\n",
    "#     \"EncounterID\": [f\"ENC{str(random.randint(1, 10000)).zfill(6)}\" for _ in range(num_transactions)],\n",
    "#     \"PatientID\": [f\"HOSP1-{str(random.randint(1, 5000)).zfill(6)}\" for _ in range(num_transactions)],\n",
    "#     \"ProviderID\": [f\"PROV{str(random.randint(1, 500)).zfill(4)}\" for _ in range(num_transactions)],\n",
    "#     \"DeptID\": [f\"DEPT{str(random.randint(1, 20)).zfill(3)}\" for _ in range(num_transactions)],\n",
    "#     \"VisitDate\": [fake.date_this_year(before_today=True, after_today=False) for _ in range(num_transactions)],\n",
    "#     \"ServiceDate\": [fake.date_this_year(before_today=True, after_today=False) for _ in range(num_transactions)],\n",
    "#     \"PaidDate\": [fake.date_this_year(before_today=True, after_today=False) for _ in range(num_transactions)],\n",
    "#     \"VisitType\": [random.choice(visit_types) for _ in range(num_transactions)],\n",
    "#     \"Amount\": [round(random.uniform(50, 1000), 2) for _ in range(num_transactions)],\n",
    "#     \"AmountType\": [random.choice(amount_types) for _ in range(num_transactions)],\n",
    "#     \"PaidAmount\": [round(random.uniform(20, 800), 2) for _ in range(num_transactions)],\n",
    "#     \"ClaimID\": [f\"CLAIM{str(random.randint(100000, 999999))}\" for _ in range(num_transactions)],\n",
    "#     \"PayorID\": [f\"PAYOR{str(random.randint(1000, 9999))}\" for _ in range(num_transactions)],\n",
    "#     \"ProcedureCode\": [random.choice(cpt_codes) for _ in range(num_transactions)],\n",
    "#     \"ICDCode\": [random.choice(icd_codes) for _ in range(num_transactions)],\n",
    "#     \"LineOfBusiness\": [random.choice(line_of_business) for _ in range(num_transactions)],\n",
    "#     \"MedicaidID\": [f\"MEDI{str(random.randint(10000, 99999))}\" for _ in range(num_transactions)],\n",
    "#     \"MedicareID\": [f\"MCARE{str(random.randint(10000, 99999))}\" for _ in range(num_transactions)],\n",
    "#     \"InsertDate\": [fake.date_this_decade(before_today=True, after_today=False) for _ in range(num_transactions)],\n",
    "#     \"ModifiedDate\": [fake.date_this_decade(before_today=True, after_today=False) for _ in range(num_transactions)]\n",
    "# }\n",
    "\n",
    "# Create DataFrames\n",
    "hospital1_transactions_df = pd.DataFrame(hospital1_transaction_data)\n",
    "# hospital2_transactions_df = pd.DataFrame(hospital2_transaction_data)\n",
    "hospital1_transactions_df_path = \"/Volumes/prod/bronze/projectdata/transactions.csv\"\n",
    "# Save to CSV files\n",
    "hospital1_transactions_df.to_csv(hospital1_transactions_df_path, index=False)\n",
    "# hospital2_transactions_df.to_csv(\"hospital2_transaction_data.csv\", index=False)\n",
    "\n",
    "print(\"Hospital 1 and Hospital 2 transaction datasets created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb48b5a4-3491-4e25-a839-aa56b5bc4bb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_transactions = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", True).load(\"/Volumes/prod/bronze/projectdata/transactions.csv\")\n",
    "df_transactions.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "317bd921-3147-4272-b7b1-4d42c638e4ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Parameters for data generation\n",
    "num_claims = 10000  # Number of claim records per hospital\n",
    "payors = [\"Medicare\", \"Medicaid\", \"BlueCross\", \"Aetna\", \"UnitedHealthcare\"]\n",
    "claim_statuses = [\"Pending\", \"Approved\", \"Rejected\", \"Paid\", \"Denied\"]\n",
    "payor_types = [\"Government\", \"Private\", \"Self-pay\"]\n",
    "\n",
    "# Generate Hospital 1 claims data\n",
    "hospital1_claims_data = {\n",
    "    \"ClaimID\": [f\"CLAIM{str(i).zfill(6)}\" for i in range(1, num_claims + 1)],\n",
    "    \"TransactionID\": [f\"TRANS{str(random.randint(1, 10000)).zfill(6)}\" for _ in range(num_claims)],\n",
    "    \"PatientID\": [f\"HOSP1-{str(random.randint(1, 5000)).zfill(6)}\" for _ in range(num_claims)],\n",
    "    \"EncounterID\": [f\"ENC{str(random.randint(1, 10000)).zfill(6)}\" for _ in range(num_claims)],\n",
    "    \"ProviderID\": [f\"PROV{str(random.randint(1, 500)).zfill(4)}\" for _ in range(num_claims)],\n",
    "    \"DeptID\": [f\"DEPT{str(random.randint(1, 20)).zfill(3)}\" for _ in range(num_claims)],\n",
    "    \"ServiceDate\": [fake.date_this_year(before_today=True, after_today=False) for _ in range(num_claims)],\n",
    "    \"ClaimDate\": [fake.date_this_year(before_today=True, after_today=False) for _ in range(num_claims)],\n",
    "    \"PayorID\": [random.choice(payors) for _ in range(num_claims)],\n",
    "    \"ClaimAmount\": [round(random.uniform(100, 5000), 2) for _ in range(num_claims)],\n",
    "    \"PaidAmount\": [round(random.uniform(50, 4500), 2) for _ in range(num_claims)],\n",
    "    \"ClaimStatus\": [random.choice(claim_statuses) for _ in range(num_claims)],\n",
    "    \"PayorType\": [random.choice(payor_types) for _ in range(num_claims)],\n",
    "    \"Deductible\": [round(random.uniform(10, 500), 2) for _ in range(num_claims)],\n",
    "    \"Coinsurance\": [round(random.uniform(0, 200), 2) for _ in range(num_claims)],\n",
    "    \"Copay\": [round(random.uniform(5, 50), 2) for _ in range(num_claims)],\n",
    "    \"InsertDate\": [fake.date_this_decade(before_today=True, after_today=False) for _ in range(num_claims)],\n",
    "    \"ModifiedDate\": [fake.date_this_decade(before_today=True, after_today=False) for _ in range(num_claims)]\n",
    "}\n",
    "\n",
    "# # Generate Hospital 2 claims data\n",
    "# hospital2_claims_data = {\n",
    "#     \"ClaimID\": [f\"CLAIM{str(i).zfill(6)}\" for i in range(1, num_claims + 1)],\n",
    "#     \"TransactionID\": [f\"TRANS{str(random.randint(1, 10000)).zfill(6)}\" for _ in range(num_claims)],\n",
    "#     \"PatientID\": [f\"HOSP1-{str(random.randint(1, 5000)).zfill(6)}\" for _ in range(num_claims)],\n",
    "#     \"EncounterID\": [f\"ENC{str(random.randint(1, 10000)).zfill(6)}\" for _ in range(num_claims)],\n",
    "#     \"ProviderID\": [f\"PROV{str(random.randint(1, 500)).zfill(4)}\" for _ in range(num_claims)],\n",
    "#     \"DeptID\": [f\"DEPT{str(random.randint(1, 20)).zfill(3)}\" for _ in range(num_claims)],\n",
    "#     \"ServiceDate\": [fake.date_this_year(before_today=True, after_today=False) for _ in range(num_claims)],\n",
    "#     \"ClaimDate\": [fake.date_this_year(before_today=True, after_today=False) for _ in range(num_claims)],\n",
    "#     \"PayorID\": [random.choice(payors) for _ in range(num_claims)],\n",
    "#     \"ClaimAmount\": [round(random.uniform(100, 5000), 2) for _ in range(num_claims)],\n",
    "#     \"PaidAmount\": [round(random.uniform(50, 4500), 2) for _ in range(num_claims)],\n",
    "#     \"ClaimStatus\": [random.choice(claim_statuses) for _ in range(num_claims)],\n",
    "#     \"PayorType\": [random.choice(payor_types) for _ in range(num_claims)],\n",
    "#     \"Deductible\": [round(random.uniform(10, 500), 2) for _ in range(num_claims)],\n",
    "#     \"Coinsurance\": [round(random.uniform(0, 200), 2) for _ in range(num_claims)],\n",
    "#     \"Copay\": [round(random.uniform(5, 50), 2) for _ in range(num_claims)],\n",
    "#     \"InsertDate\": [fake.date_this_decade(before_today=True, after_today=False) for _ in range(num_claims)],\n",
    "#     \"ModifiedDate\": [fake.date_this_decade(before_today=True, after_today=False) for _ in range(num_claims)]\n",
    "# }\n",
    "\n",
    "# Create DataFrames\n",
    "hospital1_claims_df = pd.DataFrame(hospital1_claims_data)\n",
    "# hospital2_claims_df = pd.DataFrame(hospital2_claims_data)\n",
    "\n",
    "df_claims_path = \"/Volumes/prod/bronze/projectdata/claims.csv\"\n",
    "# Save to CSV files\n",
    "hospital1_claims_df.to_csv(df_claims_path, index=False)\n",
    "# hospital2_claims_df.to_csv(\"hospital2_claim_data.csv\", index=False)\n",
    "\n",
    "print(\"Hospital 1 and Hospital 2 claim datasets created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44717520-c269-49e1-9109-476c360c8eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_claims = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", True).load(df_claims_path)\n",
    "df_claims.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c948b4e0-df6b-4a7a-b444-7cfcb8f49c36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Parameters for data generation\n",
    "num_providers_hospital1 = 25  # Number of providers in Hospital 1\n",
    "num_providers_hospital2 = 30  # Number of providers in Hospital 2\n",
    "specializations = [\"Cardiology\", \"Neurology\", \"Orthopedics\", \"General Surgery\", \n",
    "                   \"Pediatrics\", \"Radiology\", \"Dermatology\", \"Oncology\", \n",
    "                   \"Anesthesiology\", \"Emergency Medicine\", \"Psychiatry\"]\n",
    "departments = [f\"DEPT{str(i).zfill(3)}\" for i in range(1, 21)]  # 20 department IDs\n",
    "\n",
    "# Generate Hospital 1 provider data\n",
    "hospital1_provider_data = {\n",
    "    \"ProviderID\": [f\"H1-PROV{str(i).zfill(4)}\" for i in range(1, num_providers_hospital1 + 1)],\n",
    "    \"FirstName\": [fake.first_name() for _ in range(num_providers_hospital1)],\n",
    "    \"LastName\": [fake.last_name() for _ in range(num_providers_hospital1)],\n",
    "    \"Specialization\": [random.choice(specializations) for _ in range(num_providers_hospital1)],\n",
    "    \"DeptID\": [random.choice(departments) for _ in range(num_providers_hospital1)],\n",
    "    \"NPI\": [fake.unique.numerify(\"##########\") for _ in range(num_providers_hospital1)]  # NPI as a 10-digit number\n",
    "}\n",
    "\n",
    "# # Generate Hospital 2 provider data\n",
    "# hospital2_provider_data = {\n",
    "#     \"ProviderID\": [f\"H2-PROV{str(i).zfill(4)}\" for i in range(1, num_providers_hospital2 + 1)],\n",
    "#     \"FirstName\": [fake.first_name() for _ in range(num_providers_hospital2)],\n",
    "#     \"LastName\": [fake.last_name() for _ in range(num_providers_hospital2)],\n",
    "#     \"Specialization\": [random.choice(specializations) for _ in range(num_providers_hospital2)],\n",
    "#     \"DeptID\": [random.choice(departments) for _ in range(num_providers_hospital2)],\n",
    "#     \"NPI\": [fake.unique.numerify(\"##########\") for _ in range(num_providers_hospital2)]  # NPI as a 10-digit number\n",
    "# }\n",
    "\n",
    "# Create DataFrames\n",
    "hospital1_providers_df = pd.DataFrame(hospital1_provider_data)\n",
    "# hospital2_providers_df = pd.DataFrame(hospital2_provider_data)\n",
    "df_provider_path = \"/Volumes/prod/bronze/projectdata/providers.csv\"\n",
    "\n",
    "# Save to CSV files\n",
    "hospital1_providers_df.to_csv(df_provider_path, index=False)\n",
    "# hospital2_providers_df.to_csv(\"hospital2_provider_data.csv\", index=False)\n",
    "\n",
    "print(\"Hospital 1 and Hospital 2 provider datasets created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97921faa-464b-46ff-8dda-7f27bf63afe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_provider_path = '/Volumes/prod/bronze/projectdata/providers.csv'\n",
    "df_provider = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", True).load(df_provider_path)\n",
    "\n",
    "df_provider.display()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2952932c-96ba-4c07-bd33-4fbc5a85a48c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_date, lit\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, BooleanType\n",
    "\n",
    "\n",
    "\n",
    "client_id = 'c1b31e75-de89-4e9e-9738-87a4779e98e9_b7b0ac50-33a3-4ba5-abc2-4f4b1050ab15'\n",
    "client_secret = 'UtTb8EIodKWT2g3BoIkgGsNbMcv0ClZKJbEij4ySzTs='\n",
    "base_url = 'https://id.who.int/icd/'\n",
    "current_date=datetime.now().date()\n",
    "\n",
    "auth_url = 'https://icdaccessmanagement.who.int/connect/token'\n",
    "auth_response = requests.post(auth_url, data={\n",
    "    'client_id': client_id,\n",
    "    'client_secret': client_secret,\n",
    "    'grant_type': 'client_credentials'\n",
    "})\n",
    "\n",
    "if auth_response.status_code == 200:\n",
    "    access_token = auth_response.json().get('access_token')\n",
    "else:\n",
    "    raise Exception(f\"Failed to obtain access token: {auth_response.status_code} - {auth_response.text}\")\n",
    "\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {access_token}',\n",
    "    'API-Version': 'v2',  # Add the API-Version header\n",
    "    'Accept-Language': 'en',\n",
    "}\n",
    "\n",
    "def fetch_icd_codes(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise Exception(f\"Failed to fetch data: {response.status_code} - {response.text}\")\n",
    "\n",
    "def extract_codes(url):\n",
    "    data = fetch_icd_codes(url)\n",
    "    codes = []\n",
    "    if 'child' in data:\n",
    "        for child_url in data['child']:\n",
    "            codes.extend(extract_codes(child_url))\n",
    "    else:\n",
    "        if 'code' in data and 'title' in data:\n",
    "            # print(data['code'],data['title']['@value'])\n",
    "            codes.append({\n",
    "                'icd_code': data['code'],\n",
    "                'icd_code_type': 'ICD-10',\n",
    "                'code_description': data['title']['@value'],\n",
    "                'inserted_date': current_date,\n",
    "                'updated_date': current_date,\n",
    "                'is_current_flag': True\n",
    "            })\n",
    "    return codes\n",
    "\n",
    "# Start from the root URL\n",
    "root_url = 'https://id.who.int/icd/release/10/2019/A00-A09'\n",
    "icd_codes = extract_codes(root_url)\n",
    "\n",
    "\n",
    "# Define the schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"icd_code\", StringType(), True),\n",
    "    StructField(\"icd_code_type\", StringType(), True),\n",
    "    StructField(\"code_description\", StringType(), True),\n",
    "    StructField(\"inserted_date\", DateType(), True),\n",
    "    StructField(\"updated_date\", DateType(), True),\n",
    "    StructField(\"is_current_flag\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "# Create a DataFrame using the defined schema\n",
    "print(icd_codes)\n",
    "df = spark.createDataFrame(icd_codes, schema=schema)\n",
    "df.write.format(\"parquet\").mode(\"append\").save(\"/mnt/bronze/icd_codes/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7104d2a-d7c7-4414-9591-b0284ae3c2a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# url = 'https://icdaccessmanagement.who.int/connect'\n",
    "\n",
    "base_url = 'https://id.who.int/icd/release/10/2019/'\n",
    "\n",
    "\n",
    "def fetch_icd_codes(url):\n",
    "    data = []\n",
    "    response = requests.get(base_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        data.append(response.json())\n",
    "        return data\n",
    "    else:\n",
    "        raise Exception(f\"Failed to fetch data: {response.status_code} - {response.text}\")\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"icd_code\", StringType(), True),\n",
    "    StructField(\"icd_code_type\", StringType(), True),\n",
    "    StructField(\"code_description\", StringType(), True),\n",
    "    StructField(\"inserted_date\", DateType(), True),\n",
    "    StructField(\"updated_date\", DateType(), True),\n",
    "    StructField(\"is_current_flag\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "# Create a DataFrame using the defined schema\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aefd17c-4fb6-49c6-9c34-9c4250935fb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DateType, BooleanType\n",
    ")\n",
    "\n",
    "base_url = 'https://id.who.int/icd/release/10/2019/'\n",
    "headers = {}  # Add headers if needed\n",
    "\n",
    "def fetch_icd_codes(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        # Adjust this part based on the actual JSON structure\n",
    "        # Example assumes a list of dicts under 'codes' key\n",
    "        codes = response.json().get('codes', [])\n",
    "        # Map each code dict to match the schema fields\n",
    "        data = [\n",
    "            {\n",
    "                \"icd_code\": code.get(\"code\"),\n",
    "                \"icd_code_type\": code.get(\"type\"),\n",
    "                \"code_description\": code.get(\"description\"),\n",
    "                \"inserted_date\": None,\n",
    "                \"updated_date\": None,\n",
    "                \"is_current_flag\": True\n",
    "            }\n",
    "            for code in codes\n",
    "        ]\n",
    "        return data\n",
    "   \n",
    "\n",
    "data = fetch_icd_codes(base_url)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"icd_code\", StringType(), True),\n",
    "    StructField(\"icd_code_type\", StringType(), True),\n",
    "    StructField(\"code_description\", StringType(), True),\n",
    "    StructField(\"inserted_date\", DateType(), True),\n",
    "    StructField(\"updated_date\", DateType(), True),\n",
    "    StructField(\"is_current_flag\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Project Data Creation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
