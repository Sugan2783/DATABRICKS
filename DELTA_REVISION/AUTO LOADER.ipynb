{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a7b4ab3-626b-4a79-8646-e17db5f9ee97",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "_path ="
    }
   },
   "outputs": [],
   "source": [
    "input_path = \"/Volumes/dev/bronze/landing/input/\"\n",
    "schema_loc = \"/Volumes/dev/bronze/landing/schema\"\n",
    "checkpoint_loc = \"/Volumes/dev/bronze/landing/checkpoint1/\"\n",
    "output_path = \"/Volumes/dev/bronze/transactions/autoloader_output\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07e0abf6-c5ac-4733-bf06-a6d0d150c2e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv = (\n",
    "    spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"delta\")                # Or json, parquet\n",
    "        .option(\"cloudFiles.schemaLocation\", schema_loc)   # Schema tracking\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")     # Type detection\n",
    "        .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")  \n",
    "        .option(\"cloudFiles.allowOverwrites\", \"true\")\n",
    "        .load(input_path)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cce05b66-ac63-44a0-924c-69009446630c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv.display()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f919deb-e88d-4e9f-9e02-6d6cb435f704",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_csv = (\n",
    "    spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"delta\")\n",
    "        .option(\"cloudFiles.schemaLocation\", schema_loc)\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    "        .option(\"cloudFiles.allowOverwrites\", \"true\")\n",
    "        .load(input_path)\n",
    ")\n",
    "\n",
    "query = (\n",
    "    df_csv.writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", checkpoint_loc)\n",
    "        .trigger(availableNow=True)  # Processes all available data and then stops\n",
    "        .start(output_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "420995bf-52e6-4692-be79-d9f4e180894d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "he .trigger(availableNow=True) option in writeStream configures the streaming query to process all available data and then stop automatically. This is useful for batch-like processing of streaming sources, where you want to ingest all current files and terminate the query when done, rather than keeping it running continuously. This mode is supported for Auto Loader and Delta sources in Databricks\n",
    "1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d637f24b-b2fb-448e-b6f7-c274edb57c23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "he .trigger(once=True) option in writeStream configures the streaming query to process all available data in a single micro-batch and then stop automatically. This is useful for scenarios where you want to run streaming logic as a one-time batch job, ingesting all current data and terminating the query when done. It is similar to .trigger(availableNow=True), but processes data in one micro-batch rather than multiple batches if new data arrives during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31b32c0d-6fb2-4873-9340-ac70b6e3d693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = (\n",
    "    df_csv.writeStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"checkpointLocation\", checkpoint_loc)\n",
    "        .trigger(once=True)\n",
    "        .start(output_path)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "AUTO LOADER",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
