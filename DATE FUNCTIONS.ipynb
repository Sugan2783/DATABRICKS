{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22fcb331-ac10-49b5-a0d2-2f56b1a126b2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"order_date\":90,\"delivery_date\":90,\"current_date\":103,\"delivery_plus_5\":104,\"delivery_minus_5\":113,\"months_between_order_delivery\":116,\"days_from_today_to_delivery\":100},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763566843159}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, date_add, date_sub, datediff, months_between, current_date, current_timestamp, year, month, dayofmonth, date_format\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"Date Functions\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, \"2024-11-10\", \"2024-11-20\"),\n",
    "    (2, \"2023-10-05\", \"2023-10-15\"),\n",
    "    (3, \"2022-05-01\", \"2022-05-10\")\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"order_date\", \"delivery_date\"])\n",
    "\n",
    "# Convert strings to date format\n",
    "df = df.withColumn(\"order_date\", to_date(col(\"order_date\"), \"yyyy-MM-dd\"))\n",
    "df = df.withColumn(\"delivery_date\", to_date(col(\"delivery_date\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Add current date and timestamp cols\n",
    "df = df.withColumn(\"current_date\", current_date())\n",
    "df = df.withColumn(\"current_timestamp\", current_timestamp())\n",
    "\n",
    "# Calculate the difference in days between delivery and order dates\n",
    "df = df.withColumn(\"days_to_delivery\", datediff(col(\"delivery_date\"), col(\"order_date\")))\n",
    "\n",
    "# Add 5 days to the delivery date\n",
    "df = df.withColumn(\"delivery_plus_5\", date_add(col(\"delivery_date\"), 5))\n",
    "\n",
    "# Subtract 5 days from the delivery date\n",
    "df = df.withColumn(\"delivery_minus_5\", date_sub(col(\"delivery_date\"), 5))\n",
    "\n",
    "# Calculate the months between order and delivery dates\n",
    "df = df.withColumn(\"months_between_order_delivery\", months_between(col(\"delivery_date\"), col(\"order_date\")))\n",
    "\n",
    "# Calculate the difference in days between current date and delivery date\n",
    "df = df.withColumn(\"days_from_today_to_delivery\", datediff(current_date(), col(\"delivery_date\")))\n",
    "\n",
    "# Extract year, month, and day\n",
    "df = df.withColumn(\"year\", year(col(\"order_date\")))\n",
    "df = df.withColumn(\"month\", month(col(\"order_date\")))\n",
    "df = df.withColumn(\"day\", dayofmonth(col(\"order_date\")))\n",
    "\n",
    "# Format date as string\n",
    "df = df.withColumn(\"formatted_date\", date_format(col(\"order_date\"), \"MMMM dd, yyyy\"))\n",
    "\n",
    "# Show results\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a0cfda3-727e-456e-aa67-d37715640e0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "FILTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56fca38f-2e0e-44c8-a142-72b4a8b81d75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Filtering Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [\n",
    "    (\"Alice\", \"HR\", 5000),\n",
    "    (\"Bob\", \"IT\", 6000),\n",
    "    (\"Charlie\", \"Finance\", 7000),\n",
    "    (\"David\", \"IT\", 4000),\n",
    "    (\"Eve\", \"HR\", 5500),\n",
    "    (\"Frank\", \"Finance\", 8000),\n",
    "]\n",
    "columns = [\"name\", \"department\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show original data\n",
    "print(\"Original Data:\")\n",
    "df.show()\n",
    "\n",
    "# Filter rows with salary greater than 6000\n",
    "print(\"Filter: Salary > 6000\")\n",
    "df.filter(df.salary > 6000).show()\n",
    "\n",
    "# Filter rows belonging to a specific department\n",
    "print(\"Filter: Department = 'IT'\")\n",
    "df.filter(df.department == \"IT\").show()\n",
    "\n",
    "# Combine multiple filter conditions\n",
    "print(\"Filter: Salary > 5000 and Department = 'HR'\")\n",
    "df.filter((df.salary > 5000) & (df.department == \"HR\")).show()\n",
    "\n",
    "# Using SQL-like where clause\n",
    "print(\"Filter: Salary < 6000 using where()\")\n",
    "df.where(\"salary < 6000\").show()\n",
    "\n",
    "# Filter using isin (e.g., department is either IT or HR)\n",
    "print(\"Filter: Department in ('IT', 'HR')\")\n",
    "df.filter(df.department.isin(\"IT\", \"HR\")).show()\n",
    "\n",
    "# Filter rows where a column is null or not null\n",
    "from pyspark.sql.functions import col\n",
    "data_with_nulls = [\n",
    "    (\"Alice\", None, 5000),\n",
    "    (\"Bob\", \"IT\", 6000),\n",
    "    (None, \"Finance\", 7000),\n",
    "    (\"David\", \"IT\", 4000),\n",
    "]\n",
    "df_with_nulls = spark.createDataFrame(data_with_nulls, columns)\n",
    "\n",
    "print(\"Filter: Rows where department is not null\")\n",
    "df_with_nulls.filter(col(\"department\").isNotNull()).show()\n",
    "\n",
    "print(\"Filter: Rows where name is null\")\n",
    "df_with_nulls.filter(col(\"name\").isNull()).show()\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33954000-1bc4-4c9c-bde5-d1d6efe4fe2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Why Use F for Functions?\n",
    "In PySpark, most aggregation functions are available in the pyspark.sql.functions module. It's a common practice to import this module as F for two reasons:\n",
    "\n",
    "Clarity: It distinguishes PySpark functions (e.g., F.sum) from Python built-ins (e.g., sum).\n",
    "Readability: Using F makes the code concise and easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eba4ff3c-7a0e-4b2d-8ae2-855370c36fce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, sum, count\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Grouping Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [\n",
    "    (\"Alice\", \"HR\", 5000),\n",
    "    (\"Bob\", \"IT\", 6000),\n",
    "    (\"Charlie\", \"Finance\", 7000),\n",
    "    (\"David\", \"IT\", 6000),\n",
    "    (\"Eve\", \"HR\", 5500),\n",
    "    (\"Frank\", \"Finance\", 8000),\n",
    "]\n",
    "columns = [\"name\", \"department\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Show original data\n",
    "print(\"Original Data:\")\n",
    "df.show()\n",
    "\n",
    "# Group by department and calculate aggregates\n",
    "print(\"Group by Department - Count:\")\n",
    "df.groupBy(\"department\").count().show()\n",
    "\n",
    "print(\"Group by Department - Sum of Salaries:\")\n",
    "df.groupBy(\"department\").sum(\"salary\").show()\n",
    "\n",
    "print(\"Group by Department - Average Salary:\")\n",
    "df.groupBy(\"department\").agg(avg(\"salary\")).show()\n",
    "\n",
    "print(\"Group by Department - Multiple Aggregates:\")\n",
    "df.groupBy(\"department\").agg(\n",
    "    count(\"name\").alias(\"employee_count\"),\n",
    "    sum(\"salary\").alias(\"total_salary\"),\n",
    "    avg(\"salary\").alias(\"average_salary\")\n",
    ").show()\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e2bd33a-72c7-4ef4-8647-9dc99f5f74bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Wrapping Up: Why PySpark?\n",
    "So, why should you love PySpark? Here’s the TL;DR:\n",
    "\n",
    "Speed: PySpark can process huge datasets in parallel across multiple machines. Your laptop never stood a chance.\n",
    "Scale: Whether you’re working with 10MB or 10TB of data, PySpark’s got your back.\n",
    "Power: SQL-like queries, machine learning, real-time analytics-all rolled into one package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "013d6cad-36b5-40fe-bd99-68a04e3ed293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Join Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame 1 (employees)\n",
    "data1 = [\n",
    "    (1, \"Alice\", \"HR\"),\n",
    "    (2, \"Bob\", \"IT\"),\n",
    "    (3, \"Charlie\", \"Finance\"),\n",
    "    (4, \"David\", \"IT\")\n",
    "]\n",
    "columns1 = [\"id\", \"name\", \"dept\"]\n",
    "df1 = spark.createDataFrame(data1, columns1)\n",
    "\n",
    "# Sample DataFrame 2 (departments)\n",
    "data2 = [\n",
    "    (\"HR\", \"Human Resources\"),\n",
    "    (\"IT\", \"Information Technology\"),\n",
    "    (\"Marketing\", \"Marketing\"),\n",
    "]\n",
    "columns2 = [\"dept\", \"dept_name\"]\n",
    "df2 = spark.createDataFrame(data2, columns2)\n",
    "\n",
    "# Perform joins\n",
    "# Inner Join\n",
    "inner_join = df1.join(df2, on=\"dept\", how=\"inner\")\n",
    "print(\"Inner Join Result:\")\n",
    "inner_join.show()\n",
    "\n",
    "# Left Join\n",
    "left_join = df1.join(df2, on=\"dept\", how=\"left\")\n",
    "print(\"Left Join Result:\")\n",
    "left_join.show()\n",
    "\n",
    "# Full Outer Join\n",
    "outer_join = df1.join(df2, on=\"dept\", how=\"outer\")\n",
    "print(\"Full Outer Join Result:\")\n",
    "outer_join.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d694725a-b934-4e3d-9a06-a14e5f00008a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Arithmetic Example\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(1, 10, 3), (2, 20, 5), (3, 15, 4)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"value1\", \"value2\"])\n",
    "\n",
    "# Perform arithmetic operations\n",
    "df = df.withColumn(\"sum\", col(\"value1\") + col(\"value2\")) \\\n",
    "       .withColumn(\"difference\", col(\"value1\") - col(\"value2\")) \\\n",
    "       .withColumn(\"product\", col(\"value1\") * col(\"value2\")) \\\n",
    "       .withColumn(\"quotient\", col(\"value1\") / col(\"value2\")) \\\n",
    "       .withColumn(\"remainder\", col(\"value1\") % col(\"value2\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fcb8147-330e-4e99-a9d0-1b507903a7e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import abs, round, ceil, sqrt\n",
    "\n",
    "# Apply math functions\n",
    "df = df.withColumn(\"absolute_value1\", abs(col(\"value1\"))) \\\n",
    "       .withColumn(\"rounded_value2\", round(col(\"value2\"), 1)) \\\n",
    "       .withColumn(\"ceil_value1\", ceil(col(\"value1\"))) \\\n",
    "       .withColumn(\"sqrt_value2\", sqrt(col(\"value2\")))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9f51fa4-abad-453b-baa8-8eab343c21d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pow, log10\n",
    "\n",
    "# Combining functions\n",
    "df = df.withColumn(\"power_value1\", pow(col(\"value1\"), 2)) \\\n",
    "       .withColumn(\"log10_value2\", log10(col(\"value2\")))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f489e056-6ba6-474d-9198-8679a41d1940",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading CSV with more options\n",
    "df = spark.read.option(\"header\", True) \\\n",
    "               .option(\"inferSchema\", True) \\\n",
    "               .option(\"nullValue\", \"N/A\") \\\n",
    "               .option(\"dateFormat\", \"MM/dd/yyyy\") \\\n",
    "               .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "               .csv(\"/path/to/your/data.csv\")\n",
    "\n",
    "df.show(5)\n",
    "\n",
    "\n",
    "# Writing CSV with delimiter and compression\n",
    "df.write.option(\"header\", True) \\\n",
    "        .option(\"sep\", \";\") \\\n",
    "        .option(\"compression\", \"gzip\") \\\n",
    "        .csv(\"/path/to/compressed_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04b4c35b-efbe-4858-bdd8-5ead23ce2983",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Null Values: Use nullValue to specify a string to interpret as null, such as \"N/A\" or \"NULL\".\n",
    "Date Format: Set dateFormat to specify the date format for date fields.\n",
    "Mode: Determines how Spark handles corrupt records. Options include:\n",
    "PERMISSIVE (default): Puts corrupt records in a separate column.\n",
    "DROPMALFORMED: Drops rows with bad records.\n",
    "FAILFAST: Throws an error for corrupt records.\n",
    "Example with null values, date format, and mode options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e35d7666-c7b6-49c0-82d7-29f959aabc49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Null Values: Use nullValue to specify a string to interpret as null, such as \"N/A\" or \"NULL\".\n",
    "Date Format: Set dateFormat to specify the date format for date fields.\n",
    "Mode: Determines how Spark handles corrupt records. Options include:\n",
    "PERMISSIVE (default): Puts corrupt records in a separate column.\n",
    "DROPMALFORMED: Drops rows with bad records.\n",
    "FAILFAST: Throws an error for corrupt records.\n",
    "Example with null values, date format, and mode options:\n",
    "    \n",
    "\n",
    "    Header: Set header=True to include column names in the first row.\n",
    "Mode: Controls the behavior if the output file or directory already exists.\n",
    "overwrite: Replaces existing data.\n",
    "append: Adds data to the existing file.\n",
    "ignore: Skips writing if the file exists.\n",
    "error or errorifexists: Throws an error if the file exists.\n",
    "\n",
    "\n",
    "Delimiter: Set sep to specify a custom delimiter.\n",
    "Compression: Choose a compression method such as gzip, bzip2, lz4, snappy, or deflate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e21951d0-86cb-4174-940e-d3b6d1f6f8e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading a JSON with multiline and schema sampling\n",
    "df = spark.read.option(\"multiline\", True) \\\n",
    "               .option(\"samplingRatio\", 0.5) \\\n",
    "               .json(\"/path/to/your/multiline_data.json\")\n",
    "\n",
    "df.show(5)\n",
    "\n",
    "# Writing JSON with partitioning by year\n",
    "df.write.option(\"header\", True) \\\n",
    "        .partitionBy(\"year\") \\\n",
    "        .json(\"/path/to/partitioned_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "148b321d-f5c9-4afb-be17-cf111409fccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "2. DataFrame Column Notation (df.colName)\n",
    "You can access columns as attributes of the DataFrame directly, making the syntax cleaner and allowing complex operations.\n",
    "\n",
    "Examples:\n",
    "\n",
    "# Selecting columns using dot notation\n",
    "df.select(df.name, df.age)\n",
    "\n",
    "# Filtering rows based on column conditions\n",
    "df.filter(df.age > 30)\n",
    "Note: Avoid this syntax for column names containing spaces or special characters.\n",
    "\n",
    "When to use it: This notation is handy for accessing and transforming columns and can make code more readable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f4fe621-fd55-4067-b7ca-eafd42956ce5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "3. Using col() Function\n",
    "The col() function from pyspark.sql.functions is versatile and ideal when passing column names as variables or when chaining multiple column operations.\n",
    "\n",
    "Examples:\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Selecting columns using col()\n",
    "df.select(col(\"name\"), col(\"age\"))\n",
    "\n",
    "# Filtering rows using col() for flexibility\n",
    "age_column = \"age\"\n",
    "df.filter(col(age_column) > 30)\n",
    "When to use it: Use col() when dynamically referencing column names or passing them as variables. This is common in reusable code or functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d22102d-2a3e-4d6e-b868-9452ae5e4473",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "4. Bracket Notation (df[\"colName\"])\n",
    "Bracket notation lets you reference columns using dictionary-style syntax. It’s flexible and frequently used for transformations and chaining.\n",
    "\n",
    "Examples:\n",
    "\n",
    "# Selecting columns\n",
    "df.select(df[\"name\"], df[\"age\"])\n",
    "\n",
    "# Filtering with expressions\n",
    "df.filter(df[\"age\"] > 30)\n",
    "\n",
    "# Applying transformations\n",
    "df.select((df[\"age\"] + 10).alias(\"age_plus_10\"))\n",
    "When to use it: Use bracket notation when dealing with columns with spaces or special characters. It’s also helpful for complex expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1788c0d9-1964-44e4-8c4c-33fa1c79888c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Summary\n",
    "Each method of referencing columns in PySpark has unique strengths:\n",
    "\n",
    "String Names: Great for SQL-style filtering and simple selections.\n",
    "Dot Notation (df.colName): Clear for simple selections and filtering.\n",
    "col() Function: Flexible for dynamic column references.\n",
    "Bracket Notation (df[\"colName\"]): Robust for complex expressions.\n",
    "lit(): Ideal for working with constants alongside column data.\n",
    "Using the right column notation can help make your PySpark code more readable, flexible, and SQL-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd48c5ec-f84f-485f-ba9c-548327e5316d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, concat, concat_ws, substring, upper, lower, initcap, \n",
    "    trim, ltrim, rtrim, regexp_replace, regexp_extract, length, \n",
    "    instr, lpad, rpad\n",
    ")\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"John\", \"Doe\", \"john.doe@example.com\", \"   active   \", \"12345\"),\n",
    "    (\"Jane\", \"Smith\", \"jane.smith@work.org\", \"inactive\", \"67890\"),\n",
    "    (\"Sam\", \"Brown\", \"sam.brown@data.net\", \"active   \", \"111213\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "spark = SparkSession.builder.appName(\"StringFunctionsWithColumn\").getOrCreate()\n",
    "df = spark.createDataFrame(data, [\"first_name\", \"last_name\", \"email\", \"status\", \"account_id\"])\n",
    "\n",
    "# Applying string functions using withColumn\n",
    "df_transformed = df \\\n",
    "    .withColumn(\"full_name\", concat_ws(\" \", col(\"first_name\"), col(\"last_name\"))) \\\n",
    "    .withColumn(\"email_uppercase\", upper(col(\"email\"))) \\\n",
    "    .withColumn(\"email_domain\", regexp_extract(col(\"email\"), r\"@(\\w+)\", 1)) \\\n",
    "    .withColumn(\"trimmed_status\", trim(col(\"status\"))) \\\n",
    "    .withColumn(\"padded_account_id\", lpad(col(\"account_id\"), 10, \"0\")) \\\n",
    "    .withColumn(\"email_prefix\", substring(col(\"email\"), 1, 5)) \\\n",
    "    .withColumn(\"cleaned_email\", regexp_replace(col(\"email\"), r\"[.@]\", \"-\")) \\\n",
    "    .withColumn(\"email_length\", length(col(\"email\"))) \\\n",
    "    .withColumn(\"first_name_initcap\", initcap(col(\"first_name\"))) \\\n",
    "    .withColumn(\"description\", concat_ws(\" | \", col(\"full_name\"), col(\"trimmed_status\"), col(\"email_domain\")))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "df_transformed.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b24f8a3-7d01-4209-900b-ecf8f4218d30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Window specification for cumulative sum\n",
    "window_spec = Window.partitionBy(\"category\").orderBy(\"date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Apply cumulative sum\n",
    "df = df.withColumn(\"cumulative_sales\", sum(\"sales\").over(window_spec))\n",
    "\n",
    "# Show the result\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97e81160-e357-4e62-b40c-76a5dab88258",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Window specification for moving average over the last 3 rows\n",
    "window_spec = Window.partitionBy(\"category\").orderBy(\"date\").rowsBetween(-2, Window.currentRow)\n",
    "\n",
    "# Apply moving average\n",
    "df = df.withColumn(\"moving_avg_sales\", avg(\"sales\").over(window_spec))\n",
    "\n",
    "# Show the result\n",
    "df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "447a85b7-49ed-4818-84ac-2c77a1efde79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Window specification excluding future rows\n",
    "window_spec = Window.partitionBy(\"category\").orderBy(\"date\").rowsBetween(Window.unboundedPreceding, -1)\n",
    "\n",
    "# Apply cumulative sum excluding current and future rows\n",
    "df = df.withColumn(\"cumulative_sales_excluding_future\", sum(\"sales\").over(window_spec))\n",
    "\n",
    "# Show the result\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "343c29dc-5e95-4489-84e7-5ffba49a7202",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, avg\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"2024-01-01\", \"A\", 100),\n",
    "    (\"2024-01-02\", \"A\", 150),\n",
    "    (\"2024-01-03\", \"A\", 200),\n",
    "    (\"2024-01-01\", \"B\", 50),\n",
    "    (\"2024-01-02\", \"B\", 75),\n",
    "    (\"2024-01-03\", \"B\", 100)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "spark = SparkSession.builder.appName(\"WindowFunctions\").getOrCreate()\n",
    "df = spark.createDataFrame(data, [\"date\", \"category\", \"sales\"])\n",
    "\n",
    "# 1. Cumulative Sum\n",
    "window_spec = Window.partitionBy(\"category\").orderBy(\"date\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df_cumulative_sum = df.withColumn(\"cumulative_sales\", sum(\"sales\").over(window_spec))\n",
    "\n",
    "# 2. Moving Average (3-day window)\n",
    "window_spec_avg = Window.partitionBy(\"category\").orderBy(\"date\").rowsBetween(-2, Window.currentRow)\n",
    "df_moving_avg = df_cumulative_sum.withColumn(\"moving_avg_sales\", avg(\"sales\").over(window_spec_avg))\n",
    "\n",
    "# 3. Excluding Current & Future Rows\n",
    "window_spec_excluding_future = Window.partitionBy(\"category\").orderBy(\"date\").rowsBetween(Window.unboundedPreceding, -1)\n",
    "df_final = df_moving_avg.withColumn(\"cumulative_sales_excluding_curennt_n_future\", sum(\"sales\").over(window_spec_excluding_future))\n",
    "\n",
    "# Window.unboundedPreceding: Starts from the first row.\n",
    "# -1: Ends one row before the current row.\n",
    "\n",
    "\n",
    "# Show the result\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfbb9c83-b407-49a7-9733-ff310403df02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, rank, dense_rank, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (\"Electronics\", \"Phone\", 1000),\n",
    "    (\"Electronics\", \"Laptop\", 1500),\n",
    "    (\"Electronics\", \"Tablet\", 800),\n",
    "    (\"Furniture\", \"Chair\", 300),\n",
    "    (\"Furniture\", \"Table\", 300),\n",
    "    (\"Furniture\", \"Desk\", 600),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "spark = SparkSession.builder.appName(\"WindowFunctions\").getOrCreate()\n",
    "df = spark.createDataFrame(data, [\"category\", \"product\", \"sales\"])\n",
    "\n",
    "# Define window specification\n",
    "window_spec = Window.partitionBy(\"category\").orderBy(\"sales\")\n",
    "\n",
    "# Apply window functions\n",
    "df_transformed = df \\\n",
    "    .withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "    .withColumn(\"dense_rank\", dense_rank().over(window_spec)) \\\n",
    "    .withColumn(\"row_number\", row_number().over(window_spec)) \\\n",
    "    .withColumn(\"cumulative_sales\", sum(\"sales\").over(window_spec)) \\\n",
    "    .withColumn(\"average_sales\", avg(\"sales\").over(window_spec))\n",
    "\n",
    "df_transformed.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DATE FUNCTIONS",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
