{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "357abd86-f4ff-4929-aff6-a08b00c517a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Write an pyspark code to find the ctr of each Ad.Round ctr to 2 \n",
    "decimal points. Order the result table by ctr in descending order \n",
    "and by ad_id in ascending order in case of a tie. \n",
    "Ctr=Clicked/(Clicked+Viewed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efec781e-a46f-4d21-ab4f-0a535e343bec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "data = [ \n",
    "(1, 1, 'Clicked'), \n",
    "(2, 2, 'Clicked'), \n",
    "(3, 3, 'Viewed'), \n",
    "(5, 5, 'Ignored'), \n",
    "(1, 7, 'Ignored'), \n",
    "(2, 7, 'Viewed'), \n",
    "(3, 5, 'Clicked'), \n",
    "(1, 4, 'Viewed'), \n",
    "(2, 11, 'Viewed'), \n",
    "(1, 2, 'Clicked') \n",
    "] \n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"AD_ID\", IntegerType(), True),\n",
    "  StructField(\"USER_ID\", IntegerType(), True),\n",
    "  StructField(\"ACTION\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_click = spark.createDataFrame(data, schema)\n",
    "df_click.display()\n",
    "\n",
    "#Ctr=Clicked/(Clicked+Viewed)\n",
    "df_click_grp = df_click.groupBy(\"AD_ID\").agg(sum(when(col(\"ACTION\") == \"Clicked\", 1)\\\n",
    "  .otherwise(0)).alias(\"clicked\"),sum(when(col(\"ACTION\") == \"Viewed\", 1)\\\n",
    "  .otherwise(0)).alias(\"Viewed\"))\n",
    "\n",
    "df_clk_ctr = df_click_grp.withColumn(\"CTR\", round(try_divide(col(\"clicked\"), col(\"Viewed\")+ col('clicked')),2))\n",
    "\n",
    "df_clk_ctr.selectExpr(\"AD_ID\", \"CTR as CTR\").orderBy(col(\"CTR\").desc()).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f6d93e4-fa5b-496f-b6de-1f3c054b1be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The try_divide function safely divides two columns, returning null if the denominator is zero or null, instead of raising an error.\n",
    "# Example:\n",
    "# try_divide(numerator, denominator)\n",
    "# If denominator is zero or null, result is null; otherwise, result is numerator/denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e34fd8a-0a4e-44a6-9e86-52753149cc4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Combine Two DF \n",
    "Write a Pyspark program to report the first name, last name, city, and state of each person in the \n",
    "Person dataframe. If the address of a personId is not present in the Address dataframe, \n",
    "report null instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90834caa-9556-42ca-98f8-eea457df9032",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema for the 'persons' table \n",
    "persons_schema = StructType([ \n",
    "StructField(\"personId\", IntegerType(), True), \n",
    "StructField(\"lastName\", StringType(), True), \n",
    "StructField(\"firstName\", StringType(), True) \n",
    "]) \n",
    "# Define schema for the 'addresses' table \n",
    "addresses_schema = StructType([ \n",
    "StructField(\"addressId\", IntegerType(), True), \n",
    "StructField(\"personId\", IntegerType(), True), \n",
    "StructField(\"city\", StringType(), True), \n",
    "StructField(\"state\", StringType(), True) \n",
    "]) \n",
    "# Define data for the 'persons' table \n",
    "persons_data = [ \n",
    "(1, 'Wang', 'Allen'), \n",
    "(2, 'Alice', 'Bob') \n",
    "] \n",
    "# Define data for the 'addresses' table \n",
    "addresses_data = [ \n",
    "(1, 2, 'New York City', 'New York'), \n",
    "(2, 3, 'Leetcode', 'California') \n",
    "]\n",
    "\n",
    "df_person = spark.createDataFrame(persons_data, persons_schema)\n",
    "df_address = spark.createDataFrame(addresses_data, addresses_schema)\n",
    "df_person.display()\n",
    "df_address.display()\n",
    "\n",
    "df_join = df_person.join(df_address, df_person.personId == df_address.personId, \"left\")\\\n",
    "    .select(\"firstName\",\"lastName\",\"city\",\"state\")\n",
    "       \n",
    "df_join.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a52f3f1f-d91d-447a-92dd-724092037eb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "04_Employees Earning More Than Their Managers \n",
    "Write a Pyspark program to find Employees Earning More Than Their \n",
    "Managers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf4754d9-2896-4503-9442-a7282fb8dc3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define the schema for the \"employees\" \n",
    "employees_schema = StructType([ \n",
    "StructField(\"id\", IntegerType(), True), \n",
    "StructField(\"name\", StringType(), True), \n",
    "StructField(\"salary\", IntegerType(), True), \n",
    "StructField(\"managerId\", IntegerType(), True) \n",
    "]) \n",
    "# Define data for the \"employees\" \n",
    "employees_data = [ \n",
    "(1, 'Joe', 70000, 3), \n",
    "(2, 'Henry', 80000, 4), \n",
    "(3, 'Sam', 60000, None), \n",
    "(4, 'Max', 90000, None) \n",
    "] \n",
    "\n",
    "df_emp = spark.createDataFrame(employees_data, employees_schema)\n",
    "df_emp.display()\n",
    "\n",
    "df_empl = df_emp.alias(\"emp\")\n",
    "df_mgr = df_emp.alias(\"mgr\")\n",
    "\n",
    "\n",
    "ddf_emp_mgr = df_empl.join(df_mgr, df_empl.managerId == df_mgr.id, \"left\")\n",
    "emp_sal = ddf_emp_mgr.filter(df_empl['salary'] > df_mgr['salary'])\n",
    "emp_sal.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14f87d48-77ff-43d7-b87e-b1bc41be0dd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Duplicate Emails \n",
    "Write a Pyspark program to report all the duplicate emails. \n",
    "Note that it's guaranteed that the email field is not NULL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c3af94b-a2a0-4528-ae6f-d6be76350189",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761862517710}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([ \n",
    "StructField(\"id\", IntegerType(), True), \n",
    "StructField(\"email\", StringType(), True)\n",
    "])  \n",
    "\n",
    "# Define data for the \"employees\" \n",
    "data = [ \n",
    "(1, 'a@b.com'), \n",
    "(2, 'c@d.com'), \n",
    "(3, 'a@b.com')\n",
    "] \n",
    "\n",
    "df_eml = spark.createDataFrame(data, schema)\n",
    "df_eml.display()\n",
    "\n",
    "df_email = df_eml.groupBy(col('email')).agg(count('email').alias(\"cnt\"))\n",
    "df_fileml = df_email.filter(col('cnt') > 1).select(\"email\")\n",
    "df_fileml.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce50569f-dbe3-49b1-b0e8-6f76d751e6d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "06_Customers Who Never Order - \n",
    "Write a Pyspark program to find all customers who never \n",
    "order anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68404706-cda6-42df-9f0d-4c19058be094",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema for the \"Customers\" \n",
    "customers_schema = StructType([ \n",
    "StructField(\"id\", IntegerType(), True), \n",
    "StructField(\"name\", StringType(), True) \n",
    "]) \n",
    "\n",
    "# Define data for the \"Customers\" \n",
    "customers_data = [ \n",
    "(1, 'Joe'), \n",
    "(2, 'Henry'), \n",
    "(3, 'Sam'), \n",
    "(4, 'Max') \n",
    "] \n",
    "\n",
    "# Define the schema for the \"Orders\" \n",
    "orders_schema = StructType([ \n",
    "StructField(\"id\", IntegerType(), True), \n",
    "StructField(\"customerId\", IntegerType(), True) \n",
    "]) \n",
    "\n",
    "# Define data for the \"Orders\" \n",
    "orders_data = [ \n",
    "(1, 3), \n",
    "(2, 1) \n",
    "]\n",
    "\n",
    "df_cust = spark.createDataFrame(customers_data, customers_schema)\n",
    "df_ord = spark.createDataFrame(orders_data, orders_schema)\n",
    "df_cust.display()\n",
    "df_ord.display()\n",
    "\n",
    "df_noorder = df_cust.join(df_ord, df_ord.customerId == df_cust.id, \"leftanti\")\n",
    "df_noorder.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88f9e891-04ec-4b87-ad83-f657f4f6c973",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "07_Rising Temperature - \n",
    "\n",
    "Write a solution to find all dates' Id with higher \n",
    "temperatures compared to its previous dates (yesterday). \n",
    "Return the result table in any order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5480e735-ce50-4b6e-b0eb-b0f7278d9c33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Define the schema for the \"Weather\" table \n",
    "weather_schema = StructType([ \n",
    "StructField(\"id\", IntegerType(), True), \n",
    "StructField(\"recordDate\", StringType(), True), \n",
    "StructField(\"temperature\", IntegerType(), True) \n",
    "]) \n",
    "\n",
    "\n",
    "# Define data for the \"Weather\" table \n",
    "weather_data = [ \n",
    "(1, '2015-01-01', 10), \n",
    "(2, '2015-01-02', 25), \n",
    "(3, '2015-01-03', 20), \n",
    "(4, '2015-01-04', 30)\n",
    "]\n",
    "\n",
    "df_temp = spark.createDataFrame(weather_data, weather_schema)\n",
    "df_temp.display()\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "win_spec= Window.orderBy(\"id\")\n",
    "prev_date = df_temp.withColumn(\"prev_temp\", lag(col(\"temperature\")).over(win_spec))\\\n",
    "  .filter(col('temperature') > col('prev_temp'))\\\n",
    "  .select(\"id\")\n",
    "  \n",
    "prev_date.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a6648c8-ca25-4428-9291-080e3a084d63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "08_Game Play Analysis I \n",
    "\n",
    "Write a solution to find the first login date for each player. \n",
    "Return the result table in any order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39758acf-19a0-441d-8b98-03a1486fd770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema for the \"Activity\" \n",
    "activity_schema = StructType([ \n",
    "StructField(\"player_id\", IntegerType(), True), \n",
    "StructField(\"device_id\", IntegerType(), True), \n",
    "StructField(\"event_date\", StringType(), True), \n",
    "StructField(\"games_played\", IntegerType(), True) \n",
    "]) \n",
    " \n",
    "# Define data for the \"Activity\" \n",
    "activity_data = [ \n",
    "(1, 2, '2016-03-01', 5), \n",
    "(1, 2, '2016-05-02', 6), \n",
    "(2, 3, '2017-06-25', 1), \n",
    "(3, 1, '2016-03-02', 0), \n",
    "(3, 4, '2018-07-03', 5) \n",
    "]\n",
    "\n",
    "df_login = spark.createDataFrame(activity_data, activity_schema)\n",
    "df_login.display()\n",
    "\n",
    "df_date = df_login.withColumn(\"event_date\", to_date(col(\"event_date\"),'yyyy-MM-dd'))\n",
    "df_date.display()\n",
    "\n",
    "win_spec = Window.partitionBy(col(\"player_id\")).orderBy(\"event_date\")\n",
    "log_dt = df_date.withColumn('rank',  dense_rank().over(win_spec)).filter(col('rank') == 1)\n",
    "log_dt.display()\n",
    "\n",
    "log_dt.selectExpr(\"player_id\", \"event_date as first_login\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd2ca845-4fd9-47d7-83e9-21eb2e6ebbb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "09_Game Play Analysis II \n",
    "\n",
    "Write a pyspark code that reports the device that is first \n",
    "logged in for each player. \n",
    "Return the result table in any order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75e220e-a5bc-4c9a-96c7-2a645017387b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema for the \"Activity\" \n",
    "activity_schema = StructType([ \n",
    "StructField(\"player_id\", IntegerType(), True), \n",
    "StructField(\"device_id\", IntegerType(), True), \n",
    "StructField(\"event_date\", StringType(), True), \n",
    "StructField(\"games_played\", IntegerType(), True) \n",
    "]) \n",
    "\n",
    "# Define data for the \"Activity\" \n",
    "activity_data = [ \n",
    "(1, 2, '2016-03-01', 5), \n",
    "(1, 2, '2016-05-02', 6), \n",
    "(2, 3, '2017-06-25', 1), \n",
    "(3, 1, '2016-03-02', 0), \n",
    "(3, 4, '2018-07-03', 5) \n",
    "]\n",
    "\n",
    "df_login = spark.createDataFrame(activity_data, activity_schema)\n",
    "df_login.display()\n",
    "\n",
    "df_date = df_login.withColumn(\"event_date\", to_date(col(\"event_date\"),'yyyy-MM-dd'))\n",
    "df_date.display()\n",
    "\n",
    "win_spec = Window.partitionBy(col(\"player_id\")).orderBy(\"event_date\")\n",
    "log_dt = df_date.withColumn('rank',  dense_rank().over(win_spec)).filter(col('rank') == 1)\n",
    "log_dt.display()\n",
    "\n",
    "log_dt.selectExpr(\"player_id\", \"device_id as first_login\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6890eb89-d387-4486-9e18-8bec5e28095e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "10_Employee Bonus \n",
    "\n",
    "Write a solution to report the name and bonus amount of \n",
    "each employee with a bonus less than 1000. \n",
    "Return the result table in any order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9760edd-cd28-461b-929e-015bfe761f52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema for the \"Employee\" \n",
    "employee_schema = StructType([ \n",
    "StructField(\"empId\", IntegerType(), True), \n",
    "StructField(\"name\", StringType(), True), \n",
    "StructField(\"supervisor\", IntegerType(), True), \n",
    "StructField(\"salary\", IntegerType(), True) \n",
    "]) \n",
    "\n",
    "# Define data for the \"Employee\" \n",
    "employee_data = [ \n",
    "(3, 'Brad', None, 4000), \n",
    "(1, 'John', 3, 1000), \n",
    "(2, 'Dan', 3, 2000), \n",
    "(4, 'Thomas', 3, 4000) \n",
    "] \n",
    "\n",
    "# Define the schema for the \"Bonus\" \n",
    "bonus_schema = StructType([ \n",
    "StructField(\"empId\", IntegerType(), True), \n",
    "StructField(\"bonus\", IntegerType(), True) \n",
    "]) \n",
    "\n",
    "# Define data for the \"Bonus\" \n",
    "bonus_data = [ \n",
    "(2, 500), \n",
    "(4, 2000) \n",
    "] \n",
    "\n",
    "df_emp = spark.createDataFrame(employee_data, employee_schema)\n",
    "df_bonus = spark.createDataFrame(bonus_data, bonus_schema)\n",
    "df_emp.display()\n",
    "df_bonus.display()\n",
    "\n",
    "emp_alias = df_emp.alias('e')\n",
    "df_bonus_alias = df_bonus.alias('b')\n",
    "\n",
    "\n",
    "df_emp_bon = emp_alias.join(df_bonus_alias, (col(\"e.empId\") == col(\"b.empId\")), \"inner\")\\\n",
    "  .filter(col('bonus') < 1000)\n",
    "\n",
    "emp_sel = df_emp_bon.select(col(\"e.empId\"), col(\"e.name\"), col(\"b.bonus\"))\n",
    "\n",
    "emp_sel.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3860e860-8b46-4268-9bf9-f842af074031",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "11_Find Customer Referee \n",
    "\n",
    "Find the names of the customer that are not referred by the \n",
    "customer with id = 2. \n",
    "Return the result table in any order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "667c129e-2d95-44ef-8718-24cbc5260689",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761878372704}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema for the Customer table \n",
    "schema = StructType([ \n",
    "StructField(\"id\", IntegerType(), True), \n",
    "StructField(\"name\", StringType(), True), \n",
    "StructField(\"referee_id\", IntegerType(), True) \n",
    "]) \n",
    "\n",
    "# Create an RDD with the data \n",
    "data = [ \n",
    "(1, 'Will', None), \n",
    "(2, 'Jane', None), \n",
    "(3, 'Alex', 2), \n",
    "(4, 'Bill', None), \n",
    "(5, 'Zack', 1), \n",
    "(6, 'Mark', 2) \n",
    "]\n",
    " \n",
    "df_ref = spark.createDataFrame(data, schema)\n",
    "df_ref.display()\n",
    "\n",
    "df_not_ref = df_ref.filter((col(\"referee_id\") != 2) | col(\"referee_id\").isNull()).select(\"name\")\n",
    "\n",
    "df_not_ref.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0cd2036-2f68-4483-862f-58f20cecdb14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "12_Cities With Completed Trades \n",
    "\n",
    "Write a pypsark code to retrieve the top three cities that \n",
    "have the highest number of completed trade orders listed in \n",
    "descending order. Output the city name and the \n",
    "corresponding number of completed trade orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "969e51ce-80bb-4b06-a655-b816e9fe2725",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema for the trades \n",
    "trades_schema = StructType([ \n",
    "StructField(\"order_id\", IntegerType(), True), \n",
    "StructField(\"user_id\", IntegerType(), True), \n",
    "StructField(\"price\", FloatType(), True), \n",
    "StructField(\"quantity\", IntegerType(), True), \n",
    "StructField(\"status\", StringType(), True), \n",
    "StructField(\"timestamp\", StringType(), True) \n",
    "]) \n",
    "\n",
    "# Define the schema for the users \n",
    "users_schema = StructType([ \n",
    "StructField(\"user_id\", IntegerType(), True), \n",
    "StructField(\"city\", StringType(), True), \n",
    "StructField(\"email\", StringType(), True), \n",
    "StructField(\"signup_date\", StringType(), True) \n",
    "]) \n",
    "\n",
    "# Create an RDD with the data for trades \n",
    "trades_data = [ \n",
    "(100101, 111, 9.80, 10, 'Cancelled', '2022-08-17 12:00:00'), \n",
    "(100102, 111, 10.00, 10, 'Completed', '2022-08-17 12:00:00'), \n",
    "(100259, 148, 5.10, 35, 'Completed', '2022-08-25 12:00:00'), \n",
    "(100264, 148, 4.80, 40, 'Completed', '2022-08-26 12:00:00'), \n",
    "(100305, 300, 10.00, 15, 'Completed', '2022-09-05 12:00:00'), \n",
    "(100400, 178, 9.90, 15, 'Completed', '2022-09-09 12:00:00'), \n",
    "(100565, 265, 25.60, 5, 'Completed', '2022-12-19 12:00:00') \n",
    "] \n",
    "\n",
    "# Create an RDD with the data for users \n",
    "users_data = [ \n",
    "(111, 'San Francisco', 'rrok10@gmail.com', '2021-08-03 12:00:00'), \n",
    "(148, 'Boston', 'sailor9820@gmail.com', '2021-08-20 12:00:00'), \n",
    "(178, 'San Francisco', 'harrypotterfan182@gmail.com', '2022-01-05 12:00:00'), \n",
    "(265, 'Denver', 'shadower_@hotmail.com', '2022-02-26 12:00:00'), \n",
    "(300, 'San Francisco', 'houstoncowboy1122@hotmail.com', '2022-06-30 12:00:00') \n",
    "] \n",
    "\n",
    "df_trades = spark.createDataFrame(trades_data, trades_schema)\n",
    "df_users = spark.createDataFrame(users_data, users_schema)\n",
    "df_trades.display()\n",
    "df_users.display()\n",
    "\n",
    "df_td_use_join = df_trades.alias('t').join(df_users.alias('u'), (col(\"t.user_id\") == col(\"u.user_id\")),\"inner\")\n",
    "df_td_use_join.display()\n",
    "\n",
    "\n",
    "df_filt = df_td_use_join.filter(col(\"status\") == 'Completed')\n",
    "\n",
    "df_city = df_filt.groupBy(\"u.city\").agg(count(\"city\").alias(\"count\"))\n",
    "  \n",
    "\n",
    "df_city.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e971c41b-b3ef-4d7c-a633-5352552b7c8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "13_Page With No Likes \n",
    "\n",
    "Write a pyspark code to return the IDs of the Facebook pages \n",
    "that have zero likes. The output should be sorted in \n",
    "ascending order based on the page IDs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0de6579-5556-4656-b46f-6e380b31ce6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema for the pages \n",
    "pages_schema = StructType([ \n",
    "StructField(\"page_id\", IntegerType(), True), \n",
    "StructField(\"page_name\", StringType(), True) \n",
    "]) \n",
    "\n",
    "# Define the schema for the page_likes table \n",
    "page_likes_schema = StructType([ \n",
    "StructField(\"user_id\", IntegerType(), True), \n",
    "StructField(\"page_id\", IntegerType(), True), \n",
    "StructField(\"liked_date\", StringType(), True) \n",
    "]) \n",
    "\n",
    "# Create an RDD with the data for pages \n",
    "pages_data = [ \n",
    "(20001, 'SQL Solutions'), \n",
    "(20045, 'Brain Exercises'), \n",
    "(20701, 'Tips for Data Analysts') \n",
    "] \n",
    "\n",
    "# Create an RDD with the data for page_likes table \n",
    "page_likes_data = [ \n",
    "(111, 20001, '2022-04-08 00:00:00'), \n",
    "(121, 20045, '2022-03-12 00:00:00'), \n",
    "(156, 20001, '2022-07-25 00:00:00') \n",
    "]\n",
    "\n",
    "df_pages = spark.createDataFrame(pages_data, pages_schema)\n",
    "df_page_likes = spark.createDataFrame(page_likes_data, page_likes_schema)\n",
    "df_pages.display()\n",
    "df_page_likes.display()\n",
    "\n",
    "df_page_join = df_pages.alias('p').join(df_page_likes.alias('l'), (col(\"p.page_id\") == col(\"l.page_id\")), \"leftanti\")\n",
    "df_page_join.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0297ed0-6a06-49d2-87ea-40f73e1bed49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "14_Purchasing Activity by Product \n",
    "\n",
    "Type \n",
    "We have been given purchasing activity DF and we need \n",
    "to find out cumulative purchases of each product over \n",
    "time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71d214b6-8966-4ff0-8bfd-84a6e63b813d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define schema for the DataFrame \n",
    "schema = StructType([ \n",
    "StructField(\"order_id\", IntegerType(), True), \n",
    "StructField(\"product_type\", StringType(), True), \n",
    "StructField(\"quantity\", IntegerType(), True), \n",
    "StructField(\"order_date\", StringType(), True), \n",
    "]) \n",
    " \n",
    "# Define data \n",
    "data = [ \n",
    "(213824, 'printer', 20, \"2022-06-27 \"), \n",
    "(212312, 'hair dryer', 5, \"2022-06-28 \"), \n",
    "(132842, 'printer', 18, \"2022-06-28 \"),   \n",
    "(284730, 'standing lamp', 8, \"2022-07-05 \") \n",
    "] \n",
    "\n",
    "df_cum = spark.createDataFrame(data, schema)\n",
    "df_cum.display()\n",
    "\n",
    "# df_pur_dt = df_cum.groupBy(col(\"order_id\")).agg(sum(\"quantity\").alias(\"total_quantity\"))\\\n",
    "#   .selectExpr(\"order_date\", \"product_type\", \"total_quantity as cum_purchased\")\n",
    "# df_pur_dt.display()\n",
    "\n",
    "\n",
    "win_spec = Window.partitionBy(\"product_type\").orderBy(\"order_date\").rowsBetween(Window.unboundedPreceding, 0) \n",
    "\n",
    "# Add a new column 'cumulative_purchases' representing the cumulative sum \n",
    "\n",
    "result_df = df_cum.withColumn(\"cumulative_purchases\", sum(\"quantity\").over(win_spec)) \n",
    "result_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f2fed76-afda-4fcb-9246-0dc46a063125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**15_Teams Power Users \n",
    "\n",
    "Write a pyspark code to identify the top 2 Power Users \n",
    "who sent the highest number of messages on Microsoft \n",
    "Teams in August 2022. Display the IDs of these 2 users \n",
    "along with the total number of messages they sent. \n",
    "Output the results in descending order based on the \n",
    "count of the messages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9922d9b-2458-4e83-9a93-9e9234793c26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([ \n",
    "StructField(\"message_id\", IntegerType(), True), \n",
    "StructField(\"sender_id\", IntegerType(), True), \n",
    "StructField(\"receiver_id\", IntegerType(), True), \n",
    "StructField(\"content\", StringType(), True), \n",
    "StructField(\"sent_date\", StringType(), True), \n",
    "]) \n",
    "\n",
    "# Define the data \n",
    "data = [ \n",
    "(901, 3601, 4500, 'You up?', '2022-08-03 00:00:00'), \n",
    "(902, 4500, 3601, 'Only if you\\'re buying', '2022-08-03 00:00:00'), \n",
    "(743, 3601, 8752, 'Let\\'s take this offline', '2022-06-14 00:00:00'), \n",
    "(922, 3601, 4500, 'Get on the call', '2022-08-10 00:00:00'), \n",
    "]\n",
    "\n",
    "df_msg = spark.createDataFrame(data, schema)\n",
    "df_msg.display()\n",
    "\n",
    "df_msg_date= df_msg.filter(col(\"sent_date\").like(\"2022-08%\"))\n",
    "\n",
    "df_msg_cnt =df_msg_date.groupBy(\"sender_id\").agg(count(\"message_id\").alias(\"cnt\"))\n",
    "\n",
    "df_msg_cnt.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67f8244e-dbee-4d7a-ad9c-f6caa4134819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "16_Select in pyspark \n",
    "\n",
    "Write a pyspark code perform below function \n",
    "● Write a pyspark code to get all employee detail. \n",
    "● Write a query to get only \"FirstName\" column from emp_df \n",
    "● Write a Pyspark code to get FirstName in upper case as \"First \n",
    "Name\". \n",
    "● Write a pyspark code to get FirstName in lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39fe05d7-1815-4e3f-813b-e761407e43b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "17_Select in pyspark \n",
    "Write a pyspark code perform below function\n",
    "\n",
    "● Write a pyspark code for combine FirstName and LastName \n",
    "and display it as \"Name\" (also include white space between \n",
    "first name & last name) \n",
    "● Select employee detail whose name is \"Vikas\" \n",
    "● Get all employee detail from EmployeeDetail table whose \n",
    "\"FirstName\" start with letter 'a'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5a53dfb-e7fd-4161-a415-639eb4109114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "18_Select in pyspark \n",
    "\n",
    "Write a pyspark code perform below function \n",
    "● Get all employee details from EmployeeDetail table whose \n",
    "\"FirstName\" contains 'k' \n",
    "●  Get all employee details from EmployeeDetail table whose \n",
    "\"FirstName\" end with 'h' \n",
    "● Get all employee detail from EmployeeDetail table whose \n",
    "\"FirstName\" start with \n",
    "● Get all employee detail from EmployeeDetail table whose \n",
    "\"FirstName\" start with any single character between 'a-p'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0937136-68b6-40df-8305-3decd2e94213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "19_Select in pyspark \n",
    "\n",
    "Write a pyspark code perform below function \n",
    "●  Get all employee detail from emp_df whose \"Gender\" end \n",
    "with 'le' and contain 4 letters. The Underscore(_) Wildcard \n",
    "Character represents any single character. \n",
    "●  Get all employee detail from EmployeeDetail table whose \n",
    "\"FirstName\" start with # 'A' and contain 5 letters. \n",
    "● Get all unique \"Department\" from EmployeeDetail table. \n",
    "● Get the highest \"Salary\" from EmployeeDetail table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df66a272-0a00-4f06-88ea-946d63700326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "20_Date in pyspark \n",
    "\n",
    "Write a pyspark code perform below function \n",
    "● Get the lowest \"Salary\" from EmployeeDetail table. \n",
    "● Show \"JoiningDate\" in \"dd mmm yyyy\" format, ex- \"15 Feb \n",
    "2013\" \n",
    "● Show \"JoiningDate\" in \"yyyy/mm/dd\" format, ex- \"2013/02/15\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd1f5ad4-e9a9-4591-8490-d3e8ca8d23d7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761960436016}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "data = [ \n",
    "[1, \"Vikas\", \"Ahlawat\", 600000.0, \"2013-02-15 11:16:28.290\", \"IT\", \n",
    "\"Male\"], \n",
    "[2, \"nikita\", \"Jain\", 530000.0, \"2014-01-09 17:31:07.793\", \"HR\", \"Female\"], \n",
    "[3, \"Ashish\", \"Kumar\", 1000000.0, \"2014-01-09 10:05:07.793\", \"IT\", \n",
    "\"Male\"], \n",
    "[4, \"Nikhil\", \"Sharma\", 480000.0, \"2014-01-09 09:00:07.793\", \"HR\", \n",
    "\"Male\"], \n",
    "[5, \"anish\", \"kadian\", 500000.0, \"2014-01-09 09:31:07.793\", \"Payroll\", \n",
    "\"Male\"], \n",
    "] \n",
    "\n",
    "# Create a schema for the DataFrame \n",
    "schema = StructType([ \n",
    "StructField(\"EmployeeID\", IntegerType(), True), \n",
    "StructField(\"First_Name\", StringType(), True), \n",
    "StructField(\"Last_Name\", StringType(), True), \n",
    "StructField(\"Salary\", DoubleType(), True), \n",
    "StructField(\"Joining_Date\", StringType(), True), \n",
    "StructField(\"Department\", StringType(), True), \n",
    "StructField(\"Gender\", StringType(), True) \n",
    "])\n",
    "\n",
    "df_emp = spark.createDataFrame(data, schema)\n",
    "df_emp.display()\n",
    "\n",
    "\n",
    "def emp_details(df, columns):\n",
    "    columns = df.columns\n",
    "    df = df.withColumn(df.columns[1], upper(df.columns[1]))\\\n",
    "        .withColumn(df.columns[2], lower(df.columns[2]))\\\n",
    "        .withColumn(\"full_name\", concat_ws(' ', col(df.columns[1]), col(df.columns[2])))\\\n",
    "        .select(columns + ['full_name'])\n",
    "    return df   \n",
    "   \n",
    "emp_df = emp_details(df_emp, df_emp.columns)\n",
    "emp_df.display()\n",
    "\n",
    "\n",
    "#17 - filter based on names\n",
    "\n",
    "def name_filter(df):\n",
    "    columns = df.columns\n",
    "    df = df.filter((col(\"First_Name\") == \"Vikas\") | (col(\"Last_name\").startswith(\"J\")))\n",
    "    return df\n",
    "\n",
    "df_names = name_filter(df_emp)\n",
    "display(df_names)\n",
    "\n",
    "\n",
    "#18 - filter based on name pattern\n",
    "\n",
    "def name_pattern(df):\n",
    "    columns = df.columns\n",
    "    df = df.filter(col(\"First_Name\").contains(\"k\") | (col(\"First_Name\").endswith('h') | (col(\"First_name\").rlike(r\"^[a-p]\"))))\n",
    "    return df\n",
    "\n",
    "n_pattern = name_pattern(df_emp)\n",
    "display(n_pattern)\n",
    "\n",
    "# 19 - filter name based on 'contains' characters\n",
    "\n",
    "def name_contains(df):\n",
    "    columns = df.columns\n",
    "    df = df.filter(((col(\"Gender\").endswith(\"le\")) & (length(col(\"Gender\")) == 4)) | (col(\"First_Name\").startswith(\"A\") & (length(col(\"First_Name\")) == 5)))\n",
    "\n",
    "    return df     \n",
    "\n",
    "df_cont = name_contains(df_emp)\n",
    "display(df_cont)\n",
    "\n",
    "# df1 = df_emp.groupby(col(\"Department\")).agg(max(col(\"Salary\")).alias(\"max_salary\"))\n",
    "# df1.display()\n",
    "\n",
    "# 20 - DATE FUNCTIONS\n",
    "\n",
    "def emp_lowsal(df):\n",
    "    df = df.agg(max(col(\"Salary\")).alias(\"max_salary\"), min(col(\"Salary\")).alias(\"Lowsalary\"))\n",
    "    return df\n",
    "\n",
    "min_max_sal = emp_lowsal(df_emp)\n",
    "display(min_max_sal)\n",
    "\n",
    "\n",
    "# def date_filter(df, columns, dateformat = [\"yyyy-MM-dd HH:mm:ss.SSS\"]):\n",
    "#     columns = df.columns\n",
    "#     for col in df.columns:\n",
    "#         if col == \"Joining_Date\":\n",
    "#             df = df.withColumn(\"Joining_Date_new\", date_format(to_timestamp(col(\"Joining_Date\"),\"yyyy-MM-dd HH:mm:ss.SSS\"),'dd mmm yyyy'))\n",
    "#         return df\n",
    "\n",
    "# df_date = date_filter(df_emp, df_emp.columns)\n",
    "# display(df_date)\n",
    "\n",
    "df_date_fmt = df_emp.withColumn(\"Joining_Date_fmt\", date_format(to_timestamp(col(\"Joining_Date\"), \"yyyy-MM-dd HH:mm:ss.SSS\"), \"dd MMM yyyy\"))\\\n",
    "        .withColumn(\"join_fmt_new\", to_date(to_timestamp(col(\"Joining_Date\"), \"yyyy-MM-dd HH:mm:ss.SSS\"), \"yyyy-MM-dd\"))\n",
    "display(df_date_fmt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ca25361-21cc-4887-b50e-30082a10d452",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_date_fmt = df_emp.withColumn(\n",
    "    \"Joining_Date_fmt\",\n",
    "    date_format(to_timestamp(col(\"Joining_Date\"), \"yyyy-MM-dd HH:mm:ss.SSS\"), \"dd MMM yyyy\"))\\\n",
    "        .withColumn(\"join_fmt_new\", to_date(to_timestamp(col(\"Joining_Date\"), \"yyyy-MM-dd HH:mm:ss.SSS\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "display(df_date_fmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ac8c2b5-7cdd-4750-9155-57fb4d91fd50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "21_Date in pyspark \n",
    "\n",
    "Write a pyspark code perform below function \n",
    "● Get only Year part of \"JoiningDate\" \n",
    "● Get only Month part of \"JoiningDate\". \n",
    "● Get only date part of \"JoiningDate\". \n",
    "● Get the current system date using DataFrame API \n",
    "● Get the current UTC date and time using DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "550d377a-b9c6-40c7-a669-9e4ce23a9c73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def date_part_extract(df):\n",
    "    df = df.withColumn(\"year\", year(col(\"Joining_Date\")))\\\n",
    "        .withColumn(\"Month\", month(col(\"Joining_Date\")))\\\n",
    "        .withColumn(\"Day\", dayofmonth(col(\"Joining_Date\")))\\\n",
    "        .withColumn(\"current_date\", current_date())\\\n",
    "        .withColumn(\"current_timestamp\", current_timestamp())\n",
    "    return df\n",
    "\n",
    "df_date_part = date_part_extract(df_date_fmt)\n",
    "display(df_date_part)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b44979e0-5840-46b6-9fb5-82f087032a28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "22_Date in pyspark \n",
    "\n",
    "Write a pyspark code perform below function \n",
    "● Get the first name, current date, joiningdate and diff between \n",
    "current date and joining date in months. \n",
    "● Get the first name, current date, joiningdate and diff between \n",
    "current date and joining date in days. \n",
    "● Get all employee details from EmployeeDetail table whose \n",
    "joining year is 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52164cb7-9606-4bde-9766-21754cfb7223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [ \n",
    "[1, \"Vikas\", \"Ahlawat\", 600000.0, \"2013-02-15 11:16:28.290\", \"IT\", \"Male\"], \n",
    "[2, \"nikita\", \"Jain\", 530000.0, \"2014-01-09 17:31:07.793\", \"HR\", \"Female\"], \n",
    "[3, \"Ashish\", \"Kumar\", 1000000.0, \"2014-01-09 10:05:07.793\", \"IT\", \"Male\"], \n",
    "[4, \"Nikhil\", \"Sharma\", 480000.0, \"2014-01-09 09:00:07.793\", \"HR\", \"Male\"], \n",
    "[5, \"anish\", \"kadian\", 500000.0, \"2014-01-09 09:31:07.793\", \"Payroll\", \"Male\"], \n",
    "] \n",
    "\n",
    "# Create a schema for the DataFrame \n",
    "schema = StructType([ \n",
    "StructField(\"EmployeeID\", IntegerType(), True), \n",
    "StructField(\"First_Name\", StringType(), True), \n",
    "StructField(\"Last_Name\", StringType(), True), \n",
    "StructField(\"Salary\", DoubleType(), True), \n",
    "StructField(\"Joining_Date\", StringType(), True), \n",
    "StructField(\"Department\", StringType(), True), \n",
    "StructField(\"Gender\", StringType(), True) \n",
    "])\n",
    "\n",
    "df_dt = spark.createDataFrame(data, schema)\n",
    "df_dt.display()\n",
    "\n",
    "def date_dif(df):\n",
    "  df = df_emp.filter(year(to_timestamp(col(\"Joining_Date\"), \"yyyy-MM-dd HH:mm:ss.SSS\")) == 2013).selectExpr(\n",
    "                     \"First_Name\", \n",
    "                     \"current_date() as current_date\", \n",
    "                     \"Joining_Date\", \n",
    "                     \"round(months_between(current_date(), to_timestamp(Joining_Date, 'yyyy-MM-dd HH:mm:ss.SSS')),2) as months_diff\",\n",
    "                     \"date_diff(current_date(), to_timestamp(Joining_Date, 'yyyy-MM-dd HH:mm:ss.SSS')) as days_diff\")            \n",
    "      \n",
    "  return df\n",
    "\n",
    "df_date_diff = date_dif(df_dt)\n",
    "display(df_date_diff)\n",
    "\n",
    "\n",
    "# from pyspark.sql.functions import year, to_timestamp, col\n",
    "\n",
    "# df_filtered_year = df_emp.filter(year(\"Joining_Date\")== 2013)\n",
    "\n",
    "# display(df_filtered_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6af2af6a-b1b7-423f-a3b1-c4e2ef178706",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, months_between, col, to_timestamp\n",
    "\n",
    "def date_dif(df):\n",
    "    df = df.withColumn(\n",
    "        \"current_date\", current_date()\n",
    "    ).withColumn(\n",
    "        \"Joining_Date_ts\", to_timestamp(col(\"Joining_Date\"), \"yyyy-MM-dd HH:mm:ss.SSS\")\n",
    "    ).withColumn(\n",
    "        \"months_diff\", months_between(col(\"current_date\"), col(\"Joining_Date_ts\"))\n",
    "    ).select(\n",
    "        \"First_Name\", \"current_date\", \"Joining_Date\", \"months_diff\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df_date_diff = date_dif(df_dt)\n",
    "display(df_date_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0026cdb6-91c1-4f38-a8dc-18ceb8ebb803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "23_Date in pyspark \n",
    "\n",
    "Write a pyspark code perform below function \n",
    "● Get all employee details from EmployeeDetail table whose \n",
    "joining month is Jan(1). \n",
    "● Get all employee details from EmployeeDetail table whose \n",
    "joining date between 2013-01-01\" and \"2013-12-01\". \n",
    "● Get how many employee exist in \"EmployeeDetail\" table. \n",
    "● Select all employee detail with First name \"Vikas\",\"Ashish\", and \n",
    "\"Nikhil\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64378496-a02f-4de4-90f9-01a5a78d2bec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [ \n",
    "[1, \"Vikas\", \"Ahlawat\", 600000.0, \"2013-02-15 11:16:28.290\", \"IT\", \"Male\"], \n",
    "[2, \"nikita\", \"Jain\", 530000.0, \"2014-01-09 17:31:07.793\", \"HR\", \"Female\"], \n",
    "[3, \"Ashish\", \"Kumar\", 1000000.0, \"2014-01-09 10:05:07.793\", \"IT\", \"Male\"], \n",
    "[4, \"Nikhil\", \"Sharma\", 480000.0, \"2014-01-09 09:00:07.793\", \"HR\", \"Male\"], \n",
    "[5, \"anish\", \"kadian\", 500000.0, \"2014-01-09 09:31:07.793\", \"Payroll\", \"Male\"], \n",
    "] \n",
    "\n",
    "# Create a schema for the DataFrame \n",
    "schema = StructType([ \n",
    "StructField(\"EmployeeID\", IntegerType(), True), \n",
    "StructField(\"First_Name\", StringType(), True), \n",
    "StructField(\"Last_Name\", StringType(), True), \n",
    "StructField(\"Salary\", DoubleType(), True), \n",
    "StructField(\"Joining_Date\", StringType(), True), \n",
    "StructField(\"Department\", StringType(), True), \n",
    "StructField(\"Gender\", StringType(), True) \n",
    "]) \n",
    "\n",
    "df_date_det = spark.createDataFrame(data, schema)\n",
    "df_date_det.display()\n",
    "\n",
    "from pyspark.sql.functions import current_date, months_between, col, months\n",
    "\n",
    "def date_det(df):\n",
    "    columns = df.columns\n",
    "    df = df.filter((month(\"Joining_Date\") == 1))\\\n",
    "        .select(*columns)\n",
    "    return df\n",
    "\n",
    "df_date_det = date_det(df_date_det)\n",
    "display(df_date_det)\n",
    "\n",
    "def date_det1(df):\n",
    "    columns = df.columns\n",
    "    df = df.filter(col(\"Joining_Date\").between(\"2013-01-01\", \"2013-12-01\"))\\\n",
    "        .select(*columns)\n",
    "    return df\n",
    "\n",
    "df_date = date_det1(df_emp)\n",
    "display(df_date)\n",
    "\n",
    "df_cnt = df_emp.agg(count(\"EmployeeID\").alias(\"empcnt\"))\n",
    "display(df_cnt)\n",
    "\n",
    "df_fname =df_emp.filter(col(\"First_Name\").isin(\"Vikas\",\"Ashish\",\"Nikhil\"))\n",
    "df_fname.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50254e0f-1d46-46e6-ab4b-eeee22c6150b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "24_Trim and case in pyspark \n",
    "\n",
    "Write a pyspark code perform below function \n",
    "● Select all employee detail with First name not in \n",
    "\"Vikas\",\"Ashish\", and \"Nikhil\". \n",
    "● Select first name from \"EmployeeDetail\" df after removing \n",
    "white spaces from right side \n",
    "● Select first name from \"EmployeeDetail\" table after removing \n",
    "white spaces from left side \n",
    "● Display first name and Gender as M/F.(if male then M, if \n",
    "Female then F) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42e5912b-71ed-49d3-85d6-cf9f41976155",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761972253352}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [ \n",
    "[1, \"Vikas\", \"Ahlawat\", 600000.0, \"2013-02-15 11:16:28.290\", \"IT\", \"Male\"], \n",
    "[2, \"nikita\", \"Jain\", 530000.0, \"2014-01-09 17:31:07.793\", \"HR\", \"Female\"], \n",
    "[3, \"Ashish\", \"Kumar\", 1000000.0, \"2014-01-09 10:05:07.793\", \"IT\", \"Male\"], \n",
    "[4, \"Nikhil\", \"Sharma\", 480000.0, \"2014-01-09 09:00:07.793\", \"HR\", \"Male\"], \n",
    "[5, \"anish\", \"kadian\", 500000.0, \"2014-01-09 09:31:07.793\", \"Payroll\", \"Male\"], \n",
    "] \n",
    "\n",
    "# Create a schema for the DataFrame \n",
    "schema = StructType([ \n",
    "StructField(\"EmployeeID\", IntegerType(), True), \n",
    "StructField(\"First_Name\", StringType(), True), \n",
    "StructField(\"Last_Name\", StringType(), True), \n",
    "StructField(\"Salary\", DoubleType(), True), \n",
    "StructField(\"Joining_Date\", StringType(), True), \n",
    "StructField(\"Department\", StringType(), True), \n",
    "StructField(\"Gender\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_not_fname = spark.createDataFrame(data, schema)\n",
    "df_not_fname.display()\n",
    "\n",
    "def not_fname(df):\n",
    "    columns = df.columns\n",
    "    df = df.filter(~df[\"First_Name\"].isin(\"Vikas\",\"Ashish\", \"Nikhil\"))\\\n",
    "        .withColumn(\"Gender\", when(df[\"Gender\"] == 'Male', 'M').otherwise('F'))\\\n",
    "        .select(rtrim(ltrim(\"First_Name\")).alias(\"First_Name\"), \"Joining_Date\", \"Department\", \"Gender\")\n",
    "    return df\n",
    "\n",
    "not_fname_df= not_fname(df_not_fname)\n",
    "not_fname_df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87a1be7f-478b-47a2-9a3a-15ee07dad989",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "25_operator in pyspark \n",
    "\n",
    "Write a pyspark code perform below function \n",
    "● Select first name from \"EmployeeDetail\" table prifixed with \n",
    "\"Hello \" \n",
    "● Get employee details from \"EmployeeDetail\" table whose \n",
    "Salary greater than 600000 \n",
    "● Get employee details from \"EmployeeDetail\" table whose \n",
    "Salary less than 700000 \n",
    "● Get employee details from \"EmployeeDetail\" table whose \n",
    "Salary between 500000 than 600000 \n",
    "Difficult Level : EASY \n",
    "DataFrame: \n",
    "● Select second highest salary from \"EmployeeDetail\" table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04e33a08-3363-4b8a-adee-b7fb76589bb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import col, concat, lit\n",
    "\n",
    "def emp_sal(df):\n",
    "    df_greater = df.withColumn(\"First_Name\", concat(lit(\"Hello\"), col(\"First_Name\")))\\\n",
    "                .filter(col(\"Salary\") >= 600000)     \n",
    "    df_less = df.filter(col(\"Salary\") < 700000)\n",
    "    df_between = df.filter(col(\"Salary\").between(500000, 600000))   \n",
    "    df_sec_sal = df.withColumn(\"sec_sal\", row_number().over(Window.orderBy(desc(\"Salary\"))))\\\n",
    "                .filter(col(\"sec_sal\") == 2) # (or) orderBy(col(\"Salary\").desc()).limit(2).offset(1)\n",
    "\n",
    "    return df_greater, df_less, df_between, df_sec_sal\n",
    "\n",
    "df_greater, df_less, df_between, df_sec_sal = emp_sal(df_emp)\n",
    "\n",
    "display(df_greater)\n",
    "display(df_less)\n",
    "display(df_between)\n",
    "display(df_sec_sal)\n",
    "\n",
    " # df_sec_highsal = df.orderBy(col(\"Salary\").desc()).limit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5614cdb9-bbb2-442b-ad48-fc824ea8f6ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "26_groupby in pyspark \n",
    "Write a pyspark code perform below function \n",
    "\n",
    "●  Write the query to get the department and department wise \n",
    "  total(sum) salary from \"EmployeeDetail\" table. \n",
    "●  Write the query to get the department and department wise \n",
    "total(sum) salary, display it in ascending order according to \n",
    "salary. \n",
    "● Write the query to get the department and department wise \n",
    "total(sum) salary, display it in descending order according to \n",
    "salary. \n",
    "●  Write the query to get the department, total no. of \n",
    "departments, total(sum) salary with respect to department \n",
    "from \"EmployeeDetail\" table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a08b6a3-e08b-43a7-8b93-b4ac933a78ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating Spark Session \n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, lit, sum, count, row_number, desc\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "data = [ \n",
    " [1, \"Vikas\", \"Ahlawat\", 600000.0, \"2013-02-15 11:16:28.290\", \"IT\", \"Male\"], \n",
    " [2, \"nikita\", \"Jain\", 530000.0, \"2014-01-09 17:31:07.793\", \"HR\", \"Female\"], \n",
    " [3, \"Ashish\", \"Kumar\", 1000000.0, \"2014-01-09 10:05:07.793\", \"IT\", \"Male\"], \n",
    " [4, \"Nikhil\", \"Sharma\", 480000.0, \"2014-01-09 09:00:07.793\", \"HR\", \"Male\"], \n",
    " [5, \"anish\", \"kadian\", 500000.0, \"2014-01-09 09:31:07.793\", \"Payroll\", \"Male\"], \n",
    "]\n",
    "\n",
    "# Create a schema for the DataFrame \n",
    "schema = StructType([ \n",
    " StructField(\"EmployeeID\", IntegerType(), True), \n",
    " StructField(\"First_Name\", StringType(), True), \n",
    " StructField(\"Last_Name\", StringType(), True), \n",
    " StructField(\"Salary\", DoubleType(), True), \n",
    " StructField(\"Joining_Date\", StringType(), True), \n",
    " StructField(\"Department\", StringType(), True), \n",
    " StructField(\"Gender\", StringType(), True) \n",
    "]) \n",
    " \n",
    "df_detemp = spark.createDataFrame(data, schema)\n",
    "df_detemp.display()\n",
    "\n",
    "\n",
    "def emp_det(df):\n",
    "    df_sal_tot = df.groupBy('Department').agg((sum('Salary')).alias('Sum_Of_salary'))\n",
    "    df_asc = df_sal_tot.orderBy(col(\"Sum_Of_salary\").asc())\n",
    "    df_desc = df_sal_tot.orderBy(col(\"Sum_Of_salary\").desc())\n",
    "    df_dept = df.groupBy(\"Department\").agg(count(\"Department\").alias(\"No_of_dept\"),(sum(\"Salary\")).alias(\"Total_Salary\"))\n",
    "    return df_sal_tot, df_asc, df_desc, df_dept\n",
    "\n",
    "# df_name_var = [df_sal_tot, df_asc, df_desc, df_dept]\n",
    "\n",
    "df_sal_tot, df_asc, df_desc, df_dept = emp_det(df_detemp)\n",
    "df_sal_tot.display()\n",
    "df_asc.display()\n",
    "df_desc.display()\n",
    "df_dept.display()\n",
    "\n",
    "\n",
    "\n",
    "# for i in df_name_var:   \n",
    "#     i.display()\n",
    "\n",
    "# display(df_sal, df_asc, df_desc, df_dept)\n",
    "\n",
    "#display all the dataframe dynamically without defining the function\n",
    "# import sys\n",
    "\n",
    "# def display_all_dataframes():\n",
    "#     for df_name, df_val in globals().items():\n",
    "#         if 'DataFrame' in str(type(df_val)):\n",
    "#             display(df_val)\n",
    "\n",
    "# display_all_dataframes() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8cc6fd5-5847-4d72-8d70-f3776fde6f77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "27_groupby in pyspark \n",
    "Write a pyspark code perform below function \n",
    "\n",
    "●  46. Get department wise average salary from \n",
    "\"EmployeeDetail\" table order by salary ascending \n",
    "●  47. Get department wise maximum salary from \n",
    "\"EmployeeDetail\" table order by salary ascending \n",
    "● 48. Get department wise minimum salary from \n",
    "\"EmployeeDetail\" table order by salary ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e82bf88-341e-4fb7-9f4d-e5432843433e",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762386264239}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, min, max\n",
    "\n",
    "data = [ \n",
    " [1, \"Vikas\", \"Ahlawat\", 600000.0, \"2013-02-15 11:16:28.290\", \"IT\", \"Male\"], \n",
    " [2, \"nikita\", \"Jain\", 530000.0, \"2014-01-09 17:31:07.793\", \"HR\", \"Female\"], \n",
    " [3, \"Ashish\", \"Kumar\", 1000000.0, \"2014-01-09 10:05:07.793\", \"IT\", \"Male\"], \n",
    " [4, \"Nikhil\", \"Sharma\", 480000.0, \"2014-01-09 09:00:07.793\", \"HR\", \"Male\"], \n",
    " [5, \"anish\", \"kadian\", 500000.0, \"2014-01-09 09:31:07.793\", \"Payroll\", \"Male\"], \n",
    "]  \n",
    " \n",
    "# Create a schema for the DataFrame \n",
    "schema = StructType([ \n",
    " StructField(\"EmployeeID\", IntegerType(), True), \n",
    " StructField(\"First_Name\", StringType(), True), \n",
    " StructField(\"Last_Name\", StringType(), True), \n",
    " StructField(\"Salary\", DoubleType(), True), \n",
    " StructField(\"Joining_Date\", StringType(), True), \n",
    " StructField(\"Department\", StringType(), True), \n",
    " StructField(\"Gender\", StringType(), True) \n",
    "])\n",
    "\n",
    "df_dept_sal = spark.createDataFrame(data, schema)\n",
    "df_dept_sal.display()\n",
    "\n",
    "def dept_salary(df):\n",
    "    df_sal_avg = df.groupBy(\"Department\").agg(avg('Salary').alias(\"avg_saalry\")).orderBy(col(\"avg_saalry\").asc())\n",
    "    df_sal_min = df.groupBy(\"Department\").agg(min('Salary').alias(\"min_salary\")).orderBy(col(\"min_salary\").asc())\n",
    "    df_sal_max = df.groupBy(\"Department\").agg(max('Salary').alias(\"max_salary\")).orderBy(col(\"max_salary\").asc())\n",
    "    return df_sal_avg, df_sal_min, df_sal_max\n",
    "\n",
    "df_sal_avg, df_sal_min, df_sal_max = dept_salary(df_dept_sal)\n",
    "\n",
    "df_sal_avg.display()\n",
    "df_sal_min.display()\n",
    "df_sal_max.display()\n",
    "\n",
    "\n",
    "\n",
    "# for i in df_name:\n",
    "#     i.display()\n",
    "\n",
    "# display(df_sal_avg, df_sal_min, df_sal_max)\n",
    "\n",
    "\n",
    "# not working for the individual cell to run all the dataframe\n",
    "\n",
    "# def display_add_df():\n",
    "#     for df_name, df_val in globals().items():\n",
    "#         if 'DataFrame' in str(type(df_val)):\n",
    "#             display(df_val)\n",
    "\n",
    "# display_add_df()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d60c73-3c90-40ca-a5c5-9f858efb7ba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "28_Join_in_pyspark \n",
    "\n",
    "Write a pyspark code perform below function \n",
    "●  Write down the query to fetch Project name assign to more \n",
    "than one Employee \n",
    "● Get employee name, project name order by firstname from \n",
    "\"EmployeeDetail\" and\"ProjectDetail\" for those employee which \n",
    "have assigned project already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c515a44-2239-49dd-9624-c06e49813941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [ \n",
    " [1, \"Vikas\", \"Ahlawat\", 600000.0, \"2013-02-15 11:16:28.290\", \"IT\", \"Male\"], \n",
    " [2, \"nikita\", \"Jain\", 530000.0, \"2014-01-09 17:31:07.793\", \"HR\", \"Female\"], \n",
    " [3, \"Ashish\", \"Kumar\", 1000000.0, \"2014-01-09 10:05:07.793\", \"IT\", \"Male\"], \n",
    " [4, \"Nikhil\", \"Sharma\", 480000.0, \"2014-01-09 09:00:07.793\", \"HR\", \"Male\"], \n",
    " [5, \"anish\", \"kadian\", 500000.0, \"2014-01-09 09:31:07.793\", \"Payroll\", \"Male\"]\n",
    "]  \n",
    "\n",
    "# Create a schema for the DataFrame \n",
    "schema = StructType([ \n",
    " StructField(\"EmployeeID\", IntegerType(), True), \n",
    " StructField(\"First_Name\", StringType(), True), \n",
    " StructField(\"Last_Name\", StringType(), True), \n",
    " StructField(\"Salary\", DoubleType(), True), \n",
    " StructField(\"Joining_Date\", StringType(), True), \n",
    " StructField(\"Department\", StringType(), True), \n",
    " StructField(\"Gender\", StringType(), True) \n",
    "]) \n",
    " \n",
    "df_e = spark.createDataFrame(data, schema)\n",
    "df_e.display()\n",
    " \n",
    "pro_schema = StructType([ \n",
    " StructField(\"Project_DetailID\", IntegerType(), True), \n",
    " StructField(\"Employee_DetailID\", IntegerType(), True), \n",
    " StructField(\"Project_Name\", StringType(), True) \n",
    "]) \n",
    "  \n",
    "# Create the data as a list of tuples \n",
    "pro_data = [ \n",
    " (1, 1, \"Task Track\"), \n",
    " (2, 1, \"CLP\"), \n",
    " (3, 1, \"Survey Management\"), \n",
    " (4, 2, \"HR Management\"), \n",
    " (5, 3, \"Task Track\"), \n",
    " (6, 3, \"GRS\"), \n",
    " (7, 3, \"DDS\"), \n",
    " (8, 4, \"HR Management\"), \n",
    " (9, 6, \"GL Management\")]\n",
    "\n",
    "pro_df = spark.createDataFrame(pro_data, pro_schema)\n",
    "pro_df.display()\n",
    "\n",
    "def proj_details(df, df1):\n",
    "    emp_proj_join = df.alias('e').join(df1.alias('p'), (col(\"e.EmployeeID\") == col(\"p.Employee_DetailID\")), \"inner\")\n",
    "    proj_more = emp_proj_join.groupBy('Project_Name').agg(count(\"Project_DetailID\").alias(\"count_proj\")).filter(col(\"count_proj\") > 1)\n",
    "\n",
    "    proj_emp_name = emp_proj_join.select(\"First_Name\", \"Project_Name\").orderBy(col('First_Name').asc())\n",
    "\n",
    "    return emp_proj_join, proj_more,proj_emp_name\n",
    "\n",
    "\n",
    "emp_proj_join, proj_more, proj_emp_name = proj_details(df_e, pro_df)\n",
    "emp_proj_join.display()\n",
    "proj_more.display()\n",
    "proj_emp_name.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ada10af1-cc22-4acc-bd9b-7acd5fa4d1c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "29_Join_in_pyspark \n",
    "Write a pyspark code perform below function \n",
    "\n",
    "●  52. Get employee name, project name order by firstname from \n",
    "\"EmployeeDetail\" and \"ProjectDetail\" for all employee even \n",
    "they have not assigned project. \n",
    "\n",
    "● 53  Get employee name, project name order by firstname from \n",
    "\"EmployeeDetail\" and \"ProjectDetail\" for all employee if \n",
    "project is not assigned then display \"-No Project Assigned\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21ff8250-afb5-48c1-920a-9f261d5040fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [ \n",
    " [1, \"Vikas\", \"Ahlawat\", 600000.0, \"2013-02-15 11:16:28.290\", \"IT\", \"Male\"], \n",
    " [2, \"nikita\", \"Jain\", 530000.0, \"2014-01-09 17:31:07.793\", \"HR\", \"Female\"], \n",
    " [3, \"Ashish\", \"Kumar\", 1000000.0, \"2014-01-09 10:05:07.793\", \"IT\", \"Male\"], \n",
    " [4, \"Nikhil\", \"Sharma\", 480000.0, \"2014-01-09 09:00:07.793\", \"HR\", \"Male\"], \n",
    " [5, \"anish\", \"kadian\", 500000.0, \"2014-01-09 09:31:07.793\", \"Payroll\", \"Male\"]\n",
    "]  \n",
    "\n",
    "# Create a schema for the DataFrame \n",
    "schema = StructType([ \n",
    " StructField(\"EmployeeID\", IntegerType(), True), \n",
    " StructField(\"First_Name\", StringType(), True), \n",
    " StructField(\"Last_Name\", StringType(), True), \n",
    " StructField(\"Salary\", DoubleType(), True), \n",
    " StructField(\"Joining_Date\", StringType(), True), \n",
    " StructField(\"Department\", StringType(), True), \n",
    " StructField(\"Gender\", StringType(), True) \n",
    "]) \n",
    " \n",
    "df_emp_noproj = spark.createDataFrame(data, schema)\n",
    "df_emp_noproj.display()\n",
    " \n",
    "pro_schema = StructType([ \n",
    " StructField(\"Project_DetailID\", IntegerType(), True), \n",
    " StructField(\"Employee_DetailID\", IntegerType(), True), \n",
    " StructField(\"Project_Name\", StringType(), True) \n",
    "]) \n",
    "  \n",
    "# Create the data as a list of tuples \n",
    "pro_data = [ \n",
    " (1, 1, \"Task Track\"), \n",
    " (2, 1, \"CLP\"), \n",
    " (3, 1, \"Survey Management\"), \n",
    " (4, 2, \"HR Management\"), \n",
    " (5, 3, \"Task Track\"), \n",
    " (6, 3, \"GRS\"), \n",
    " (7, 3, \"DDS\"), \n",
    " (8, 4, \"HR Management\"), \n",
    " (9, 6, \"GL Management\")]\n",
    "\n",
    "pro_noemp_df = spark.createDataFrame(pro_data, pro_schema)\n",
    "pro_noemp_df.display()\n",
    "\n",
    "\n",
    "def No_proj_assign(df,df1):\n",
    "    no_proj_emp = df.alias('e').join(df1.alias('p'), (col(\"e.EmployeeID\") == col(\"p.Employee_DetailID\")), \"left\").select(\"EmployeeID\",\"First_Name\",\"Last_Name\",\"Project_Name\").orderBy(col(\"First_Name\").asc())\n",
    "\n",
    "    if no_proj_emp.filter(col(\"Project_Name\").isNull()):\n",
    "        no_proj_null = no_proj_emp.fillna(\"No Project Assigned\", subset=[\"Project_Name\"])\n",
    "\n",
    "        return no_proj_emp, no_proj_null\n",
    "\n",
    "no_proj_emp, no_proj_null = No_proj_assign(df_emp_noproj,pro_noemp_df)\n",
    "no_proj_emp.display()\n",
    "no_proj_null.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79e5de86-15c3-41ce-8d99-e6cd53222bec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "30_Join_in_pyspark \n",
    "Write a pyspark code perform below function \n",
    "\n",
    "●  56. Write a pyspark code to find out the employeename who \n",
    "has not assigned any project, and display \"-No Project Assigned\"( tables :- [EmployeeDetail], [ProjectDetail]). \n",
    "\n",
    "● 57. Write a pyspark code to find out the project name which is \n",
    "not assigned to any employee( tables :- [EmployeeDetail],[ProjectDetail])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc036263-7f0f-4903-8b11-8e7a381dfd16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f9d2b7c-ac91-4d68-a395-88a2af0ec9df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, when\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = [ \n",
    " [1, \"Vikas\", \"Ahlawat\", 600000.0, \"2013-02-15 11:16:28.290\", \"IT\", \"Male\"], \n",
    " [2, \"nikita\", \"Jain\", 530000.0, \"2014-01-09 17:31:07.793\", \"HR\", \"Female\"], \n",
    " [3, \"Ashish\", \"Kumar\", 1000000.0, \"2014-01-09 10:05:07.793\", \"IT\", \"Male\"], \n",
    " [4, \"Nikhil\", \"Sharma\", 480000.0, \"2014-01-09 09:00:07.793\", \"HR\", \"Male\"], \n",
    " [5, \"anish\", \"kadian\", 500000.0, \"2014-01-09 09:31:07.793\", \"Payroll\", \"Male\"], \n",
    "] \n",
    "\n",
    " \n",
    "# Create a schema for the DataFrame \n",
    "schema = StructType([ \n",
    " StructField(\"EmployeeID\", IntegerType(), True), \n",
    " StructField(\"First_Name\", StringType(), True), \n",
    " StructField(\"Last_Name\", StringType(), True), \n",
    " StructField(\"Salary\", DoubleType(), True), \n",
    " StructField(\"Joining_Date\", StringType(), True), \n",
    " StructField(\"Department\", StringType(), True), \n",
    " StructField(\"Gender\", StringType(), True) \n",
    "]) \n",
    " \n",
    "pro_schema = StructType([ \n",
    " StructField(\"Project_DetailID\", IntegerType(), True), \n",
    " StructField(\"Employee_DetailID\", IntegerType(), True), \n",
    " StructField(\"Project_Name\", StringType(), True) \n",
    "]) \n",
    "  \n",
    "# Create the data as a list of tuples \n",
    "pro_data = [ \n",
    " (1, 1, \"Task Track\"), \n",
    " (2, 1, \"CLP\"), \n",
    " (3, 1, \"Survey Management\"), \n",
    " (4, 2, \"HR Management\"), \n",
    " (5, 3, \"Task Track\"), \n",
    " (6, 3, \"GRS\"), \n",
    " (7, 3, \"DDS\"), \n",
    " (8, 4, \"HR Management\"), \n",
    " (9, 6, \"GL Management\") \n",
    "] \n",
    "\n",
    "df_emp = spark.createDataFrame(data, schema)\n",
    "pro_df = spark.createDataFrame(pro_data, pro_schema)\n",
    "\n",
    "def no_project(df, df1): \n",
    "    df_emp_proj = df.alias('e').join(df1.alias('p'), (col(\"e.EmployeeID\") == (col('p.Employee_DetailID'))), 'left')\n",
    "    df_emp_proj_null = df_emp_proj.filter(col(\"Project_Name\").isNull()).withColumn('Project_Name', when(col(\"Project_Name\").isNull(), \"No Project Assigned\").otherwise(col(\"Project_Name\"))).select(\"First_Name\", \"Project_Name\")\n",
    "\n",
    "    df_proj_not_assign = df.alias('e').join(df1.alias('p'), (col(\"e.EmployeeID\") == (col('p.Employee_DetailID'))), 'right')\n",
    "    df_emp_null = df_proj_not_assign.filter(col(\"First_Name\").isNull()).select(\"Project_Name\")\n",
    "                                            \n",
    "    return df_emp_proj, df_emp_proj_null, df_emp_null\n",
    "\n",
    " \n",
    "df_emp_proj, df_emp_proj_null, df_emp_null = no_project(df_emp, pro_df)\n",
    "\n",
    "df_emp_proj.display()\n",
    "df_emp_proj_null.display()\n",
    "df_emp_null.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f9c1de7-43de-4b9f-9a0a-b3449c36b65c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "31_Histogram of Tweets \n",
    "\n",
    "write a query to obtain a histogram of tweets posted per user in 2022. Output the \n",
    "tweet count per user as the bucket and the number of Twitter users who fall into that \n",
    "bucket. \n",
    "\n",
    "In other words, group the users by the number of tweets they posted in 2022 and \n",
    "count the number of users in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d89035a-d6a7-4915-aeee-528902bac31f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([ \n",
    " StructField(\"tweet_id\", IntegerType(), True), \n",
    " StructField(\"user_id\", IntegerType(), True), \n",
    " StructField(\"msg\", StringType(), True), \n",
    " StructField(\"tweet_date\", StringType(), True) \n",
    "]) \n",
    "  \n",
    "# Define the data \n",
    "data = [ \n",
    " (214252, 111, 'Am considering taking Tesla private at $420. Funding secured.', '2021-12-30 00:00:00'), \n",
    " (739252, 111, 'Despite the constant negative press covfefe', '2022-01-01 00:00:00'), \n",
    " (846402, 111, 'Following @NickSinghTech on Twitter changed my life!', '2022-02-14 00:00:00'), \n",
    " (241425, 254, 'If the salary is so competitive why won’t you tell me what it is?', '2022-03-01 00:00:00'), \n",
    " (231574, 148, 'I no longer have a manager. I can\\'t be managed', '2022-03 23 00:00:00') \n",
    "]\n",
    "\n",
    "df_tweet = spark.createDataFrame(data, schema)\n",
    "df_tweet.display()\n",
    "\n",
    "def get_user(df):\n",
    "    df_user = df.groupBy(\"user_id\").agg(count(\"tweet_id\").alias(\"tweet_count\"))\n",
    "\n",
    "    df_year = df.withColumn(\"year\", year(expr(\"try_cast(tweet_date as timestamp)\")))\\\n",
    "        .filter(col(\"year\") == 2022)\\\n",
    "        .groupBy(\"user_id\")\\\n",
    "        .agg(count(\"tweet_id\").alias(\"tweet_count\"))\n",
    "\n",
    "    df_ucnt =df_year.groupBy('tweet_count').agg(count('user_id').alias(\"user_cnt\")).select(\"tweet_count\", \"user_cnt\")\n",
    "    \n",
    "    return df_user, df_year, df_ucnt\n",
    "\n",
    "d_user = [d_user,df_year,df_ucnt]\n",
    "d_user = get_user(df_tweet)\n",
    "\n",
    "for i in d_user:\n",
    "    i.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3764a20-32f8-479b-83d2-d6ced109e3db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "32_pyspark_transformation \n",
    "\n",
    "Write a pyspark code to transform the DataFrame to display each student's marks in Math and English as \n",
    "separate columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc366186-e8c9-46e1-a425-4b71984634e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=[ \n",
    "('Rudra','math',79), \n",
    "('Rudra','eng',60), \n",
    "('Shivu','math', 68), \n",
    "('Shivu','eng', 59), \n",
    "('Anu','math', 65), \n",
    "('Anu','eng',80) \n",
    "] \n",
    "  \n",
    "schema = StructType([ \n",
    " StructField(\"Name\", StringType(), True), \n",
    " StructField(\"Sub\", StringType(), True), \n",
    " StructField(\"Marks\", IntegerType(), True) \n",
    "])\n",
    "\n",
    "df_marks = spark.createDataFrame(data, schema)\n",
    "df_marks.display()\n",
    "\n",
    "df_pivot = df_marks.groupBy(\"Name\").pivot(\"Sub\").agg(sum(\"Marks\"))\n",
    "df_pivot.display()\n",
    "\n",
    "# df_sql = df_marks.withColumn(\"pivot\", expr('''\n",
    "#                    select * from (\n",
    "#                        pivot(\n",
    "#                        sum(Marks) for sub in ('math','eng')\n",
    "#                    ))'''))\n",
    "\n",
    "# df_sql.display()\n",
    "\n",
    "# df_s = df_marks.createOrReplaceTempView(\"marks\")\n",
    "\n",
    "\n",
    "# df_st = spark.sql('''SELECT *\n",
    "# FROM (\n",
    "#     SELECT Name,\n",
    "#            Sub        \n",
    "#     FROM marks\n",
    "# )\n",
    "# PIVOT (\n",
    "#     SUM(Marks)\n",
    "#     FOR sub IN ('math','eng')\n",
    "# )''')\n",
    "# df_st.display()\n",
    " \n",
    "\n",
    "\n",
    "# df_marks.createOrReplaceTempView(\"marks\")\n",
    "\n",
    "# df_sql = spark.sql(\"\"\"\n",
    "#     SELECT\n",
    "#         Name,\n",
    "#         SUM(CASE WHEN Sub = 'math' THEN Marks ELSE 0 END) AS math,\n",
    "#         SUM(CASE WHEN Sub = 'eng' THEN Marks ELSE 0 END) AS eng\n",
    "#     FROM marks\n",
    "#     GROUP BY Name\n",
    "# \"\"\")\n",
    "# df_sql.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2be0528-0b3f-4c93-bbca-67f2b455cd9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "33_Hobbies Data Transformation \n",
    "\n",
    " Problem Statement: \n",
    "Transform a dataset with individuals' names and \n",
    "associated hobbies into a new format using PySpark. \n",
    "Convert the comma-separated hobbies into separate \n",
    "rows, creating a DataFrame with individual rows  for \n",
    "each person and their respective hobbies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bab41350-180c-47b9-bd27-ba724e671b85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample input data \n",
    "data = [(\"Alice\", \"badminton,tennis\"), \n",
    "        (\"Bob\", \"tennis,cricket\"), \n",
    "        (\"Julie\", \"cricket,carroms\")] \n",
    "  \n",
    "# Create a DataFrame \n",
    "df_hobby = spark.createDataFrame(data, [\"name\", \"hobbies\"])\n",
    "df_hobby.display()\n",
    "\n",
    "df_hob_split = df_hobby.withColumn(\"hobbies\", explode(split(col(\"hobbies\"), \",\")))\n",
    "df_hob_split.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b9bbfc0-0d4b-41d3-b46b-7ca7026764b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "34_ Histogram of Tweets \n",
    "\n",
    " write a pyspark code to obtain a histogram of tweets \n",
    "posted per user in 2022. Output the tweet count per \n",
    "user as the bucket and the number of Twitter users who \n",
    "fall into that bucket.In other words, group the users by \n",
    "the number of tweets they posted in 2022 and count the \n",
    "number of users in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d38305f-c1ee-4f80-a7c7-90a19c1e04bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([ \n",
    " StructField(\"tweet_id\", IntegerType(), True), \n",
    " StructField(\"user_id\", IntegerType(), True), \n",
    " StructField(\"msg\", StringType(), True), \n",
    " StructField(\"tweet_date\", StringType(), True) \n",
    "]) \n",
    "  \n",
    "# Define the data \n",
    "data = [ \n",
    " (214252, 111, 'Am considering taking Tesla private at $420. Funding secured.', '2021-12-30 00:00:00'), \n",
    " (739252, 111, 'Despite the constant negative press covfefe', '2022-01-01 00:00:00'), \n",
    " (846402, 111, 'Following @NickSinghTech on Twitter changed my life!', '2022-02-14 00:00:00'), \n",
    " (241425, 254, 'If the salary is so competitive why won’t you tell me what it is?', '2022-03-01 00:00:00'), \n",
    " (231574, 148, 'I no longer have a manager. I can\\'t be managed', '2022-03 23 00:00:00') \n",
    "]\n",
    "\n",
    "df_tweet = spark.createDataFrame(data, schema)\n",
    "df_tweet.display()\n",
    "\n",
    "def get_user(df):\n",
    "    df_user = df.groupBy(\"user_id\").agg(count(\"tweet_id\").alias(\"tweet_count\"))\n",
    "\n",
    "    df_year = df.withColumn(\"year\", year(expr(\"try_cast(tweet_date as timestamp)\")))\\\n",
    "        .filter(col(\"year\") == 2022)\\\n",
    "        .groupBy(\"user_id\")\\\n",
    "        .agg(count(\"msg\").alias(\"cnt\"))\n",
    "\n",
    "    df_ucnt =df_year.groupBy(col('cnt')).agg(count('user_id').alias(\"bucket\")).select(col(\"cnt\"),\"bucket\")\n",
    "    \n",
    "    return df_user, df_year, df_ucnt\n",
    "\n",
    "d_user = [d_user,df_year,df_ucnt]\n",
    "d_user = get_user(df_tweet)\n",
    "\n",
    "for i in d_user:\n",
    "    i.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51241022-9f9e-4b2e-bcc0-50cefc50c068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "35_Classes More Than 5 Students \n",
    "\n",
    " Write a pyspark code to find all the classes that have at \n",
    "least five students.Return the result table in any order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7522850-b3ed-4afa-9966-968dae4782f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema for the DataFrame \n",
    "schema = StructType([ \n",
    " StructField(\"StudentID\", StringType(), True), \n",
    " StructField(\"ClassName\", StringType(), True) \n",
    "]) \n",
    "\n",
    "# Data to be inserted into the DataFrame \n",
    "data = [ \n",
    " ('A', 'Math'), \n",
    " ('B', 'English'), \n",
    " ('C', 'Math'), \n",
    " ('D', 'Biology'), \n",
    " ('E', 'Math'), \n",
    " ('F', 'Computer'), \n",
    " ('G', 'Math'), \n",
    " ('H', 'Math'), \n",
    " ('I', 'Math') \n",
    "]\n",
    "\n",
    "df_students = spark.createDataFrame(data, schema)\n",
    "df_students.display() \n",
    "\n",
    "df_stud_cnt = df_students.groupBy(\"ClassName\").agg(count(\"StudentID\").alias(\"stud_cnt\"))\\\n",
    "  .filter(col(\"stud_cnt\") > 5)\n",
    "df_stud_cnt.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4735b9e8-281c-4f93-ad3c-5c2db59ea055",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "36_Rank Scores Problem \n",
    "\n",
    " Write a pyspark code to rank scores. If there is a tie between \n",
    "two scores, both should have the same ranking. Note that \n",
    "after a tie, the next ranking number should be the next \n",
    "consecutive integer value.In other words, there should be no \n",
    "“holes” between ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47220028-19b6-4e42-989e-7f28da4d4d6f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"Score\":197},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762485387789}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, dense_rank, spark_partition_id\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define the schema for the DataFrame \n",
    "schema = StructType([ \n",
    " StructField(\"Id\", IntegerType(), True), \n",
    " StructField(\"Score\", DoubleType(), True) \n",
    "]) \n",
    "  \n",
    "# Data to be inserted into the DataFrame \n",
    "data = [ \n",
    " (1, 3.50), \n",
    " (2, 3.65), \n",
    " (3, 4.00), \n",
    " (4, 3.85), \n",
    " (5, 4.00), \n",
    " (6, 3.65) \n",
    "] \n",
    "\n",
    "df_rank = spark.createDataFrame(data, schema)\n",
    "df_rank.display()\n",
    "\n",
    "def score(df):\n",
    "  df_r = df.withColumn(\"Dense_Rank\", dense_rank().over(Window.orderBy(col(\"Score\")))).withColumn(\"Partition_id\", spark_partition_id())\n",
    "  return df_r\n",
    "\n",
    "df_score = score(df_rank)\n",
    "df_score.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd08429a-59a8-4ab9-8c9c-8116623fef62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "37_Triangle Judgement Problem \n",
    "  \n",
    "A pupil Tim gets homework to identify whether three line segments could \n",
    "possibly form a triangle. \n",
    "\n",
    "However, this assignment is very heavy because there are hundreds of records \n",
    "to calculate.  \n",
    "\n",
    "Could you help Tim by writing a pyspark code to judge whether these three \n",
    "sides can form a triangle,assuming df triangle holds the length of the three sides x, y and z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1eba657-489b-4b40-95b2-17b3088bb0e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([ \n",
    " StructField(\"x\", IntegerType(), True), \n",
    " StructField(\"y\", IntegerType(), True), \n",
    " StructField(\"z\", IntegerType(), True) \n",
    "]) \n",
    "  \n",
    "# Data to be inserted into the DataFrame \n",
    "data = [ \n",
    " (13, 15, 30), \n",
    " (10, 20, 15)]\n",
    "\n",
    "df_triangle = spark.createDataFrame(data, schema)\n",
    "df_triangle.display()\n",
    "\n",
    "df_tri = df_triangle.withColumn(\"valid_triangle\", when((col(\"x\") +col(\"y\") > col(\"z\")) & (col(\"x\") + col(\"z\") > col(\"y\")) & (col(\"x\") + col(\"y\") > col(\"z\")), \"Yes\").otherwise(\"No\"))\n",
    "df_tri.display() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b99488d-e99c-4e76-aff7-26b302297ed7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "38_Biggest Single Number Problem \n",
    " \n",
    "Df contains many numbers in column num including duplicated ones. \n",
    "Can you write a pyspark code to find the biggest number, which only \n",
    "appears once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53d319f8-ce03-4c6d-83dc-6421f7c70b7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema for the DataFrame \n",
    "schema = StructType([StructField(\"num\", IntegerType(), True)]) \n",
    "  \n",
    "# Your data \n",
    "data = [(8,), (8,), (3,), (3,), (1,), (4,), (5,), (6,)] \n",
    "  \n",
    "# Create a PySpark DataFrame \n",
    "df_dup = spark.createDataFrame(data, schema=schema)\n",
    "df_dup.display()\n",
    "\n",
    "df_duplicate = df_dup.withColumn(\"Row_Number\", row_number().over(Window.partitionBy(col(\"num\")).orderBy(col(\"num\").desc()))).filter(col(\"Row_Number\") > 1).drop(\"Row_Number\")\n",
    "df_duplicate.display()\n",
    "\n",
    "df_djoin = df_dup.join(df_duplicate, on=\"num\", how=\"leftanti\")\n",
    "df_max = df_djoin.agg(max(col(\"num\")).alias(\"Max_Num\"))\n",
    "df_djoin.display()\n",
    "df_max.display()\n",
    "\n",
    "# df_d = list(df_dup)\n",
    "# print(df_d)\n",
    "# df_nd = list(df_duplicate)\n",
    "# print(df_nd)\n",
    "\n",
    "# non_dup =[]\n",
    "# for i in df_d:\n",
    "#     if df_d != df_nd:\n",
    "#         non_dup.append(i)\n",
    "#         print(non_dup)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcc82e1c-106b-4efc-832e-92d27591d018",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "39_Not Boring Movies Problem \n",
    "\n",
    " X city opened a new cinema, many people would like to go to this \n",
    "cinema.   The cinema also gives out a poster indicating the movies’ ratings \n",
    "and descriptions.  Please write a Pyspark Code to output movies with an \n",
    "odd numbered ID and  a description that is not ‘boring’. Order the result \n",
    "by rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39efae66-8ec7-4e5b-b756-21d1b29fb36c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema for the DataFrame \n",
    "schema = StructType([ \n",
    " StructField(\"id\", IntegerType(), True), \n",
    " StructField(\"movie\", StringType(), True), \n",
    " StructField(\"description\", StringType(), True), \n",
    " StructField(\"rating\", FloatType(), True) \n",
    "]) \n",
    "  \n",
    "# Your data \n",
    "data = [ \n",
    " (1, \"War\", \"great 3D\", 8.9), \n",
    " (2, \"Science\", \"fiction\", 8.5), \n",
    " (3, \"Irish\", \"boring\", 6.2), \n",
    " (4, \"Ice song\", \"Fantasy\", 8.6), \n",
    " (5, \"House card\", \"Interesting\", 9.1) \n",
    "]\n",
    "\n",
    "df_movie = spark.createDataFrame(data, schema)\n",
    "df_movie.display()\n",
    "\n",
    "# df_movie1 = df_movie.withColumn(\"rating\", col(\"rating\").cast(\"double\"))\n",
    "# df_movie1.display()\n",
    "\n",
    "df_nbore = df_movie.filter(~(col(\"description\") == \"boring\") & (col(\"id\") % 2 == 1))\n",
    "df_nbore.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e0d3247-6632-4ad3-99bf-5578ec71cd17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "40_Swap Gender Problem \n",
    "\n",
    "   Given a df salary, such as the one below, that has m=male \n",
    "and f=female values. Swap all f and m values (i.e., change all f \n",
    "values to m and vice versa) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e56e9912-7618-4bd3-808a-789c9c6db7a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the schema for the DataFrame \n",
    "schema = StructType([ \n",
    " StructField(\"id\", IntegerType(), True), \n",
    " StructField(\"name\", StringType(), True), \n",
    " StructField(\"sex\", StringType(), True), \n",
    " StructField(\"salary\", IntegerType(), True), \n",
    "]) \n",
    "  \n",
    "# Define the data \n",
    "data = [ \n",
    " (1, \"A\", \"m\", 2500), \n",
    " (2, \"B\", \"f\", 1500), \n",
    " (3, \"C\", \"m\", 5500), \n",
    " (4, \"D\", \"f\", 500), \n",
    "] \n",
    "\n",
    "df_gender = spark.createDataFrame(data, schema)\n",
    "df_gender.display() \n",
    "\n",
    "df_gender_swap = df_gender.withColumn(\"sex\", when(col(\"sex\") == \"m\", \"f\").otherwise(\"m\"))\n",
    "df_gender_swap.display()\n",
    "\n",
    "df_col = df_gender.columns\n",
    "\n",
    "def swapgen(df):\n",
    "    df_s = lambda x:x.replace('m','f')\n",
    "    df_s = lambda x:x.replace('f','m')\n",
    "    return df_s\n",
    "\n",
    "df_gen = swapgen(df_gender.columns[2])\n",
    "print(df_gen)\n",
    "\n",
    "\n",
    "    # if 'sex' in df_col:\n",
    "    #     df_s = lambda x:x.replace('m','f')\n",
    "    # else:\n",
    "    #     df_s = lambda x:x.replace('f','m')     \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "ddf_gender_swapped = df_gender.replace(\n",
    "    {\"m\": \"f\", \"f\": \"m\"},\n",
    "    subset=[\"sex\"]\n",
    ")\n",
    "\n",
    "display(ddf_gender_swapped)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ABI_PYSPARK PRACTICE",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
