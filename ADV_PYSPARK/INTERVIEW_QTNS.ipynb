{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6d1ac9f-bace-40ea-9758-11833e6f6a1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    " ====================== Antriksh’s coforge interview ==================\n",
    "=================================================================\n",
    "Q add columns in a df table: name,eng,hindi,math. Sum all the cols(except name)\n",
    " //using spark sql\n",
    "devdf.createOrReplaceTempView(\"stud\")\n",
    "\n",
    "val odf = spark.sql(\"select sid,(hindi + math + eng) as total from stud group by sid\")\n",
    "Or\n",
    "val odf = devdf.select((col(\"hindi\")+col(\"eng\")+col(\"math\")).alias(\"total\"))\n",
    "\n",
    "Q there are 5 ids (key) , 10 partitions in data. How many files will be there?\n",
    "ans.\t50 files\n",
    " val ddf2 = ddf1.repartition(10)\n",
    "ddf2.write.format(\"csv\").option(\"path\", \"partitioned_data2\").partitionBy(\"sid\").save\n",
    "ee\n",
    "Q Now I want 5 files only (as many number of keys are there)\n",
    "ans. coalesce()\n",
    "val ddf2 = ddf1.coalesce(1)\n",
    "ddf2.write.format(\"csv\").option(\"path\", \"partitioned_data3\").partitionBy(\"sid\").save\n",
    "\n",
    "\n",
    "Q In the hive table, I want to remove all duplicate records?\n",
    "ans. \n",
    "create table new table select distinct * from oldtable;\n",
    "//OR\n",
    "INSERT OVERWRITE TABLE dup_demo SELECT DISTINCT * FROM dup_demo;\n",
    "\n",
    "\n",
    "Q empid ename manager_iD\n",
    "1 ram 4\n",
    "4 rohit 5\n",
    "name the manager who is also an employee.\n",
    "ans. select empid from table where empid in (select manager_id from table);\n",
    "\n",
    "Q In the hive table, once in a while data is getting updated, retrieve the latest data.\n",
    "select * from transactions where to_date(dt) in (select max(to_date(dt)) from transactions) ;\n",
    "\n",
    "Q a table is partitioned on city, 5 city…  new data came with 2 new cities, how will new 2 city partitions be created.\n",
    "Ans.  \n",
    "\n",
    "step1) set hive.exec.dynamic.partition.mode=nonstrict;\n",
    "step2)Create a new table(orders_state3) with the same schema, then insert data into it.\n",
    "step3)insert into orders_state partition(state) select * from orders_state3;\n",
    "\n",
    "Q what type of joins are there in spark\n",
    "Join Strategy Types in spark:\n",
    "\t1. Broadcast Hash Join (broadcast join)\n",
    "\tSmall table(df) will go to each partition of big table(df) and will join within each executor.\n",
    "\t2. Shuffle hash join\n",
    "\tStep 1- Shuffling: The data from the Join tables are partitioned based on the Join key. It does shuffle the data across partitions to have the same Join keys of the record assigned to the corresponding partitions.\n",
    "Step 2- Hash Join: A classic single node Hash Join algorithm is performed for the data on each partition.\n",
    "\t3. Shuffle sort merge join\n",
    "\t\t\tShuffle Phase: Both large tables will be repartitioned as per the Join keys across the partitions in the cluster.\n",
    "Sort Phase: Sort the data within each partition parallelly.\n",
    "Merge Phase: Join the sorted and partitioned data. It is a merging of the dataset by iterating over the elements and joining the rows having the same value for the Join keys.\n",
    "\t4. Broadcast Nested Loop Join\n",
    "\t5. Cartesian Join\n",
    "\n",
    "Q How does SMB work? \n",
    "Q. how will you do SMB without buckets in spark.\n",
    "Q. files are there in multiple folders, you need to create one data frame out of it.\n",
    "Ans. val input = spark.read.option(\"header\", value = true)\n",
    " .csv(\"C:\\\\Users\\\\dev30\\\\Downloads\\\\us-500\\\\data1.csv\",\n",
    "   \"C:\\\\Users\\\\dev30\\\\OneDrive\\\\Documents\\\\data1.csv\")\n",
    "\n",
    "\n",
    "Q. groupbykey vs reducebykey?\n",
    "groupByKey:\n",
    "Syntax:\n",
    "sparkContext.textFile(\"hdfs://\")\n",
    "                    .flatMap(line => line.split(\" \") )\n",
    "                    .map(word => (word,1))\n",
    "                    .groupByKey()\n",
    "                    .map((x,y) => (x,sum(y)))\n",
    "\n",
    "groupByKey can cause out of disk problems as data is sent over the network and collected on the reduced workers.\n",
    "\n",
    "reduceByKey:\n",
    "Syntax:\n",
    "\n",
    "sparkContext.textFile(\"hdfs://\")\n",
    "                    .flatMap(line => line.split(\" \"))\n",
    "                    .map(word => (word,1))\n",
    "                    .reduceByKey((x,y)=> (x+y))\n",
    "\n",
    "Data is combined at each partition, only one output for one key at each partition to send over the network. reduceByKey required combining all your values into another value with the exact same type.\n",
    "\n",
    "aggregateByKey:\n",
    "\n",
    "same as reduceByKey, which takes an initial value.\n",
    "\n",
    "3 parameters as input i. initial value ii. Combiner logic iii. sequence op logic\n",
    "\n",
    "\n",
    "Example:\n",
    "val keysWithValuesList = Array(\"foo=A\", \"foo=A\", \"foo=A\", \"foo=A\", \"foo=B\", \"bar=C\", \"bar=D\", \"bar=D\")\n",
    "    val data = sc.parallelize(keysWithValuesList)\n",
    "    //Create key value pairs\n",
    "    val kv = data.map(_.split(\"=\")).map(v => (v(0), v(1))).cache()\n",
    "    val initialCount = 0;\n",
    "    val addToCounts = (n: Int, v: String) => n + 1\n",
    "    val sumPartitionCounts = (p1: Int, p2: Int) => p1 + p2\n",
    "    val countByKey = kv.aggregateByKey(initialCount)(addToCounts, sumPartitionCounts)\n",
    "\n",
    "output: Aggregate By Key sum Results bar -> 3 foo -> 5\n",
    "\n",
    "combineByKey:\n",
    "\n",
    "3 parameters as input\n",
    "\n",
    "    Initial value: unlike aggregateByKey, need not pass constant always, we can pass a function that will return a new value.\n",
    "    merging function\n",
    "    combine function\n",
    "\n",
    "Example:\n",
    "\n",
    "val result = rdd.combineByKey(\n",
    "                        (v) => (v,1),\n",
    "                        ( (acc:(Int,Int),v) => acc._1 +v , acc._2 +1 ) ,\n",
    "                        ( acc1:(Int,Int),acc2:(Int,Int) => (acc1._1+acc2._1) , (acc1._2+acc2._2)) \n",
    "                        ).map( { case (k,v) => (k,v._1/v._2.toDouble) })\n",
    "        result.collect.foreach(println)\n",
    "\n",
    "Q. where do we need to use groupkey?\n",
    "\n",
    "Q group by key partition? 200, key \n",
    "K1ey  number\n",
    "1\t20\n",
    "1\t30\n",
    "2\t60\n",
    "2 \t 80\n",
    "groupbyKey 1,(20 + 30)\n",
    "2 , (60+80)\n",
    "\n",
    "reduce ByKey (1,60)\n",
    "\n",
    "Q for each department, give emp ids of 5th highest salary.\n",
    "Ans. \n",
    "select dept_id, dense_rank() over(partition by dept_id order by salary desc) as rnk from table where rnk=5;\n",
    "\n",
    "Q.How Serialization helps in optimization.\n",
    "●\tSerialization plays an important role in the performance for any distributed application. By default, Spark uses Java serializer.\n",
    "●\tSpark can also use another serializer called ‘Kryo’ serializer for better performance.\n",
    "●\tKryo serializer is in compact binary format and offers processing 10x faster than Java serializer.\n",
    "●\tTo set the serializer properties:\n",
    "conf.set(“spark.serializer”, “org.apache.spark.serializer.KryoSerializer”\n",
    "\n",
    "Q. physical plan vs logical plan.\n",
    "Logical Plan just depicts what I expect as output after applying a series of transformations like join, filter, where, groupBy, etc clause on a particular table.\n",
    "Physical Plan is responsible for deciding the type of join, the sequence of the execution of filter, where, groupBy clause, etc.\n",
    "This is how SPARK SQL works internally!!!\n",
    "\n",
    "Q. How does a physical plan work?\n",
    "\n",
    "Q. You have data in 4 partitions, what will happen when you will use mapPartition, and when you will use map. How is it going to process?\n",
    "\tMap will iterate row by row while map partition will iterate partition by partition\n",
    "\n",
    "Q. How will you see the logical plan and physical plan?\n",
    "Ans  using Explain or logs\n",
    "\n",
    "========================================================================\n",
    "Dev’s Unify Tech R1\n",
    "========================================================================\n",
    "Q1- All occurrences of second highest element in array\n",
    "object practice {\n",
    "  def main ( args: Array[String]){\n",
    "    val l1 = List(10,9,11,7,8,6,5,12,12,11,11,4)\n",
    "    var h1:Int = l1.min\n",
    " \n",
    "    var h2:Int = h1\n",
    "    def secMax(l:List[Int]):Int =\n",
    "    {\n",
    "      for(x <- l)\n",
    "      {\n",
    "        if(x>h1)\n",
    "        {\n",
    "          h2=h1\n",
    "          h1=x\n",
    "        }\n",
    "        else if(x>h2 && x!=h1)\n",
    "        {\n",
    "          h2=x\n",
    "        }\n",
    "      }\n",
    "     h2\n",
    "    }\n",
    "    \n",
    "    var h4:Int = secMax(l1)\n",
    "    println(l1.count(x=>x==h4))\n",
    "    \n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "Ques 2: - sort-merge-join vs broadcast join \n",
    "\n",
    "Broadcast Hash Join\n",
    "\tSend the small df to each partition node.\n",
    "\n",
    " Shuffle hash join\n",
    "\tShuffle is involved in big df to get similar keys in the same partition. Then it sends the small df to each partitioned node.\n",
    "Shuffle sort-merge join\n",
    "\tEach partition has sorted keys, similarly sorted keys from other tables will be shuffled in the same partition.\n",
    "\n",
    "\n",
    "Q3 - create a file and write data on it \n",
    "   Explained touch and cat command\n",
    "Q4- how would you create a spark session \n",
    "Val spark = sparkSession.builder()\n",
    "      .appName(“xyz”)\n",
    "      .master(“local[*]).getOrCreate\n",
    "\n",
    "Q- 5 - \n",
    "spark-shell --master yarn --numexecuter 20 --executor-memory 21G --executors-core 5 \n",
    "\n",
    "spark-submit --class class-name /sapark_pro.jar   --master yarn --numexecuter 20 --executor-memory 21G --executors-core 5 \n",
    "Q- 6- Name of the students who scored second highest marks\n",
    "mark\n",
    "id,name,marks\n",
    "1,R1,78\n",
    "2,R2,75\n",
    "3,R3,78\n",
    "4,R4,75\n",
    "\n",
    "Ans - R2\n",
    "R4\n",
    "select name, dence_rank() over(order by marks) as rnk form mark where rnk=2;\n",
    "\n",
    "========================================================================\n",
    "Dev’s Unify Tech R2 \n",
    "========================================================================\n",
    "\n",
    "Q1- //List - [10,9,11,7,8,6,5,12,4] - Second largest Number\n",
    "\n",
    "object HelloWorld {\n",
    " def main(args: Array[String]) = println(\"Hello Scala!\")\n",
    "\n",
    " val l = List(10, 9, 11, 7, 8, 6, 5, 12, 12, 4)\n",
    " var first = 0\n",
    " var Second = 0\n",
    "\n",
    " def compare(v: Int) = {\n",
    "   //println(v)\n",
    "   if (v > first) {\n",
    "     Second = first\n",
    "     first = v\n",
    "   }\n",
    "   else if (v > Second && v != first) {\n",
    "     Second = v\n",
    "   }\n",
    " }\n",
    "\n",
    " val greater = l.map(x => compare(x))\n",
    " println(Second)\n",
    "}\n",
    "\n",
    "Q-2 given two files sum up both the file column wise ?\n",
    "/*\n",
    "File1.txt\n",
    "=========\n",
    "1 2 3 1\n",
    "1 2 3 2\n",
    "2 2 2 1\n",
    "\n",
    "File2.txt\n",
    "=========\n",
    "2 3 1 1\n",
    "3 1 2 1\n",
    "1 1 1 1\n",
    "\n",
    "Col1 = 1+1+2+2+3+1 = 10\n",
    "Col2 = 11\n",
    "Col3 = 12\n",
    "Col4 = 7\n",
    "\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.SparkContext\n",
    "\n",
    "import scala.io.StdIn\n",
    "\n",
    "object interview_code_spark extends App {\n",
    " Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    " val sc = new SparkContext(\"local[*]\", \"word_count\")\n",
    "\n",
    " val input1 = sc.textFile(\"D:\\\\file1.txt\")\n",
    " val input2 = sc.textFile(\"D:\\\\file2.txt\")\n",
    "\n",
    " val combined = input1.union(input2)\n",
    " val comb_split = combined.map(x => x.split(\" \"))\n",
    "\n",
    " val comb_int = comb_split.map(x => x.map(e => e.toInt))\n",
    " val resultant = comb_int.reduce((xx, yy) => xx.zip(yy).map { case (a, b) => a + b })\n",
    "\n",
    " resultant.foreach(println)\n",
    "\n",
    " StdIn.readInt()\n",
    "\n",
    "}\n",
    "\n",
    "Q. window function in spark using dataframe\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "    val windowSpec = Window.partitionBy(\"name\").orderBy(\"marks\")\n",
    "     \n",
    "     df1.withColumn(\"row_number\",row_number.over(windowSpec)).show()\n",
    "========================================================================\n",
    "\tDev’s Mind tree \n",
    "========================================================================\n",
    "\n",
    "Q1- how to read csv file having , separated fields and also , separated values in columns?? \n",
    "Approach ⇒ dataframe, RDD + regular expression\n",
    "\n",
    "(in csv column values are inside brackets \"\", if we don't have \"\", we will lose the data )\n",
    "\n",
    "Sno,name,marks\n",
    "1,dev,kumar,70\n",
    "2,ant,kethwas,90\n",
    "3,raj,kumar,100\n",
    "4,jay,verma,88\n",
    "\n",
    "1,”dev,kumar”,70\n",
    "2,”ant,kethwas”,90\n",
    "3,”raj,kumar”,100\n",
    "4,”jay,verma”,88\n",
    "\n",
    "val Input = StdIn.readLine()\n",
    "Val Input2 = sc.parallelize(input)\n",
    "\n",
    "Val input3 = input2.map(x=>{\n",
    "Val a = x.split(“,”)\n",
    "(a(0),a(1)+a(2),a(3)) \n",
    "})\n",
    "\n",
    "\n",
    "Q2- write a program to create an array of elements of the first array that are not in the second array ?? //\n",
    "\n",
    "import scala.io.StdIn._\n",
    "\n",
    "object A_minus_B_array {\n",
    "\n",
    " def main(args: Array[String]): Unit = {\n",
    "\n",
    "   val a = readLine.split(\" \").map(_.toInt)\n",
    "   val b = readLine.split(\" \").map(_.toInt)\n",
    "\n",
    "   val ans = findMissing(a, b)\n",
    "   ans.foreach(println)\n",
    " }\n",
    "\n",
    " def findMissing(a: Array[Int], b: Array[Int]): List[Int] = {\n",
    "   var list = List[Int]()\n",
    "   for (elem <- a if !b.contains(elem))\n",
    "     list = list :+ elem\n",
    "   list\n",
    " }\n",
    "}\n",
    "\n",
    "Q3- find the employee details having the third largest salary?\n",
    "\n",
    "Q4- replace a column having values like 10k, 30k to 10000, 30000 and so on??\n",
    "Ans:\n",
    "\n",
    "System.setProperty(\"hadoop.home.dir\", \"C:\\\\HADOOP_HOME\\\\\")\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "\n",
    "val spark = SparkSession.builder\n",
    " .appName(\"first app\")\n",
    " .master(\"local[*]\")\n",
    " .getOrCreate()\n",
    "\n",
    "val devdf = spark.read\n",
    " .option(\"header\", true)\n",
    " .csv(\"dev1.csv\")\n",
    "\n",
    "devdf.createOrReplaceTempView(\"emp\")\n",
    "\n",
    "val odf = spark.sql(\"select emp_id,emp_name,concat(substr(salary,0,length(salary)-1),'000') as salary from emp\")\n",
    "\n",
    "val odf2 = odf.withColumn(\"salary\", col(\"salary\").cast(\"Int\"))\n",
    "\n",
    "odf2.printSchema()\n",
    "\n",
    "val partitionWindow = Window.orderBy(col(\"salary\").desc)\n",
    "\n",
    "val partitionWindowRank = rank().over(partitionWindow)\n",
    "\n",
    "val odf3 = odf2.select(col(\"*\"), partitionWindowRank as \"dense_rank\").where(col(\"dense_rank\") === 2)\n",
    "\n",
    "odf3.show\n",
    "odf2.show\n",
    "\n",
    "\n",
    "Q5- repartition and coalesce ?? (when, how)\n",
    " \n",
    "\n",
    "Q6- how will you save the same data in json format?\n",
    "input2.write.format(\"json\").option(\"path\",\"C:\\\\Users\\\\dev30\\\\Downloads\\\\us-500\\\\us-500.json\").save\n",
    "\n",
    "========================================================================= \n",
    "  Dheeraj = Mindtree\n",
    "========================================================================\n",
    "\n",
    "Q7 - What is broadcast in spark\n",
    "\n",
    "Q8. Dataframe vs Dataset\n",
    "\n",
    "Q9. Bucket in hive\n",
    "\n",
    "Q10. How to read from kafka and offset\n",
    "\n",
    "kafka-console-consumer --topic example-topic --bootstrap-server broker:9092 \\\n",
    " --property print.key=true \\\n",
    " --property key.separator=\"-\" \\\n",
    " --partition 1 \\\n",
    " --offset 6\n",
    "\n",
    "Q11. Write a code to read parquet file(product files and orders files) using spark and show all the data available in that file, status = sold \n",
    "\n",
    "Q12. What is Data Lineage (RDD)\n",
    "\n",
    "Q13.  Create IPL schedule like each team play at least 2 match with every team\n",
    "\n",
    "KKR\t\n",
    "DC\t\n",
    "RCB\t\n",
    "CSK\t\n",
    "MI\t\n",
    "Pune\t\n",
    "KP\t\n",
    "SH\t\n",
    "\n",
    "Q14. Find the duplicate from the given dataset using dataframe\n",
    "Q15. How to handle duplicate reading in kafka\n",
    "=======================================================================  \n",
    "  Dheeraj = NH Health\n",
    "=====================================================================\n",
    "1.\tWhat is difference b/w reparations and coalesce\n",
    "2.\tWhat is salting and the cause of using salting\n",
    "3.\tWhat happen if file size is small\n",
    "4.\tWhat is memory out of exception in spark\n",
    "5.\tHow to deploy spark code and monitor and how do you test the code\n",
    "6.\tHow stages creates in spark and when it gets created\n",
    "7.\tDifference between persist and cache and broadcast join\n",
    "8.\tWhat all tables available in your project and how you join those tables\n",
    "9.\tWhere is your big data setup, cloud or on-premises\n",
    "10.\tGiven one table and find the the employee total working hours using only one column which is “timestamp”\n",
    "11.\tWhat is DAG\n",
    "12.\tWhat is difference b/w val or var\n",
    "13.\tWrite spark code to read csv file using scala\n",
    "14.\tHow to join two table and get only the left table matching information from right table\n",
    "15.\tWrite the syntax for repartition and coalesce\n",
    "16.\tScenario given like\n",
    "\t1 flatmap\n",
    "2. Map\n",
    "3. Persist\n",
    "4. reduceByKey\n",
    "5. groupByKey\n",
    "6. Df.count\n",
    "17.\tTell me the flow of execution and number of stages \n",
    "18.\tWhat is EMR and how you deploy spark code using EMR and also how to create EMR cluster and SSH to your machine\n",
    "19.\tWhere you store hive table data in hdfs or cloud and can hdfs is in cloud also ?(tricky question)\n",
    "20.\tWhen stages gets create on spark UI\n",
    "21.\tTell me the complete project flow in organization project like from development to deploy spark code and how you monitor\n",
    "\n",
    "========================================================================\n",
    "Dev’s coforge\n",
    "========================================================================\n",
    "\n",
    "1.\tsize of the hdfs files \n",
    "--> hadoop fs -du -s -h  /user\n",
    "\n",
    "2.\tFind number of nulls in a dataframe column\n",
    "--> df.filter(df(colName).isNull || df(colName) === \"\" || df(colName).isNaN).count()\n",
    "\n",
    "\n",
    "3.\tHcatalog in hive \n",
    "Basically, a table as well as a storage management layer for Hadoop is what we call HCatalog. Its main function is that it enables users with different data processing tools, for example, Pig, MapReduce to make the read and write data easily on the grid.\n",
    "\n",
    "4.\tchange a managed table into  external table\n",
    "\talter table table_name SET TBLPROPERTIES('EXTERNAL'='TRUE');\n",
    "\n",
    "5.\tsqoop command to import data from S3 \n",
    "\tsqoop import -Dfs.s3a.access.key=$ACCES_KEY -Dfs.s3a.secret.key=$SECRET_KEY\n",
    "--connect $CONN --username $USER --password $PWD --table $TABLENAME --target-dir s3a://example-bucket/target-directory\n",
    "\n",
    "6.\tread multiple csvs \n",
    "→ val input = spark.read.option(\"header\", value = true)\n",
    " .csv(\"C:\\\\Users\\\\dev30\\\\Downloads\\\\us-500\\\\data1.csv\",\n",
    "   \"C:\\\\Users\\\\dev30\\\\OneDrive\\\\Documents\\\\data1.csv\")\n",
    "\n",
    "→ spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../Downloads/*.csv\") spark.read.option(\"header\", \"true\").csv(\"../Downloads/*.csv\")\n",
    "\n",
    "7.\thive vs impala \n",
    "Cloudera Impala being a native query language, avoids startup overhead which is commonly seen in MapReduce/Tez based jobs (MapReduce programs take time before all nodes are running at full capacity). In Hive, every query has this problem of “cold start” whereas Impala daemon processes are started at boot time itself, always being ready to process a query.\n",
    "Hadoop reuses JVM instances to reduce startup overhead partially but introduces another problem when large haps are in use. Cloudera benchmarks have 384 GB memory which is a big challenge for the garbage collector of the reused JVM instances.\n",
    "MapReduce materializes all intermediate results, which enables better scalability and fault tolerance (while slowing down data processing). Impala streams intermediate results between executors (trading off scalability).\n",
    "Hive generates query expressions at compile time whereas Impala does runtime code generation for “big loops”.\n",
    "Apache Hive might not be ideal for interactive computing whereas Impala is meant for interactive computing.\n",
    "Hive is batch based Hadoop MapReduce whereas Impala is more like an MPP database.\n",
    "Hive supports complex types but Impala does not.\n",
    "Apache Hive is fault tolerant whereas Impala does not support fault tolerance. When a hive query is run and if the DataNode goes down while the query is being executed, the output of the query will be produced as Hive is fault tolerant. However, that is not the case with Impala. If a query execution fails in Impala it has to be started all over again.\n",
    "\n",
    "8.\thow to fix HDFS corrupt file\n",
    "hdfs fsck /\n",
    "9.\tWhy  spark, hive itself can do this?\n",
    " \n",
    "10.\tHow to change data type of column in a hive table?\n",
    "→ ALTER TABLE tableA CHANGE ts ts BIGINT AFTER id;\n",
    "11.\tExplode in a hive ?? \n",
    "The LATERAL VIEW statement is used with user-defined table generating functions such as EXPLODE() to flatten the map or array type of a column.The explode function can be used on both ARRAY and MAP with LATERAL VIEW.   \n",
    "Explode function:\n",
    "The explode function explodes an array to multiple rows. Returns a row-set with a single column (col), one row for each element from the array.  \n",
    "Lateral View :\n",
    "Lateral views explode the array data into multiple rows. In other words, lateral view expands the array into rows. \n",
    "select std_id,stud_name,location,courses from std_course_details LATERAL VIEW explode(course) courses_list as courses;\n",
    "\n",
    "========================================================================\n",
    "Dheeraj = capgemini\n",
    "========================================================================\n",
    "\n",
    "1.\tRunning spark jobs \n",
    "2.\tAccumulator vs broadcast variable \n",
    "Result - got \n",
    "========================================================================\n",
    "antriksh= capgemini\n",
    "========================================================================\n",
    "\n",
    "\n",
    "Authors table\n",
    "authorname\tbookname\t\n",
    "\n",
    "Rabindra\tb1\n",
    "james\t\tb2\n",
    "robin\t\tb3\n",
    "Satyajeet\tb4\n",
    "Noha\t\tb5\n",
    "Nolan\t\tb6\n",
    "Elon\t\tb7\n",
    "Rabindra\tb8\tBooks table\n",
    "bookname soldcopies\n",
    "\n",
    "b1\t3\n",
    "b2\t25\n",
    "b3\t3\n",
    "b4\t10\n",
    "B5 \t0\n",
    "b6\t12\n",
    "B7\t8\n",
    "B8\t10\n",
    "\n",
    "Q. top 3 authors who sold most books using the above 2 tables.\n",
    "select authorname, sum(soldcopies) s from \n",
    "authors a join \n",
    "books b on a.bookname=b.bookname \n",
    "group by authorname \n",
    "order by s desc limit 3;\n",
    "\n",
    "Q stream vs batch\n",
    "\n",
    "Q types of files used in spark\n",
    "  ORC, Parquert , CSV , text ,sequence \n",
    "Q you have a csv file of 70 lac records (big). you need to upload it to the database. but it is taking 12 hours right now, do it in less time.\n",
    "\n",
    "Q how to update column data type in hive\n",
    "ALTER TABLE table_name CHANGE column_name column_name new_datatype;\n",
    "\n",
    "Q types of time data type in hive\n",
    "\n",
    "Q we are uploading data into the database from a csv file, it is taking 14 hours. Data is big. What should we do to reduce processing time from 14 hours so that it takes less time?\n",
    "\n",
    "========================================================================\n",
    "\t\t\t\t\tAntriksh’s BCG\n",
    "========================================================================\n",
    "\n",
    "https://towardsdatascience.com/strategies-of-spark-join-c0e7b4572bcf\n",
    "\n",
    "Q1 Tell me about yourself\n",
    "Q2 can you explain working of your project from business point of view\n",
    "Q3 size of cluster, node size, cores, \n",
    "Q4 Spark Architecture\n",
    "Q5 Spark session vs spark context\n",
    "Ans. In spark 2, there is a universal entry point for the spark engine.\n",
    "Spark Context:\n",
    "Prior to Spark 2.0.0 sparkContext was used as a channel to access all spark functionality.\n",
    "The spark driver program uses spark context to connect to the cluster through a resource manager (YARN orMesos..).\n",
    "sparkConf is required to create the spark context object, which stores configuration parameters like appName (to identify your spark driver), application, number of cores and memory size of the executor running on the worker node.\n",
    "\n",
    "In order to use APIs of SQL, HIVE, and Streaming, separate contexts need to be created.\n",
    "\n",
    "Example:\n",
    "creating sparkConf :\n",
    "\n",
    "val conf = new SparkConf().setAppName(“RetailDataAnalysis”).setMaster(“spark://master:7077”).set(“spark.executor.memory”, “2g”)\n",
    "\n",
    "creation of sparkContext:\n",
    "val sc = new SparkContext(conf)\n",
    "\n",
    "Spark Session:\n",
    "\n",
    "SPARK 2.0.0 onwards, SparkSession provides a single point of entry to interact with underlying Spark functionality and\n",
    "allows programming Spark with DataFrame and Dataset APIs. All the functionality available with sparkContext are also available in sparkSession.\n",
    "\n",
    "In order to use APIs of SQL, HIVE, and Streaming, no need to create separate contexts as sparkSession includes all the APIs.\n",
    "\n",
    "Once the SparkSession is instantiated, we can configure Spark’s run-time config properties.\n",
    "\n",
    "Example:\n",
    "\n",
    "Creating Spark session:\n",
    "val spark = SparkSession\n",
    ".builder\n",
    ".appName(“WorldBankIndex”)\n",
    ".getOrCreate()\n",
    "\n",
    "Configuring properties:\n",
    "spark.conf.set(“spark.sql.shuffle.partitions”, 6)\n",
    "spark.conf.set(“spark.executor.memory”, “2g”)\n",
    "\n",
    "Spark 2.0.0 onwards, it is better to use sparkSession as it provides access to all the spark Functionalities that sparkContext does. Also, it provides APIs to work on DataFrames and Datasets.\n",
    "\n",
    "Q6 When will you get an OOM error in the driver machine?\n",
    "Ans. \n",
    "1) collect\n",
    "\t2) broadcast join\n",
    "\n",
    "Q7 when you write your dataframe, we get an output file in multiple partitions. Can you get your csv output file into one single file?\n",
    "Ans. \n",
    "df.coalesce(1).write.csv(\"address\")\n",
    "df.repartition(1).write.csv(\"address\")\n",
    "\n",
    "Q8 you are reading data in two dataframes, applied join on your dataframe and saved it. How many stages will be created?\n",
    "\n",
    "Q9 property to set partitions constant throughout application\n",
    "\n",
    "Q10 Where do you use SparkSQL?\n",
    "Spark SQL is a Spark module used for structured data processing. DataFrames represent a distributed collection of data, in which data is organized into columns that are named. DataFrames provide a domain-specific language that can be used for structured data manipulation in Java, Scala, and Python.\n",
    "\n",
    "Q11 you have 6 node clusters, 64 gb, 16 core.\n",
    "Ans. how will you decide the number of exec, cores, memory.\n",
    "\n",
    "63 gb, 15 core\n",
    "5 core, 21 gb\n",
    "3 executors per node.\n",
    "\n",
    "Thin exec: 1 core= 1 exec. No parallelism\n",
    "Fat exec: 15 core = 1 exec. Throughput suffers\n",
    "\n",
    "Q spark memory distribution\n",
    " \n",
    "Q is “persist” an action or transformation?\n",
    "Though cache() or persist() is just another function on RDD which marks RDD to be cached or persisted. The first time an RDD is evaluated as a consequence of an action, it will be persisted/cached. So, cache() or persist() is neither an action nor a transformation\n",
    "\n",
    "Q When will you use coalesce and when will you use repartition?\n",
    "coalesce uses existing partitions to minimize the amount of data that's shuffled. repartition creates new partitions and does a full shuffle. coalesce results in partitions with different amounts of data (sometimes partitions that have much different sizes) and repartition results in roughly equal sized partitions.\n",
    "\n",
    "Q tell any instance when you encountered OOMs in your work.\n",
    "Ans. I was aggregating on the basis of customer segment. For example if data is kept in 8 partitions, then let’s say there are 2 customer segments then data will be aggregated or shuffled to these two partitions and they are not able to accommodate. \n",
    "\n",
    "Q where do we use spark context?\n",
    "\n",
    "Main entry point for Spark functionality. A SparkContext represents the connection to a Spark cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster. Note: Only one SparkContext should be active per JVM.\n",
    "\n",
    "Q you have one string “my name is Antriksh” replace your name with xyz.\n",
    "val str = \"my name is antsant\"\n",
    "val s= str.replace(\"antsant\",\"dev\")\n",
    "\n",
    "Q “geeksforgeeks” prints only duplicate characters.\n",
    "Ans.\n",
    "val str2 = \"ParallelCollectionRDD\"\n",
    "val s = str2.foldLeft(Map[Char, Int]()) {\n",
    "      case (map, char) =>\n",
    "        val oldCount: Int = map.getOrElse(char, 0)\n",
    "        map.+(char -> (oldCount + 1))\n",
    "    }\n",
    ".filter(x => x._2 >=2).map(x => x._1)\n",
    "\n",
    "Q Traits in scala?\n",
    "Traits are like interfaces in Java. But they are more powerful than the interface in Java because in the traits you are allowed to implement the members. Traits can have methods(both abstract and non-abstract), and fields as its members.\n",
    "Traits are created using trait keywords.\n",
    "// Scala program to illustrate how to\n",
    "// create traits\n",
    "\n",
    "// Trait\n",
    "trait MyTrait\n",
    "{\n",
    "\tdef pet\n",
    "\tdef pet_color\n",
    "}\n",
    "\n",
    "// MyClass inherits trait\n",
    "class MyClass extends MyTrait\n",
    "{\n",
    "\t\n",
    "\t// Implementation of methods of MyTrait\n",
    "\tdef pet()\n",
    "\t{\n",
    "\t\tprintln(\"Pet: Dog\")\n",
    "\t}\n",
    "\t\n",
    "\tdef pet_color()\n",
    "\t{\n",
    "\t\tprintln(\"Pet_color: White\")\n",
    "\t}\n",
    "\t\n",
    "\t// Class method\n",
    "\tdef pet_name()\n",
    "\t{\n",
    "\t\tprintln(\"Pet_name: Dollar\")\n",
    "\t}\n",
    "}\n",
    "\n",
    "object Main\n",
    "{\n",
    "\t\n",
    "\t// Main method\n",
    "\tdef main(args: Array[String])\n",
    "\t{\n",
    "\t\tval obj = new MyClass();\n",
    "\t\tobj.pet();\n",
    "\t\tobj.pet_color();\n",
    "\t\tobj.pet_name();\n",
    "\t}\n",
    "}\n",
    "In Scala, we are allowed to implement the method(only abstract methods) in traits. If a trait contains method implementation, then the class which extends this trait need not implement the method which is already implemented in a trait. As shown in the below example.\n",
    "Example:\n",
    "// Scala program to illustrate the concept of\n",
    "// abstract and non-abstract method in Traits\n",
    "  \n",
    "// Trait with abstract and non-abstract methods\n",
    "trait MyTrait\n",
    "{\n",
    "    // Abstract method \n",
    "    def greeting\n",
    "  \n",
    "    // Non-abstract method\n",
    "    def tutorial\n",
    "    {\n",
    "        println(\"This is a tutorial\" + \n",
    "                \"of Traits in Scala\")\n",
    "    }\n",
    "}\n",
    "  \n",
    "  \n",
    "// MyClass inherits trait\n",
    "class MyClass extends MyTrait\n",
    "{\n",
    "      \n",
    "    // Implementation of abstract method\n",
    "    // No need to implement a non-abstract \n",
    "    // method because it already implemented\n",
    "    def greeting()\n",
    "    {\n",
    "        println(\"Welcome to GeeksfoGeeks\")\n",
    "    }\n",
    "} \n",
    "  \n",
    "object Main\n",
    "{\n",
    "    // Main method\n",
    "    def main(args: Array[String]) \n",
    "    {\n",
    "        val obj = new MyClass();\n",
    "        obj.greeting\n",
    "        obj.tutorial\n",
    "    }\n",
    "}\n",
    "\n",
    "Output:\n",
    "Welcome to GeeksfoGeeks\n",
    "This is a tutorial of Traits in Scala\n",
    "\n",
    "Traits do not contain constructor parameters.\n",
    "When a class inherits one trait, then use extends keyword.\n",
    "Syntax:\n",
    "class Class_Name extends Trait_Name{\n",
    "// Code..\n",
    "}\n",
    "When a class inherits multiple traits then use extends keyword before the first trait and after that use with keyword before other traits. As shown in the below example.\n",
    "Syntax:\n",
    "class Class_Name extends Trait_Name1 with Trait_Name2 with Trait_Name3{\n",
    "// Code..\n",
    "}\n",
    "Example:\n",
    "// Scala program to illustrate how\n",
    "// a class inherits multiple traits\n",
    "  \n",
    "// Trait 1\n",
    "trait MyTrait1\n",
    "{   \n",
    "    // Abstract method \n",
    "    def greeting \n",
    "}\n",
    "  \n",
    "//Trait 2\n",
    "trait MyTrait2\n",
    "{ \n",
    "    // Non-abstract method\n",
    "    def tutorial\n",
    "    {\n",
    "        println(\"This is a tutorial\" + \n",
    "               \"of Traits in Scala\")\n",
    "    }\n",
    "}\n",
    "  \n",
    "// MyClass inherits multiple traits\n",
    "class MyClass extends MyTrait1 with MyTrait2\n",
    "{\n",
    "      \n",
    "    // Implementation of abstract method\n",
    "    def greeting()\n",
    "    {\n",
    "        println(\"Welcome to GeeksfoGeeks\")\n",
    "    }\n",
    "} \n",
    "  \n",
    "object Main \n",
    "{\n",
    "    // Main method\n",
    "    def main(args: Array[String]) \n",
    "    {\n",
    "        val obj = new MyClass();\n",
    "        obj.greeting\n",
    "        obj.tutorial\n",
    "    }\n",
    "}\n",
    "\n",
    "Output:\n",
    "Welcome to GeeksfoGeeks\n",
    "This is a tutorial of Traits in Scala\n",
    "\n",
    "An abstract class can also inherit traits by using extends keyword.\n",
    "Syntax:\n",
    "abstract class Class_name extends Trait_Name{\n",
    "// code..\n",
    "}\n",
    "In Scala, one trait can inherit another trait by using an extends   keyword.\n",
    "Syntax:\n",
    "trait Trait_Name1 extends Trait_Name2{\n",
    "// Code..\n",
    "}\n",
    "Traits support multiple inheritance.\n",
    "In Scala, a class can inherit both normal classes or abstract classes and traits by using extends keyword before the class name and with the keyword before the trait’s name.\n",
    "Syntax:\n",
    "class Class_Name1 extends Class_Name2 with Trait_Name{\n",
    "// Code..\n",
    "}\n",
    "In Traits, abstract fields are those fields containing initial value and concrete fields are those fields which contain the initial value. we are allowed to override them in the class which extends the trait. If a field is declared using the var keyword, then there is no need to write an override keyword when we override them. And if a field is declared using the val keyword, then you must write the override keyword when we override them.\n",
    "Example:\n",
    "// Scala program to illustrate \n",
    "// concrete and abstract fields in traits\n",
    "  \n",
    "trait MyTrait\n",
    "{\n",
    "      \n",
    "    // Abstract field\n",
    "    var value: Int \n",
    "      \n",
    "    // Concrete field\n",
    "    var Height = 10\n",
    "    val Width = 30\n",
    "}\n",
    " \n",
    "class MyClass extends MyTrait\n",
    "{\n",
    "      \n",
    "    // Overriding MyTrait's fields\n",
    "    var value = 12\n",
    "    Height = 40\n",
    "    override val Width = 10\n",
    "      \n",
    "    // Method to display the fields\n",
    "    def Display()\n",
    "    {\n",
    "        printf(\"Value:%d\", value);\n",
    "        printf(\"\\nHeight:%d\" ,Height);\n",
    "        printf(\"\\nWidth:%d\", Width);\n",
    "    }\n",
    "}\n",
    "  \n",
    "object Main\n",
    "{\n",
    "      \n",
    "    // Main method\n",
    "    def main(args: Array[String])\n",
    "    {\n",
    "        val obj = new MyClass();\n",
    "        obj.Display();\n",
    "    }\n",
    "}\n",
    "\n",
    "Output:\n",
    "Value:12\n",
    "Height:40\n",
    "Width:10\n",
    "We can also add traits to an object instance. Or in other words, We can directly add a trait in the object of a class without inheriting that trait into the class. We can add a trait in the object instance by using a keyword.\n",
    "Syntax:\n",
    "val object_name = new Class_name with Trait_Name;\n",
    "Example:\n",
    "// Scala program to illustrate how \n",
    "// to add a trait to an object instance \n",
    "   \n",
    "class MyClass{}\n",
    "trait MyTrait\n",
    "{\n",
    "    println(\"Welcome to MyTrait\");\n",
    "}\n",
    "object Main \n",
    "{\n",
    "      \n",
    "    // Main method\n",
    "    def main(args: Array[String])\n",
    "    {\n",
    "          \n",
    "        // Here MyTrait is added to the \n",
    "        // object instance of MyClass\n",
    "        val obj = new MyClass with MyTrait;\n",
    "    }\n",
    "}\n",
    "\n",
    "Q tail recursion in scala?\n",
    "A recursive function is tail recursive when a recursive call is the last thing executed by the function.\n",
    "============================ capgemini L2 Antriksh=========================\n",
    "\n",
    "1.\tYou are given some data, you need to use few operations against that data. How would you use HIVE or Spark?? (when will you use Hive and when Spark)\n",
    "\n",
    "2.\tSpark vs MapReduce ??\n",
    "\n",
    "3.\tSpark framework ? How is it different from Mapreduce?\n",
    "\n",
    "4.\tHow would you say spark is faster than Mapreduce? \n",
    "\n",
    "5.\tYou have given a list say l1 = [1,4,3,2,4,4,4,2,3,5] . write a function in scala to print the frequency of duplicate numbers ??\n",
    "6.\tHow do you save the spark code you use? In what extension?\n",
    "a.\t.scala\n",
    "7.\tHow do you execute your spark job? In what format is it in spark?\n",
    "8.\tHow would you create a jar file\n",
    "9.\tWhat command would you write to submit your spark job?how would you give resources? How dynamic resource allocation would work?\n",
    "10.\tName some transformations and actions\n",
    "11.\tPartitioning vs bucketing\n",
    "\n",
    "========================== Kumulus Pvt. Ltd. c============================\n",
    "\n",
    "1.\tGiven one data table where you have to choose only year from the date column\n",
    "Ans: select date_format(“date”, “M”) as year from datetable\n",
    "2.\t6 Node and 64 ram, 64 core, how many executor will be there\n",
    "Ans: explain same thing which is taught in the course\n",
    "3.\tPersist and cache\n",
    "4.\tRead Json file, where data is stored as key, value pair, Write spark code to pick only key from the file\n",
    "Ans: i) using RDD\n",
    "II) using dataframe by creating table with manual schema\n",
    "5.\tHow to read data from kafka using spark\n",
    "6.\tWhat is accumulator in spark\n",
    "Accumulators are variables that are only “added” to through an associative and commutative operation and can therefore be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric types, and programmers can add support for new types.\n",
    "Result - got \n",
    "===================== Maveric - Dheeraj ===============================\n",
    "\n",
    "1. I need to bring the data to HDFS using sqoop. Data is changing 3 times a day, so what would be my design approach?\n",
    "2. write spark code for filtering data from HIVE/HDFS (Sal>10000) and store in another HIVE table with id, name, level, age and salary. Use Spark Core functions.\n",
    "3. write spark code for finding employees whose level changed from 2 to 3 in the last one month, and increase their amount by 15%. Update existing table.\n",
    "\n",
    "======================= All State - Antriksh ==============================\n",
    "\n",
    "Q project overview?\n",
    "\n",
    "Q challenges you face on a regular basis?\n",
    "Ans. We often need to aggregate data at customer segments and we may end up getting oom error. We use salting to solve it.\n",
    "\n",
    "Q What would you do if a cluster fails?\n",
    "Ans. We never encounter such problems. I think administrators will look into these types of failure.\n",
    "\n",
    "Q What if a node will fail while your job is executing?\n",
    "\n",
    "Q shell script? Find a file which was created/modified before a certain date? Can you do it in scala?\n",
    "\n",
    "ls -lt  | grep 'Apr 17'\n",
    "\n",
    "find . -type f -ls |grep 'filename'\n",
    "\n",
    "\n",
    "Q Is it still important to use rdd code? If yes, then when?\n",
    "There are many instances where we need to use lower level constructs \n",
    "1.\tWhen we read data from unstructured files and need to apply lower level transformations \n",
    "2.\tTo check number of partition of DF\n",
    "\n",
    "Q How do you see dag for dataframe?\n",
    " Spark web UI\n",
    "\n",
    "Q What is dag?\n",
    "In Airflow, a DAG – or a Directed Acyclic Graph – is a collection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies.\n",
    "A DAG is defined in a Python script, which represents the DAGs structure (tasks and their dependencies) as code.\n",
    "Q How do you read from a kafka stream?\n",
    "# Start ZooKeeper.  Run this command in its own terminal.\n",
    "  ./bin/zookeeper-server-start ./etc/kafka/zookeeper.properties\n",
    "\n",
    "# Start Kafka.  Run this command in its own terminal\n",
    "  ./bin/kafka-server-start ./etc/kafka/server.properties\n",
    "\n",
    "# Create the input topic\n",
    "  ./bin/kafka-topics --create \\\n",
    "          --bootstrap-server localhost:9092 \\\n",
    "          --replication-factor 1 \\\n",
    "          --partitions 1 \\\n",
    "          --topic streams-plaintext-input\n",
    "\n",
    "# Create the output topic\n",
    "  ./bin/kafka-topics --create \\\n",
    "          --bootstrap-server localhost:9092 \\\n",
    "          --replication-factor 1 \\\n",
    "          --partitions 1 \\\n",
    "          --topic streams-wordcount-output\n",
    "\n",
    "Next, we generate some input data and store it in a local file at\n",
    "\n",
    " /tmp/file-input.txt:\n",
    "\n",
    "echo -e \"all streams lead to kafka\\nhello kafka streams\\njoin kafka summit\" > /tmp/file-input.txt\n",
    "Lastly, we send this input data to the input topic:\n",
    "cat /tmp/file-input.txt | ./bin/kafka-console-producer --broker-list localhost:9092 --topic streams-plaintext-input\n",
    "# Run the WordCount demo application.\n",
    "# The application writes its results to a Kafka output topic -- there won't be any STDOUT output in your console.\n",
    "# You can safely ignore any WARN log messages.\n",
    "  ./bin/kafka-run-class org.apache.kafka.streams.examples.wordcount.WordCountDemo\n",
    "\n",
    "./bin/kafka-console-consumer --bootstrap-server localhost:9092 \\\n",
    "        --topic streams-wordcount-output \\\n",
    "        --from-beginning \\\n",
    "        --formatter kafka.tools.DefaultMessageFormatter \\\n",
    "        --property print.key=true \\\n",
    "        --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \\\n",
    "        --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer\n",
    "======================================================================\n",
    "Q \n",
    "Input:\n",
    "\n",
    "table1: \n",
    "userid| Ip address \n",
    "1|190.8.18.7\n",
    "2|190.8.9.7\n",
    "\n",
    "tabl2:\n",
    "userid| address| username\n",
    "1|AAAA|u\n",
    "2|BBBB|A\n",
    "3|CCCC|C\n",
    "4\n",
    "5\n",
    "6\n",
    "...\n",
    "3,6,9,12\n",
    "\n",
    "output\n",
    "Userid|ipaddress|username|address\n",
    "\n",
    "Get the output. Do it in sql, then do it using df.\n",
    "Ans.\n",
    "select t1.userid,t1.ipaddress,t2.username,t2.address\n",
    "\ttable1 t1 join table2 t2\n",
    "\ton t1.userid=t2.userid;\n",
    "\n",
    "df1.join(df2,df1.col(\"userid\")===df2.col(\"userid\"),inner)\n",
    "\n",
    "Q in above table, get records which are divisible by 3 (ex 3,6,9,12,15,....)\n",
    "Ans\n",
    "\n",
    "rdd.filter(x=>x._1%3==0)\n",
    "\tor\n",
    "df.where(col(“id”)%3==0)\n",
    "\n",
    "Q you have data from chat logs. write code to find all the occurrences of “utkarsh”.\n",
    "\n",
    "1, \"hi utkarsh how are you\" \n",
    "2, \"what's up\"\n",
    "\n",
    "Ans.\n",
    "\n",
    "var input = textFile(\"some location\")\n",
    "\n",
    "var flatInp = input.flatmap(x=>x._2.split(\" \"))\n",
    "\n",
    "var findNm = flatInp.filter(x=>x==\"utkarsh\")\n",
    "\n",
    "var output = findNm.count\n",
    "\n",
    "//can we use count instead of reducebykey? As the code is in pure scala\n",
    "\n",
    "Q on what factor parallelism in your spark code depends on?\n",
    "\n",
    "Core,partitions,key\n",
    "\n",
    "Q \n",
    "job1 SHTP\n",
    "job2 \n",
    "Job3 \n",
    "\n",
    "Job4\n",
    "\n",
    "Schedule a job, such that if job1 fails then execute job4, if it doesn’t then execute job2 and then 3\n",
    "Ans.\n",
    "\n",
    "Q\n",
    "Input Dataframe:\n",
    "Date, State, ActiveCount\n",
    "2021/01/01, Karnataka, 4000\n",
    "2021/01/01, Delhi, 3500\n",
    "2021/01/01, Assam, 400\n",
    "2021/01/02, Karnataka, 300\n",
    "2021/01/02, Delhi, 3300\n",
    "2021/01/02, Assam, 420\n",
    "\n",
    "    Output Dataframe:\n",
    "Date, State, ActiveCount, Threshold\n",
    "2021/01/01, Karnataka, 4000, Y\n",
    "2021/01/01, Delhi, 3500, Y\n",
    "2021/01/01, Assam, 400, N\n",
    "2021/01/02, Karnataka, 300, N\n",
    "2021/01/02, Delhi, 3300, Y\n",
    "2021/01/02, Assam, 420, N\n",
    "\n",
    "If activeCount>1000 then put threshold as Y else N\n",
    "\n",
    "Ans.\n",
    "\n",
    "def thresh(activeCount:Int):Char = \n",
    "{\n",
    "\tif(activeCount>1000)\n",
    "\t{'y'}\n",
    "\telse\n",
    "\t{'N'}\n",
    "}\n",
    "\n",
    "\n",
    "udf(\"thresh\",thresh(_:Int):Char)\n",
    "\n",
    "InputDf.withColumn(“threshold”,\"thresh(activeCount)\")\n",
    "\tOr\n",
    "\n",
    "val newDf = df.withColumn(\"thresh\", when(col(“activeCount”) >=1000, “Y”).otherwise(“N”) \n",
    "\n",
    "//inputDf.withColumn(“threshold”,col(“”))\n",
    "\n",
    "\n",
    "Q write binary search in scala\n",
    "Ans.\n",
    " \n",
    "var l1 = List(1,4,5,7,8,12,33)\n",
    "var s = 5\n",
    "var n = l1.size\n",
    "def bisearch(l1,a,n,s)\n",
    "{\n",
    "\tif(l1(n/2)==s)\n",
    "\t{l1(n/2)}\n",
    "\telse if(l1(n/2)<s)\n",
    "\t{\n",
    "\t\tbisearch(l1,0,n/2,s)\n",
    "\t}\n",
    "\telse if(l1(i)>l1(n/2)\n",
    "\t{\n",
    "\t\tbisearch(l1,n/2,n,s)\n",
    "\t}\n",
    "}\n",
    "\n",
    "Q\n",
    "userid| Ip address \n",
    "1|190.8.18.7\n",
    "2|190.8.9.7\n",
    "\n",
    "userid| address| username\n",
    "1|AAAA|u\n",
    "2|BBBB|A\n",
    "3|CCCC|C\n",
    "\n",
    "Ans.\n",
    "\n",
    "Antijoin\n",
    "\n",
    "========================= Media.net coding test ===========================\n",
    "\n",
    "Q \n",
    "event\tstartDate\tEndDate\n",
    "A1\t10-07-21\t11-07-21\n",
    "A2\t11-07-21\t12-07-21\n",
    "B\t15-07-21\t16-07-21\n",
    "D\t01-08-21\t02-08-21\n",
    "\n",
    "o/p\n",
    "event\tstartDate\tEndDate\n",
    "A1A2\t10-07-21\t12-07-21\n",
    "B\t15-07-21\t16-07-21\n",
    "D\t01-08-21\t02-08-21\n",
    "\n",
    "Explanation: A1 & A2 are two different events in the input table. You need to write a sql query to combine all the events which are falling on consecutive dates.\n",
    "\n",
    "Q\n",
    "Var input1= List(1,2,3,5,7,9)\t//yes\n",
    "Var input2 = List(2,-5,3,10,11)\t//no\n",
    "\n",
    "Write a code (preferably in scala) to find if a sub-array has sum with it’s element as zero.\n",
    "Ex: in above l2 we have 2+3-5=0\n",
    "\n",
    "o/p should be 1 if it exists and 0 if it doesn’t\n",
    "\n",
    "Q\n",
    "Movie_actors\n",
    "Mid\taid\tmcid\n",
    "\n",
    "Movies\n",
    "Movie_title\tmid\n",
    "\n",
    "\n",
    "Write a query to print movie_title where the actors have worked in more than one movie.\n",
    "Ans.\n",
    "Select m.movie_title \n",
    "from movies m join movie_actos ma\n",
    "On ma.mid=m.mid\n",
    "where m.aid in (select m.aid from movie_actos group by aid having count(*)>1);\n",
    "=======================================================================\n",
    "Dev’s legato\n",
    "======================================================================\n",
    "\n",
    "Q1. Joins in SQL \n",
    " No of records after applying inner, left outer and right outer join\n",
    "\n",
    "Q2 .-  row num, rank and dense_rank ??\n",
    "\n",
    "Q3. - unix shell script to read a text file “|” delimited , and print the third column?\n",
    "\tExplained in spark scala\n",
    "Q.4 - one file query.hql  is given with a simple query \n",
    "\n",
    "“Select * from db.tablename where id = some_number\n",
    "Modify the query.hql to accept db , tbl name and id from the hive  CLI while executing the file.\n",
    "Hint = Hive config\n",
    "\n",
    "hive -f filename.hql\n",
    "\n",
    "beeline -u \"jdbc:hive2://master01:2181,master02:2181,master03:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2\" -f file.hql --hivevar HDFSDIR=/tmp/folder\n",
    "\n",
    "Q5. -  analytical functions in Hive   \n",
    "\n",
    " ========================Cognizant -Dheeraj===============================\n",
    "\n",
    "1.\tRemove duplicate records from hive\n",
    "INSERT OVERWRITE TABLE dup_demo SELECT DISTINCT * FROM dup_demo;\n",
    "\n",
    "2.\tWord count program using df/ spark sql\n",
    "\n",
    "3.\tWhat happens when cache memory full \n",
    "\n",
    "4.\tSpark Architecture\n",
    "\n",
    "5.\tCase class\n",
    "\n",
    "6.\tDAG\n",
    "\n",
    "7.\tLineage\n",
    "=====================================================================\n",
    "DUNNHUMBY Dev\n",
    "===================================================================\n",
    "\n",
    "Hive - partition , bucketing (scenario based)\n",
    "\n",
    "Spark - repartition , coalesce (concept) ,What is broadcast in spark\n",
    "Scenario based question - cache, persists\n",
    "\n",
    "Sql queries == PostgreSQL exercises (pgexercises.com)\n",
    "PostgreSQL exercises (pgexercises.com)\n",
    "\n",
    "Store \t| Recpt_num    | date \t\t| prod_code \n",
    "------------------------------------------------\n",
    "S01     | R0105        |  2019-01-05|  P12321\n",
    "S01     | R0158        |  2019-01-09|  P12GBY\n",
    "S02     | R487          |  2019-05-12|  P234\n",
    "S02     | R0896        |  2020-01-11|  P9087\n",
    "\n",
    "\n",
    "Write trasformation to create col tran_code using logic \"tran_code => combine store, recpt_num, date ('-' removed)  seperated by '_'. Example value S01_R0105_20190105\"\n",
    "df1.show(false)\n",
    "\n",
    "val df2 = df1.withColum(\"tran_code\", concat(col(\"Store\"),\"_\",col(\"Recpt_num\",\"_\",col(\"date\").replace('-',''))\n",
    "                           \n",
    "              //col._*.mkString(\"-\")  //concat_ws()\n",
    "val dfResults = dfSource.select(concat_ws(\",\",dfSource.columns.map(c => col(c)): _*))\n",
    "\n",
    "                                            \n",
    "l2 = [1,None,2,3,None, None,4,5,None]\n",
    "out = [1,1,2,3,3,3,4,5,5]\n",
    "                                          var last_known = l2(0)\n",
    "                                            for( i <- 1 until l2.lenght)\n",
    "                                            {\t\t\t\t\t\t\t\t\t\n",
    "                                              if(l2(i) == None)\n",
    "                                              {\n",
    "                                                 l2(i) =  last_known \n",
    "                                              }\n",
    "                                              \n",
    "                                              else \n",
    "                                               last_known = l2(i)\n",
    "                                            }\n",
    "\n",
    "Result - got \n",
    "==============================================================\n",
    "======================= Hashed by Deloitte Antriksh) ================\n",
    "\n",
    "Design classes and database for BookMyShow portal\n",
    "---------------------------------------------------------------------------------\n",
    "*Requirements:*\n",
    "- Users should login to start using the portal.\n",
    "- The portal should list down the different cities where the theatres are located.\n",
    "- Once the user selects the city it should display the movies released in that particular city to that user.\n",
    "- Once the user selects the movie, the portal should display the cinemas running that movie and the available shows.\n",
    "- Users should be able to select the show at a particular theater.\n",
    "- The portal should display the seating arrangement of the cinema hall to the user.\n",
    "- Users should be able to select multiple seats according to their choice.\n",
    "- Once seats are selected the user will book the ticket.\n",
    "\n",
    "Users \n",
    "User_id varchar\n",
    "Password password \n",
    "\n",
    "City \n",
    "City_id int\n",
    "City name varchar \n",
    "\n",
    "Theaters \n",
    "Theater_id \n",
    "Theater_name \n",
    "Th_city\n",
    "Location \n",
    "Available_sheets [100,60,40,5,3]\n",
    "\n",
    "show\n",
    "Show_id\n",
    "M_start_time\n",
    "M_end_time\n",
    "\n",
    "\n",
    "Movies \n",
    "Movie_id\n",
    "Movie_name\n",
    "\n",
    "Bookings\n",
    "B_id\n",
    "User_id \n",
    "Theater_id\n",
    "show_id\n",
    "Movie_id\n",
    "seats\n",
    "\n",
    "B01 u01 t23 s1 “avengers” 60,40\n",
    "\n",
    "- The portal should list down the different cities where the theatres are located.\n",
    "Select distinct(city) from theatres;\n",
    "\n",
    "- Once the user selects the city it should display the movies released in that particular city to that user.\n",
    "\n",
    "Q Nosql vs sql datbase\n",
    "==================================================================\n",
    "++++++++++++++++++++ Dev’s paypal ++++++++++++++++++++++++++\n",
    "\n",
    "1 = What is a tree?\n",
    "Advantages of tree DS?\n",
    "\n",
    "Complexity \n",
    "\n",
    "2 - Find the index of a number in a given array?\n",
    " \n",
    "Told him = zipWithIndex method\n",
    " Linear  search \n",
    "\n",
    "Tell me more Optimize way \n",
    "If the array is sorted and  rotated \n",
    "\n",
    "== binary search => could not implement for the rotated array\n",
    "Explain = arr.indexOf(element)\n",
    "\n",
    "3 =  call by value and reference \n",
    "\n",
    "==============================================================================================dev surprize interview with impetus ==================\n",
    "1.\tWorking of HDFS? When you put a file from local into HDFS how does it happen?\n",
    "We can first move the file from our local to the edge node’s local using winscp. \n",
    "Once we have our file in the edge node then we can use the put command to copy the file from the edge node’s local to hdfs.\n",
    "(hadoop fs -put <local path> <hdfs path> \n",
    "or\n",
    " (hadoop fs -copyFromLocal <local> <hdfs>\n",
    "\n",
    "Whenever a file gets stored in hdfs, it stores the file in the form of blocks which are distributed in clusters. For example, If a file is of 1.4gb (1408mb) then it will be divided in 11 blocks, 128 mb each along with its default replication factor.\n",
    "\n",
    "2.\tComponent of Hadoop\n",
    "\t\n",
    "Following are the components that collectively form a Hadoop ecosystem:\n",
    "●\tHDFS: Hadoop Distributed File System\n",
    "●\tYARN: Yet Another Resource Negotiator\n",
    "●\tMapReduce: Programming based Data Processing\n",
    "●\tSpark: In-Memory data processing\n",
    "●\tPIG, HIVE: Query based processing of data services\n",
    "●\tHBase: NoSQL Database\n",
    "●\tMahout, Spark MLLib: Machine Learning algorithm libraries\n",
    "●\tSolar, Lucene: Searching and Indexing\n",
    "●\tZookeeper: Managing cluster\n",
    "●\tOozie: Job Scheduling\n",
    "3.\tComponent of HDFS\n",
    "The main components of HDFS are as described below:\n",
    "NameNode is the master of the system. It maintains the name system (directories and files) and manages the blocks which are present on the DataNodes.\n",
    "DataNodes are the slaves which are deployed on each machine and provide the actual stor­age. They are responsible for serving read and write requests for the clients.\n",
    "Secondary NameNode is responsible for performing periodic checkpoints. In the event of NameNode failure, you can restart the NameNode using the checkpoint.\n",
    "\n",
    "4.\tCreating hive table on an unstructured file (serde properties)\n",
    "\n",
    "5.\tMap side join\n",
    " \n",
    "6.\tPartition \n",
    "7.\tBucketing \n",
    "8.\tFind exception, error and other log from a unstructured file \n",
    "\n",
    "9.\tORC vs Parquet ? why ORC for Hive\n",
    "10.\tBroadcast join\n",
    "11.\tFunction currying\n",
    "12.\tData frame vs dataset ( why dataframe)\n",
    "13.\tMysql query \n",
    "\n",
    "Id, name, subject , marks \n",
    "1 , abc, math , 50 \n",
    "1 , abc, physics,60\n",
    "1 , abc , chemistry, 20\n",
    "\n",
    "Output-  abc, [math,physics, chemistry], 130\n",
    "\n",
    "group_concat ( subject, separator ‘,’ )\n",
    "\n",
    "2. Second highest selling product each day \n",
    "dense_rank()\n",
    "\n",
    "Result - Got \n",
    "=========================surprise interview with legato================\n",
    "\n",
    "Q select count(*) from EMP,DEPT;\n",
    "\tIn the above query, what join will be applied?\n",
    "Ans. \tCartesian join.\n",
    "\n",
    "Why?\n",
    "As we haven’t mentioned the join keys, in all other joins (left, right, inner) we need to mention the key. As we haven’t mentioned it will do the cartesian join.\n",
    "\tCartesian join?\n",
    "\tEx:\n",
    "\tT1\tT2\n",
    "\tC1 \tC1\n",
    "\t1\t23\n",
    "\t2\t3\n",
    "\t4\t1\n",
    "\t1\t\n",
    "\t\n",
    "\tFor every record in T1c1, it will match with all the record in T2C1\n",
    "\tSo it will have a total of 12 records after executing the above query.\n",
    "\n",
    "Q\n",
    "select count(*) from EMP left outer join DEPT on EMP.deptno=DEPT.deptno;\n",
    "\n",
    "Do it for left, right, inner\n",
    "emp\n",
    "10\n",
    "20\n",
    "30\n",
    "40\n",
    "50\n",
    "60\tdept\n",
    "10\n",
    "90\n",
    "20\n",
    "\n",
    "\n",
    "Ans. 7\n",
    "\n",
    "Q\n",
    "val a = \"Hi Antriksh Kethwas\"\n",
    "o/p : ('H',1),('i',2),(' ',2)\n",
    "\n",
    "Write code in scala to count each character out of string\n",
    "\n",
    "========================= =========================\n",
    "1.\tTypes of file format\n",
    "2.\tCan we load orc file format directly into hive table or have to use any other approach\n",
    "3.\tWrite spark program to convert column into row\n",
    "4.\tHow to eliminate the special character from file then load into hive table\n",
    "5.\tYou have two table orders, a customer table. Find total amount for each month among the customer whose age is greater then 18\n",
    "6.\tFind the second highest salary from the emp table\n",
    "7.\tFind the common record from the two table without using inner join\n",
    "\n",
    "========================= Dev impetus last round =========================\n",
    "1.\tHow to create a partition  table in hive , write a syntax?\n",
    "\n",
    "2.\tHow to load data in static  partition \n",
    "3.\tExplain inheritance by creating classes to calculate area of shapes ( square, circle,rectangle etc).\n",
    "4.\tExplain word count program in map reduce phase.( input to the mapper , output of the mapper, input to the reducer and output to the reducer with internal working)\n",
    "5.\tWrite a code to read data from hive and add a column based on the two columns if the value of both the column is same)\n",
    "6.\tHow to send configuration files to all the nodes while submitting a spark job.\n",
    "7.\tTypes of partition in spark\n",
    "Ans.\n",
    "\tThere are two types of partitions, hash partition and range partition. The choice on which range or hash partition will be applied depends on the following factors\n",
    "a.\tAvailable resources — Number of cores on which tasks can run on.\n",
    "b.\tExternal data sources — Size of local collections, Cassandra table or HDFS file determine number of partitions.\n",
    "c.\tTransformations used to derive -- RDD There are a number of rules to determine the number of partitions when a RDD is derived from another RDD.\n",
    "C1 c2 c3\n",
    "1 2 3\n",
    "1 2 3\n",
    "\n",
    "Write a query to get duplicate records ?\n",
    "\n",
    "Select c1,c2,c3 from t1 group by c1,c2,c3 having count(*) >1;\n",
    "\n",
    "Write a query to get the employee names those are not Manager using JOIN\n",
    "\n",
    "=========================Antriksh MIQ  R2===============================\n",
    "1.\tVendor of your Hadoop distribution /cluster ?\n",
    "2.\tHow do dataframes code get internal optimization ??\n",
    "3.\tHow does the catalyst optimizer work internally?\n",
    "4.\tRead a file in a data frame and count the number of shows watched and total views ?\n",
    " \n",
    "Theboys avengers goodwillhunting Theboys\n",
    "5.\tOptimization you practice in spark jobs?\n",
    "\n",
    "\n",
    "========================Antriksh Legato R2=================================\n",
    "Question - \n",
    "\n",
    "tab A\n",
    " c1\ttab B\n",
    "c1\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\t\n",
    "1\n",
    "2\n",
    "3\n",
    "\n",
    "\n",
    "Write a sql query to select only rows in tableA which are not matching in tableB. In above case, it is 4 & 5.\n",
    "\n",
    "Ans.\n",
    "select * from \n",
    "\tA left join B\n",
    "\ton A.c1=B.c1\n",
    "\twhere A.c1 not in (select * from B);\n",
    "\n",
    "\n",
    "Q In an employee table, find the 5th highest salary.\n",
    "\n",
    "Q 5th highest sal in employee table ?\n",
    "\n",
    "Q (Scenario based questions on project in my organization)\n",
    "\n",
    "Q How do you insert new partition data in a hive table?\n",
    "Result - got \n",
    "===================extra questions==========================\n",
    "1.How can we avoid/manage or clear hive_staging_date directories when we create or insert a table in hive ?\n",
    "2. Which is faster beeline or hive shell ?\n",
    "3. How does hive know that new data is added ? Does it have a mechanism to check data updates from a file on which the table is built ?\n",
    "Drop an external table along with data\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS names_text(\n",
    "  a INT, b STRING)\n",
    "  ROW FORMAT DELIMITED\n",
    "  FIELDS TERMINATED BY ','\n",
    "  STORED AS TEXTFILE\n",
    "  LOCATION '/user/andrena'\n",
    "  TBLPROPERTIES ('external.table.purge'='true');    \n",
    "\n",
    "ALTER TABLE addresses_text SET TBLPROPERTIES ('external.table.purge'='false');\n",
    "  \n",
    "=======================To The New - Antriksh============================\n",
    "\n",
    "Q add a new column “grand total ” in the emp table against each employee record which will show the grand total of the department of that employee. You have input as below table:\n",
    "\n",
    "table emp\n",
    "empId empName sal grandTotal depId \n",
    "1\t\t   a\t35\n",
    "2\t\t   b\t6\n",
    "34\t\t   a\t35\n",
    "4\t\t   b\t6\n",
    "\n",
    "My approach:\n",
    "grandtotal of dep\n",
    "\n",
    "val depDf = empDf.groupBy(col(\"depId\")).agg(expr(\"sum(sal)\").as(\"grandTot\").select(col(\"depId\"),col(\"grandTot\")\n",
    "\n",
    "\n",
    "depDf2.join(broadcast(empDf), depDf2.depId===empDf.depId, left)\n",
    "\n",
    "Q input = \"baloon\", write a function to return list of characters repeated more than once.\n",
    "\n",
    "val input = \"baloonbl\"\n",
    "\n",
    "def charCount(inp: String) = {\n",
    "    \n",
    "     inp.groupBy(x=>x).map(x=>(x._1,x._2.size)).filter(x=>x._2>=2).keys\n",
    "}\n",
    "charCount(input).foreach(println)\n",
    "\n",
    "Q .  \n",
    "empId, sal\n",
    "12\t400\t\n",
    "13\t233\t\n",
    "14\t111\t\n",
    "\n",
    "select empId,sal, rank() over (partition by dept order by sal desc) as salRank from empTable where salRank=2;\n",
    "\n",
    "=====================Devs capgemini ==================================\n",
    "I gave answer to all the technical questions\n",
    "But he was not convinced regarding project or work i do,\n",
    "Asked about => \n",
    "you not a part of deployment team\n",
    "Deployment tool used to deploy the project \n",
    "Data pipeline \n",
    "File format you used \n",
    "\n",
    "Important == guy in the panel was MC with no manners \n",
    "====================================================================\n",
    "Hi!!!!!\n",
    "Please also add the interviewer's name while writing interview experience.\n",
    "=====================================================================\n",
    "Apple: First round interview for Devendra Kumar with Santhana Kumar Meikandan on 27th July 2021 @\n",
    "=====================================================================\n",
    "1- introduction \n",
    "2- detailed description about work\n",
    "3- project in details , your contribution, challenges , other \n",
    "4- let assume we have a job running daily against the data we collect daily. And the job    usually takes a max of 4 hour to complete. one certain day we received the data (same size) and we ran the job and it took around 7 hours. \n",
    "What could be the issue ?How would you approach optimizing it?\n",
    "5- Given an array of integer numbers and an integer target, return indices of the two numbers such that they add up to the target.\n",
    "Input: nums = [3,2,4], target = 6\n",
    "[2,3,3,4,8,9] // nlogn +  n \n",
    "//O(nlogn)\n",
    "//1 = \n",
    "start ,end \n",
    "==\n",
    "7\n",
    "Output: [1,2]\n",
    "\n",
    "object Solution {\n",
    "    def twoSum(nums: Array[Int], target: Int): Array[Int] = {\n",
    "        var ans = List[(Int,Int)] ()\n",
    "        var tailed = nums.tail.toSet //n + (n-1) ------\n",
    "        \n",
    "        for( i <- nums ) // n\n",
    "        {\n",
    "            if(tailed.contains(target -i) //n\n",
    "            {\n",
    "               ans +: (nums.indexOf(i),nums.indexOf(target-i))\n",
    "                             \n",
    "              }\n",
    "              tailed = tailed.tail\n",
    "    }\n",
    "    ans\n",
    "}\n",
    "//O(n^2)\n",
    "\n",
    "6- in your job some tasks are taking a lot time and some of them are finishing their process in less time \n",
    "Explain possible issue  and solution \n",
    "\n",
    "===================================================================================================== Antriksh To the New round 2 ==================\n",
    "\n",
    "1- Write a program to check whether a number is palindrome or not ?? using scala , without using predefined functions ?\n",
    "2- write a query in mysql to select alternate records (you do not have any column with sequentially sorted data?\n",
    " \n",
    "Select cl1, clo2,coln, row_number over(order by col1) rn where rn %2 ==0;\n",
    "\n",
    "3- Can we allocate driver memory more than executor memory? If yes ? Explain such cases when you do the same?\n",
    "Ans: And the driver-memory flag controls the amount of memory to allocate for a driver, which is 1GB by default and should be increased in case you call a collect() or take(N) action on a large RDD inside your application. Here 384 MB is the maximum memory (overhead) value that may be utilized by Spark when executing jobs.\n",
    "4- you have 40 GB of executor memory , and the file size you want to process is around 80 GB. How would you process that?\n",
    "80GB / 128MB \n",
    "Transformations +++\n",
    "\n",
    "Some partition + tasks \n",
    "Scheduling tasks on the multiple executors \n",
    "\n",
    "5- what happens when the name node goes down in your cluster?\n",
    " Fsimage + editlog = name node restore\n",
    "6- what happens when the data node goes down in your cluster? \n",
    "7- Mark down correct and incorrect statements ?\n",
    "\n",
    "●\tSelect a,b,max(c) from t1  ()\n",
    "●\tSelect a,b,max(c) from t1 group by a,b,c (correct)\n",
    "●\tSelect a,b,max(c) from t1 group by a,b  (correct)\n",
    "Select a,b,max(c) from t1 group by c  ()\n",
    "A b c \n",
    "antriksh kethwas 40\n",
    "antriksh kethwas 50\n",
    "other name 40\n",
    "\n",
    "antriksh kethwas 50\n",
    "other name 40\n",
    "============================================================================\n",
    "APPLE :- Second round interview for Devendra Kumar with Abhishek Khandelwal on 5th August 2021 @ 4PM IST\n",
    "============================================================================\n",
    "1.\t- introduction \n",
    "2.\t- project and role / responsibilities \n",
    "3.\t- we have a text file size 100 TB having -100 partitions, 1 tb each, what would be the best approach to check whether a given word exists in the file or not ??\n",
    "\n",
    "4.\tHow dynamical allocation works in spark , how spark decides the number of executors ??  details ??\n",
    "5.\tWhat is the difference between sc.textFile and sc.wholeTextFile ??\n",
    "6.\t- given a file like \n",
    "\t1\n",
    "\t2\n",
    "\t3\n",
    "\t.\n",
    "\t.\n",
    "\t1 tb data \n",
    "\n",
    "Val avg =  rdd.reduce(avg)\n",
    "\n",
    "def avg(a: Int , b:Int ): Int ={\n",
    "\n",
    "(a+b )/2\n",
    "}\n",
    "\n",
    "Tell me whether above code will compile or not ;\n",
    "\n",
    "7.\tWrite a program to demonstrate the difference between map and flat map ? \n",
    "8.\tWhat a tail recursion ?? write a program to reverse a string using tail recursion ?\n",
    "object MyClass {\n",
    "    def main(args: Array[String]) {\n",
    "        //@tailrec \n",
    "        def rev (s:String) : String ={\n",
    "           /*  \n",
    "             if (s == \"\")\n",
    "            s \n",
    "            else \n",
    "             rev(s.tail) + s.head \n",
    "             */\n",
    "             s match {\n",
    "              case \"\"  =>  return s\n",
    "              case _  => rev(s.tail) + s.head \n",
    "              \n",
    "             }\n",
    "        }\n",
    "        \n",
    "        println( rev (\"HELLO\"))\n",
    "    }\n",
    "}\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------------------\n",
    "The main difference, as you mentioned, is that textFile will return an RDD with each line as an element while wholeTextFiles returns a PairRDD with the key being the file path. If there is no need to separate the data depending on the file, simply use textFile.\n",
    "When reading uncompressed files with textFile, it will split the data into chunks of 32MB. This is advantageous from a memory perspective. This also means that the ordering of the lines is lost, if the order should be preserved then wholeTextFiles should be used.\n",
    "wholeTextFiles will read the complete content of a file at once, it won't be partially spilled to disk or partially garbage collected. Each file will be handled by one core and the data for each file will be one a single machine making it harder to distribute the load.\n",
    "========================================================================\n",
    "Table 1 - Workflow\n",
    "id, name, wf_code\n",
    "1, test wflof, \"import re; import exp; var a = skfksf\"\n",
    "2, my abc wflow, \"var key = my_passord\"\n",
    "\n",
    "\n",
    "Table 2 - wflow_sec_compliance\n",
    "expression_type, expression_value\n",
    "Slack, [xolb][a-b][0-9]\n",
    "JDBC Pwd, [a-zA-Z][24]\n",
    "\n",
    "\n",
    "sec_compliance\n",
    "id, wf_name, wf_code, is_sec_cmp, matched_string\n",
    "2, my abc wflow, \"var pass\", \"1\",\"password\"\n",
    "\n",
    "========================================================================\n",
    "//val df1 = spark.sql(\"select * from workflow\")\n",
    "//val df2 = spark.sql(\"select * from wflow_sec_compliance\")\n",
    "\n",
    "def is_sec_cmp(str: String, reg:String): Char = \n",
    "{\n",
    "\tval reg2 = reg.r\t\n",
    "\n",
    "\tcase reg:\n",
    "\t\tmatch str\n",
    "}\n",
    "\n",
    "register.udf(\"is_sec_cmp\",is_sec_cmp(str: String, reg:String): Char)\n",
    "\n",
    "val df3 = spark.sql(\"select * from workflow, wflow_sec_compliance\")\t//cartesian\n",
    "\n",
    "//id, name, wf_code, expression_type, expression_value df3\n",
    "//id, name,wf_code,is_sec_cmp,matched_string dff\n",
    "\n",
    "val df4 = df3.withColumn(\"is_sec_cmp\", selectExpr(\"is_sec_cmp(wf_code, expression_value)\")\n",
    "\n",
    "============= MIQ’s 5th round (Antriksh’s int with Rohit) =======================\n",
    "Rohit Srivastava2:09 PM\n",
    "Workflow\n",
    "id, name, wf_code\n",
    "1, test wflof, \"import re; import exp; var a = skfksf\"\n",
    "Rohit Srivastava2:10 PM\n",
    "2, my abc wflow, \"var key = my_passord\"\n",
    "Table 2 - wflow_sec_compliance\n",
    "expression_type, expression_value\n",
    "Slack, [xolb][a-b][0-9]\n",
    "JDBC Pwd, [a-zA-Z][24]\n",
    "Rohit Srivastava2:12 PM\n",
    "sec_compliance\n",
    "id, wf_name, wf_code, is_sec_cmp, matched_string\n",
    "2, my abc wflow, \"var pass\", \"1\",\"passowrd\"\n",
    "You2:34 PM\n",
    "Table 1 - Workflow\n",
    "id, name, wf_code\n",
    "1, test wflof, \"import re; import exp; var a = skfksf\"\n",
    "\n",
    "2, my abc wflow, \"var key = my_passord\"\n",
    "\n",
    "Table 2 - wflow_sec_compliance\n",
    "expression_type, expression_value\n",
    "Slack, [xolb][a-b][0-9]\n",
    "JDBC Pwd, [a-zA-Z][24]\n",
    "\n",
    "sec_compliance\n",
    "id, wf_name, wf_code, is_sec_cmp, matched_string\n",
    "2, my abc wflow, \"var pass\", \"1\",\"passowrd\"\n",
    "\n",
    "\n",
    "     =======================================================================\n",
    "Dev’s Cognizant\n",
    "======================================================================\n",
    "1- project and actual work, pipeline , data source ?\n",
    "2- designing pattern you use in your project\n",
    "3-  hive internal(managed) / external table\n",
    "Can we store managed table’s data in other loc than warehouse dir ?\n",
    "4- syntax to read data in spark \n",
    "  Spark sql command to to get data based on some conditions \n",
    "5- how do you decide executors / executor’s memory / cores \n",
    "  Spark submit syntax\n",
    "6- few more scenario based questions \n",
    "7- répartition / coalesce\n",
    "8- What is COB and role based optimizer \n",
    "Cost Based optimization basically it required admin privileges\n",
    "9- replace null value to 0 while loading data in to hive \n",
    "We can use coalesce\n",
    "10- dataframe vs dataset vs RDD\n",
    "\n",
    "  object Main {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    println(\"Hello world!\")\n",
    "  }\n",
    "}\n",
    "Imagine you are building some sort of service that will be called by up to 1,000 client applications to get simple end-of-day stock price information (open, close, high, low). \n",
    "You may assume that you already have the data, and you can store it in any format you wish. \n",
    "How would you design the client-facing service that provides the information to client applications? \n",
    "You are responsible for the development, rollout, and ongoing monitoring and maintenance of the feed. \n",
    "Describe the different methods you considered and why you would recommend your approach. \n",
    "Your service can use any technologies you wish, and can distribute the information to the client applications in any mechanism you choose. \n",
    "\n",
    "INPUT\n",
    "stock=value\n",
    "//df -> store 1min \n",
    "/data_dump/167293474929.csv  - minute 10000 stocks\n",
    "stock,value\n",
    "TCS,189\n",
    "INFY,990\n",
    "\n",
    "9AM - 4PM\n",
    "10000Stocks per min\n",
    "\n",
    "//open close high low 10000stocks\n",
    "//1000 clients 10000 stock request\n",
    "\n",
    "{ stock1: { open: close: high: low:} }\n",
    "\n",
    "folder ( per min a file ) --> spark streaming  -> (1 min ) (prev , curr_staate) ( initial open , running high , running low )- >\n",
    "\n",
    " (open,close ,high , low  )\n",
    "\n",
    "Given two arrays of integers, compute the pair of values (one value in each array) with the smallest (non-negative) difference. Return the difference.\n",
    "\n",
    "\t\tA[] = {1, 3, 15, 11, 2}\n",
    "\t\tB[] = {23, 127, 235, 19, 8} \n",
    "(11,8) 3\n",
    "\n",
    "n^2 \n",
    "\n",
    "var min_value = Integer.max\n",
    "var ele1 = A(0); var ele2 = B(0)\n",
    "for( i <- A){\n",
    "for( j <- B)\n",
    "\n",
    " if( (i-j).abs < min_value)\n",
    " {\n",
    "    min_value = (i-j)\n",
    " }    \n",
    "min_value\n",
    "}\n",
    "\n",
    "\n",
    "// 1,2,3,11,15\n",
    "// 8,19,23,127,235\n",
    "\n",
    "B.min = 8 \n",
    "\n",
    "order_details\n",
    "\tcustomer_name \t\tproduct_name\t\tquantity\t\tt_price\n",
    "\tjohn\t\t\t        apple\t\t\t      6\t\t\t      10\n",
    "\tjohn\t\t\t        milk\t\t\t      3\t\t\t      5\n",
    "\tjobin\t\t\t        milk\t\t\t      5\t\t\t      8\n",
    "\tjohn\t\t\t        apple\t\t\t      10\t\t\t    18\n",
    "\tjobin\t\t\t        apple\t\t\t      12\t\t\t    20\n",
    "\n",
    "Find the total quantity and total price per customer and per product\n",
    "\n",
    "select customer_name, product_name, sum(quantity), sum(t_price) from order_details group by customer_name , product_name ;\n",
    "\n",
    "Find the customer with highest spending \n",
    "select customer_name , rank( total ) R  over ( cluster by customer_name order by total) from( select customer_name, sum(t_price) total from order_details group by customer_name ) ) t where R =1 ;\n",
    " \n",
    " john 33 \n",
    " jobin 28 \n",
    "jonnny 45\n",
    "\n",
    "\n",
    "Result - no response after HR round (higher CTC expectation) \n",
    "==================================================================\n",
    "Dev’s KPMG \n",
    "==================================================================\n",
    "1.\tSingleton vs companion object \n",
    "2.\tCase class \n",
    "3.\tRecursion vs tail recursion // write a program to find nth fibonacci number using tail recursion \n",
    "4.\tHow to read a file in scala (core scala)\n",
    "5.\tMap vs flatmap  (explain with an example) \n",
    "6.\tSyntax to use map function with a lambda/ anonymous function \n",
    "7.\tWrite a syntax to find duplicates row based on 2 columns using windows function on a data frame\n",
    "No -response after interview\n",
    "==================================================================\n",
    "Dev’s Quantiphi\n",
    "=====================================================================\n",
    "1- lambda function in python and map function  \n",
    "2-  WAP to implement this - \n",
    "input = [\"a\", \"z\", \"a\", \"c\", \"z\"]\n",
    "output -> {\"a\": true, \"z\": true, \"c\": false}  ( True if it appears twice/thrice, False if not )\n",
    "def check ( list : List[Char]) ={\n",
    "\n",
    "   var mapped = list.groupBy(identity).map(x => (x._1, x._2.size) .map( x => (x._1, if (x._2 >=2) true ; else false) \n",
    "  //               \t      a,[a,a]\t\ta->2\t\t\ta->true\n",
    "  //\t\t     z,[z,z]\t\tz->2\t\t\tz-> true\t\n",
    "  //\t\t     c, [c]\t\t\tc->1\t\t\tc-> false\n",
    "  mapped \n",
    "\n",
    "}\n",
    "# mapped = list.groupBy( lambda x : x) .map( lambda y :(y[0],y[1].len)) .map( lambda z : (z[0], 'true' if (z[1] >=2 ) else 'false'))\n",
    "\n",
    "#x => x \n",
    "\n",
    "3- Tuple Vs List in python\n",
    "4- functionalities of airflow  // how can we pass data between 2 taks \n",
    "5- kafka use cases \n",
    "6-  SQL query to find 4th highest salary\n",
    "name , salary\n",
    "7- SQL query to find running average of temp by month\n",
    "year , month, day , temp\n",
    "8- order by vs sort by \n",
    "9- partitioning / bucketing use cases\n",
    "10- RDD use cases ? how does it achieve fault tolerance ?\n",
    "11- cloud services / aws / GCP\n",
    "Result - got \n",
    "==================================================================\n",
    "===============Genpact for Morgan Stanley - Antriksh ================\n",
    "\n",
    "Q Spark engine vs mapReduce engine\n",
    "\n",
    "==========================================================\n",
    "Dev Lumic = > BarRaiser\n",
    "==========================================================\n",
    "1.\tCloud knowledge/Big data tools\n",
    "2.\tGit\n",
    "3.\tCoding/Scripting\n",
    "4.\tData Modeling/Database knowledge\n",
    "Linux - commands( word coung , find vs grep) , concepts , architecture\n",
    "Spark - scenario based , job running , concepts, data frame -coding, \n",
    "Hive -\n",
    "HDFS -\n",
    "Cloud - > EMR,glue, redshift , node failure, \n",
    "\t\tHow to import data from RDBMS into S3\n",
    "Cron scheduler -   * * * * * * * what are 7 * in cron\n",
    "Scala -  code to find largest substring having no duplicate chars\n",
    "Git - revert last commit , git parse , \n",
    "Shell script -\n",
    "SQL: \n",
    "Team1 Team2 MatchResult \n",
    " RR KKR 2 \n",
    " MI CSK 2 \n",
    " RCB KXP 1 \n",
    " DD RR 0 \n",
    " KKR RR 1 \n",
    " CSK RCB 2 \n",
    " KXP DD 2\n",
    "\n",
    " Match Result descriptions: \n",
    " 1 => Match won by Team 1 \n",
    " 2 => Match won by Team 2 \n",
    " 0 => Draw\n",
    "\n",
    "Output should have following columns\n",
    "\n",
    "Team Played Won Lost Draw \n",
    "RR 3 0 2 1 \n",
    "CSK 2 1 1 0 \n",
    "RCB 2 2 0 0\n",
    "\n",
    "Q. Length of the longest substring without repeating characters\n",
    "Sample Input: \"aabbcd\"\n",
    "Output: \"bcd\"\n",
    "Result - I declined second round invitation \n",
    "====================================================================\n",
    "Devs Altimetrik \n",
    "====================================================================\n",
    "Round 1 => basic and famous questions, hive spark, scala, sql \n",
    "\n",
    "Round 2 =>  spark concepts , scala, sql queries\n",
    "Keep the updated records + no old ad new extra records left \n",
    "Input1: old \n",
    "Name Phone Address\n",
    "Jack xxxx BLR\n",
    "John yyyy CH\n",
    "Santosh xxxx HYD\n",
    "\n",
    "Input2: updated \n",
    "Name Phone Address\n",
    "Jack xxxx ND\n",
    "John yyyy MA\n",
    "Cherukuri bbbb Del\n",
    "\n",
    "Output:\n",
    "Name Phone Address\n",
    "Jack xxxx ND\n",
    "John yyyy MA\n",
    "Santosh xxxx HYD\n",
    "Cherukuri bbbb Del\n",
    "\n",
    "Round 3 - EL , manager \n",
    "Result - not processing further due to higher salary expectation\n",
    "=======================================================================\n",
    "Neeraj’s PWC Tech-R1 \n",
    "=======================================================================\n",
    "(1)\tYou have been given (a,b,a,c,f,b,e,d) characters you have to put repeated characters in file1 and characters with exact count in file2.\n",
    "\n",
    "val input = sc.textfile()\n",
    "\n",
    "val rdd1 = input.flatmap(seperatedBy(\",\"));\n",
    "val rdd2 = rdd1. map (x => (x,1))\n",
    "val rdd3 = rdd2 .reduceByKey((x,y) =>(x+y))\n",
    "\n",
    "val rdd4 = rdd3.  \n",
    "\n",
    "(a,1)\n",
    "(b,2)\n",
    "\n",
    "(2)\twrite a query for the product which has no transaction from the below table details. \n",
    "\n",
    "\n",
    "product -> pid, name, price\n",
    "user -> uid, name, email\n",
    "trxn -> trxid, uid,pid, date\n",
    "\n",
    "select p.pid, p.name from product p left  outer join trxn t on \n",
    "p.pid = t.tid where t.txid=null;\n",
    "\t\t\t\t\t(or)\n",
    "select * from product where pid not in(select distinct pid from trxn);\n",
    "\n",
    "(3)\tDifference between groupByKey() and reduceByKey().\n",
    "(4)\tBrief Discussion on the project.\n",
    "\n",
    "=======================================================================\n",
    "Neeraj’s PWC Tech-R2 \n",
    "=======================================================================\n",
    "\n",
    "(1)\tDetails Discussion on the project. What Challenges I have faced till now in the project and how to overcome them and what is your project pipeline.\n",
    "\n",
    "(2)\tYou have been given below two dataframes  write your code -\n",
    "\n",
    "DF1:User\n",
    "Cols: UserId, Username\n",
    "\n",
    "DF2: Events\n",
    "cols: EventId, EventType, UserId, Timestamp\n",
    "\n",
    "1.Display the count of events for each user\n",
    "DF: Username, CountofEvents\n",
    "=======================================================================\n",
    "Neeraj’s Morgan Stanley Tech-R1 \n",
    "=======================================================================\n",
    "(1) All optimization techniques in spark.\n",
    "(2) replica Factor in \n",
    "(3) All joins in SQL\n",
    "(4) what do u mean by stages in Spark\n",
    "(5) how to read csv file describe\n",
    "(6) what is the advantages to store the data in hdfs with compare other files\n",
    "\n",
    "(7) table-Result\n",
    "\tdept   dept_name \tStd_name \tstd_total_mark \n",
    " Q- Find out the student details who is having 2nd highest marks for each department.\n",
    "\n",
    "(8) Do you know shell scripting?\n",
    "(9) Describe a little bit about your project.\n",
    "\n",
    "\n",
    "(10)\n",
    "table A\n",
    "col1\n",
    "1\n",
    "1\n",
    "null\n",
    "\n",
    "table B\n",
    "Col1\n",
    "1\n",
    "1\n",
    "1\n",
    "null\n",
    "\n",
    "inner join- 6 \n",
    "left outer join - 7\n",
    "right outer - 7\n",
    "full cross - 12\n",
    "\n",
    "(11)\n",
    "table A\n",
    "col1\n",
    "1\n",
    "1\n",
    "\n",
    "table B\n",
    "Col1\n",
    "1\n",
    "1\n",
    "1\n",
    "\n",
    "inner join-  \n",
    "left outer join - \n",
    "right outer - \n",
    "full cross - \n",
    "=====================================================================\n",
    "Dev’s fragma data\n",
    "====================================================================\n",
    "1- onLine test \n",
    "A-    Pyspark project ( implement 5 functions , reading data, filtering , joining , and writing back to HDFS use partition by a column) \n",
    "B- Spark questions - multiple choice \n",
    "C- Hive Questions -multiple choice\n",
    "Video interview \n",
    "1- project \n",
    "2- some concepts of spark\n",
    "3- sql normalization\n",
    "4- ask about a table - never heard that table name\n",
    "5- cloud - no experience \n",
    "6- databricks -  no experience \n",
    "\n",
    "====================================================================\n",
    "Dev’s IQVIA 1st R\n",
    "====================================================================\n",
    "\n",
    "7- database vs data warehouse\n",
    "8- spark internal , what happens when we run a spark job\n",
    "Final round on - 29th sept\n",
    "\n",
    " SPARK -SCALA- \n",
    "1.    Syntax to create SparkSession? What is local in. master(“local”) \n",
    "2.    Data frame dataset\n",
    "3.    Case class (use cases), syntax to create case class?\n",
    "4.    Chase vs persist? How does memory and disk work?\n",
    "5.    Different write modes in spark?\n",
    "Ans : Save Modes/Write Modes\n",
    "Save operations can optionally take a SaveMode, that specifies how to handle existing data if present. It is important to realize that these save modes do not utilize any locking and are not atomic. Additionally, when performing an Overwrite, the data will be deleted before writing out the new data.\n",
    "Scala/Java\tAny Language\tMeaning\n",
    "SaveMode.ErrorIfExists (default)\t\"error\" or \"errorifexists\" (default)\tWhen saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown.\n",
    "SaveMode.Append\t\"append\"\tWhen saving a DataFrame to a data source, if data/table already exists, contents of the DataFrame are expected to be appended to existing data.\n",
    "SaveMode.Overwrite\t\"overwrite\"\tOverwrite mode means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.\n",
    "SaveMode.Ignore\t\"ignore\"\tIgnore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected not to save the contents of the DataFrame and not to change the existing data. This is similar to a CREATE TABLE IF NOT EXISTS in SQL.\n",
    "\n",
    "6.    What are all the transformations you have used in your project?\n",
    "7.    Wide vs narrow transformations\n",
    "8.    Traits??\n",
    "9.    Diamond problem?\n",
    "10. Higher order functions \n",
    "11. World count program in spark\n",
    "12. What is Set in Scala?\n",
    "13. What is a map Partition?\n",
    "14. What is a map in Scala?\n",
    "15. val states = Map(\"AL\" -> \"Alabama\", \"AK\" -> \"Alaska\", \"AZ\" -> \"Arizona\")\n",
    "find the value of “AL”\n",
    "val ans = states(\"AL\")\n",
    "16. What is a static variable in Scala? Var vs Val\n",
    "17. What error does it throw when you try to reassign a value in a static variable?\n",
    "18. Error vs exceptions\n",
    "19. What optimization techniques do you use?\n",
    "20. Reading parquet file in spark, syntax\n",
    "HIVE-\n",
    "1.    Creating external table, syntax\n",
    "2.    Creating partitioned table\n",
    "3.    Partition and bucketing\n",
    "4.    Loading data into Hive table\n",
    "5.    Adding a partition in a hive table  \n",
    " Ans:  ALTER TABLE test\n",
    "                     ADD PARTITION (dt=’2014-03-05’)\n",
    "                     LOCATION ‘s3://test.com/2014-03-05’\n",
    "6.    Removing duplicates from hive tables\n",
    "7.    How to delete data with with schema of external table  \n",
    "SQL-\n",
    "1.    Nth highest salary from a table department wise\n",
    "=======================================================================\n",
    "Dev’s IQVIA 2nd R\n",
    "====================================================================\n",
    "1.\tWrite a word count program \n",
    "2.\tExplain higher order function with an example // write code \n",
    "3.\tShow latest / updated record \n",
    "EmpID EMPNM City Date\n",
    "1 Jon Goa 201811\n",
    "1 Jon Delhi 201901\n",
    "2 Sonu Kolkatta 201901\n",
    "2 Sonu Delhi 201709\n",
    "\n",
    "select tmp.EmpID ,tmp.EMPNM , tmp.City from ( select EmpID ,EMPNM , City, dense_rank( ) over ( order by date  desc) rnk from Employee ) as tmp where rnk = 1 ;\n",
    "\n",
    "Show employee details with all the cities he has visited => \n",
    "select EmpID, EMPNM , group_concat(City) from Employee ;/// ( group by EmpID,EMPNM)\n",
    "4.\tSpark / concepts \n",
    "5.\tSpark submit commands \n",
    "> spark-submit --class class1 spark_job.jar --deploy-mode client /cluster  --master yarn --conf   --executor-core 5 --executor-memory 18G --num-executors 20 //--max-executors \n",
    "--driver-memory 50G \n",
    "\n",
    "6.\tAdd a column before given column\n",
    "7.\tData frame vs Data set \n",
    "8.\t\n",
    "Result :- HSE ….\n",
    "\n",
    "======================================================================\n",
    "DEv’s Tekion 1st R coding \n",
    "========================================================================\n",
    "1.\tGiven n sorted linked lists , merge them in sorted manner?\n",
    "1 -> 2 -> 3 \n",
    "3 -> 4 -> 5\n",
    "5 -> 9 -> 10\n",
    "…….\n",
    "Output - 1 -> 2 -> 3 -> 3 -> 4 -> 5 -> 5 -> 9 -> 10 \n",
    "\n",
    "2.\tGiven a list of Strings , combine all the anagrams together in List of lists?\n",
    "input: strs = [\"eat\",\"tea\",\"tan\",\"ate\",\"nat\",\"bat\"]\n",
    "Output: [[\"bat\"],[\"nat\",\"tan\"],[\"ate\",\"eat\",\"tea\"]]\n",
    "\n",
    "def groupAnagrams( strs):\n",
    "      result = {}\n",
    "      for i in strs:\n",
    "         x = \"\".join(sorted(i))\n",
    "         if x in result:\n",
    "            result[x].append(i)\n",
    "         else:\n",
    "            result[x] = [i]\n",
    "      return list(result.values())\n",
    "\n",
    "print(groupAnagrams([\"eat\", \"tea\", \"tan\", \"ate\", \"nat\", \"bat\"]))\n",
    "\n",
    "\n",
    "Rejected \n",
    "\n",
    "================================================================\n",
    "DEv’s EY \n",
    "===================================================================\n",
    "Telephonic => 20 mins \n",
    "Spark,Scala,Hive   concepts\n",
    "\n",
    "Video interview => \n",
    "Spark internal working \n",
    "Scenario based question on  partition after  left join  on 2 dfs having 10 and 20 partitions \n",
    "Dag , how it works \n",
    "stages , scenario based on transformation , how many stages \n",
    "Code on Dataframe => return all the columns  as a list of string\n",
    "    withcolum() with match case \n",
    "\n",
    "=======================================================================\n",
    "Neeraj’s Quantiphi Assessment round \n",
    "=======================================================================\n",
    "-\tThere will be 20 MCQ questions based on sql, hive, spark, scala some coding input/output\n",
    " \t\n",
    "=======================================================================\n",
    "Neeraj’s Quantiphi Tech-R1 \n",
    "=======================================================================\n",
    " You have been given below 2 tables\n",
    "\n",
    "Table: Orders\n",
    "\n",
    "Column                  Type\n",
    "OrderID                 Integer\n",
    "CustomerID         Integer\n",
    "Purchase_date   Timestamp\n",
    "ProductID    \t Integer\n",
    "Product                String\n",
    "Quantity              Integer\n",
    "Price         \tFloat\n",
    "\n",
    "Table: Customers\n",
    "\n",
    "Column                  Type\n",
    "ID                     \t   Integer\n",
    "name                      String\n",
    "Contact_no           Integer\n",
    "\n",
    "Q1-Query to find all the customer names who have purchased the top 5 selling products in the Year 2018.\n",
    "\t\n",
    "/* select distinct customers.name from orders join customers on orders.cid=customers.id where productid in \n",
    "\t(select t1.productid from (\n",
    "\tselect productid, sum(quantity) as sale from orders where year(purchase_date)='2021' group by productid order by sale desc limit 3) t1\n",
    "    ) ;\n",
    "  by antriksh*/\n",
    "\n",
    "\n",
    "\n",
    "Ans - Suppose data is available in below format. \n",
    "\n",
    "101 123 todaysdate 100 TV 3 Rs90000\n",
    "102 234 todaysdate 100 TV 2 Rs60000\n",
    "103 123 todaysdate 200 SamusungMobile 1 Rs 50000\n",
    "104 234 todays date 200 SamusungMobile 2   Rs100000\n",
    "\n",
    "select (select from dense_rank()over(order by price desc)prc)temp where temp.prc=1 and date between\n",
    "\n",
    "Q2 - You have been given below table. Write a sql query to delete the duplicate records\n",
    "\n",
    "CREATE TABLE sales_team_emails (\n",
    "   id INT AUTO_INCREMENT,\n",
    "   sales_person_name VARCHAR(255),\n",
    "   sales_person_email VARCHAR(255),\n",
    "   PRIMARY KEY (id)\n",
    ");\n",
    "\n",
    "Create table unique_records as (Select id,sales_person_name, sales_person_email from sales_team_emails group by id,sales_person_name, sales_person_email); solution by Antriksh\n",
    "\n",
    "Insert overwrite table sales_team_emails (select distinct * from sales_team_emails); \n",
    "in hive - solution by dev\n",
    "\n",
    "Example: \n",
    "+----+-------------------+----------------------+--------------------------------\n",
    "| id | sales_person_name | sales_person_email   \t       |\n",
    "+----+-------------------+----------------------+--------------------------------|\n",
    "|  1 | Aditi \t   \t |  aditi@hotmail.com \t        |\n",
    "|  2 | James F Stone \t |  jamesstone@yahoo.com   |\n",
    "|  3 | James Stone   \t |  jamesstone@yahoo.com   |\n",
    "|  4 | Aditi Sharma\t |  aditi@hotmail.com              |\n",
    "+----+-------------------+----------------------+----------------------------------\n",
    "\n",
    "Partial Ans(Which I tried)\n",
    "with EMPYEECTE As (select *, row_number() over (partition by id order by id) as row_number from sales_team_emails ) \n",
    "delete from EMPYEECTE where row_number >1\n",
    "\n",
    "Q3- Write a hive query to view all the databases whose name begins with “db”\n",
    "\tShow databases like db “db*”;\n",
    "Q4- Write word count program in spark.\n",
    "Q5- What is a higher order function in scala.\n",
    "Q6- What is a wide and narrow transformation?\n",
    "\n",
    "=======================================================================\n",
    "Neeraj’s Quantiphi Managerial Round \n",
    "=======================================================================\n",
    "●\tDiscussion regarding your project, issues which you have faced in your project.\n",
    "●\tHow will you handle your junior’s if they work under you? How is your interaction with the client?\n",
    "\n",
    "\n",
    "=======================================================================\n",
    "Neeraj’s CitiusTech-R1\n",
    "=======================================================================\n",
    "Q1 - What do you mean by decorators ?\n",
    "Q2 - You have been given below table\n",
    "eName\t\t eSalary \teDepartment\n",
    "Sat \t\t5555 \t\tHR\n",
    "mat \t\t6767 \t\tIT\n",
    "\n",
    "Now you have to add new column ‘current_date’ \n",
    "\n",
    "eName\t\t eSalary \teDepartment \t\tcurrent_date\n",
    "Sat \t\t5555 \t\tHR \t\t\t09-04-2021    \n",
    "mat \t\t6767 \t\tIT \t\t\t09-04-2021   \n",
    "\n",
    "Suggestion -  Here you can use withColumn() to add a new column. \n",
    "\n",
    "Q3 - Write Sql query to find out 2nd highest salary of each department.\n",
    "Ans- select tmp.emapname,tmp.sal,tmp.dept (select empname,sal,dept dens_rank() over(group by dept            order by sal desc)rank) tmp where tmp.rank=2;\n",
    "\n",
    "Q4- You have been given two table- Table1 & Table2\n",
    "table1\n",
    "ID\n",
    "1\n",
    "1\n",
    "null\n",
    "0\n",
    "0\n",
    "3\n",
    "\n",
    "table2\n",
    "ID\n",
    "0\n",
    "0\n",
    "1\n",
    "null\n",
    "4\n",
    "6\n",
    "7\n",
    "\n",
    "-\tWhat will be the total record  for inner join.(Ans -6) \n",
    "\n",
    "==============================================================\n",
    "Neeraj’s CitiusTech-R2 (Managerial)\n",
    "=============================================================\n",
    "●\tBrief discussion about your project, what is your project flow ?\n",
    "●\tNo. of team members in your project, What is your role ?\n",
    "●\tHow you inject spark with hive\n",
    "\n",
    "==========================================================\n",
    "Neeraj’s Scienaptic-R1\n",
    "=============================================================\n",
    "Q1- You have been given a table with 2 columns A & B. Replace all ‘0’ with ‘*’\n",
    "\n",
    "table   \t\t\t\ttable  \n",
    "A    B\t\t\t\tA    B\n",
    "1    0\t\t\t\t1    *\t\n",
    "0    1\t\t=>\t               *    1\n",
    "0    0\t\t\t\t*     *\n",
    "1    1\t\t\t\t1    1\n",
    "\n",
    "Partial Ans-\n",
    "val rdd1 = rdd.flatmap(x=> x.split(\" \"));\n",
    "val rdd2 =  rdd1.map(x=> x.split(\" \")(0),x.split(\" \")(1)) \n",
    "val rdd3 = rdd2.filter(x => if(x._2 = 0) {(x._1,*)})\n",
    "\n",
    "Q2- You have a csv format file with 1000 columns and 1cr of row. How will you optimize it at the time of reading and processing ?\n",
    "Q3- What do you mean by rolling window, tumbling window, sliding window ?\n",
    "Q4- When can we get Out Of Memory in the master node \n",
    "Q5- Difference between groupByKey() and reduceByKey().\n",
    "\n",
    "==================================================================\n",
    "Neeraj’s Altimetrik-R1\n",
    "=================================================================\n",
    "\n",
    "Q1- Given a table containing bus route details, write a query/program to find all possible routes between two stations and time between them.\n",
    "\n",
    "Input :\n",
    "\n",
    "Busid \tStation \tArrival_Time\n",
    "1 \tst1 \t\t4:20\n",
    "1 \tst2 \t\t5:30\n",
    "1 \tst3 \t\t7:30\n",
    "\n",
    "Output:\n",
    "\n",
    "Busid \tRoute \t\tDuration\n",
    "1 \tst1-st2 \t\t70 \n",
    "1\tst2-st3 \t\t120\n",
    "1 \tst1-st3 \t\t190\n",
    "\n",
    "Q2- what is your project pipeline?\n",
    "Q3- which mode you use in your project client mode or cluster mode and what is the reason to use that mode?\n",
    "Q4- How to debug jobs ?\n",
    "Q5- What optimization do you use in your project ?\n",
    "====================================================================\n",
    "Naman’s Legato\n",
    "======================================================================\n",
    "\n",
    "Q1. Difference between managed and external tables in hive?\n",
    "Q2. coalesce vs repartition ?\n",
    "Q3. What is DAG?\n",
    "Q4. What is a trait?\n",
    "Q5. Some questions related to the project, where to take your data and where to dump your data ?\n",
    "Q6. Write a query to find the nth highest salary?\n",
    "Q7. Which model are you using agile and waterfall?\n",
    "Q8. What is left equi join?\n",
    "Q9. Difference between reducebykey and groupbykey?\n",
    "Q10. Transformations, wide transformation vs narrow transformation?\n",
    "Q11. Higher order functions?\n",
    "Q12. Which technique did you use for join optimization?\n",
    "\n",
    "=====================================================================\n",
    "\t\t\t\t\tNaman’s Impetus\n",
    "=======================================================================\n",
    "\n",
    "Q1. Hive Architecture?\n",
    "Q2. Spark Architecture?\n",
    "Q3. Interface in scala?\n",
    "Q4. Some questions related to project flow?\n",
    "Q5. Client mode vs Cluster mode?\n",
    "Q6. What about executor,core,ram?\n",
    "\n",
    "=======================================================================\n",
    "\t\t\t\t\tNaman’s Tiger Analytics\n",
    "========================================================================\n",
    "Q1. Input :- [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "       Output :- out1 - [1,5,9,13]\n",
    "          Out2 - [2,6,10,14]\n",
    "         Out3 - [3,7,11,15]\n",
    "         Out4 : [4,8,12,16]\n",
    "\n",
    "Q2. Input :- [math - 40, physics-50, chemistry - 80, hindi-90, english - 70]\n",
    "\tThe marks of the subject is greater than 60 then print all the subjects?\n",
    "       Output :- [name: vishnu {chemistry,hindi,english}]  \n",
    "\n",
    "Q3 There will be a table cust_id,sales_order,quantity,time. So you write a query who will be the max sales of this table?\n",
    "\n",
    "Q4: Write a word count problem using rdd?\n",
    "\n",
    "Q5. There are some data like :-\n",
    "1.\t john cena\n",
    "2.\t rohit sharma\n",
    "3.\tchris gayle\n",
    "You can write a program using dataframe/dataset and change the first lower letter to upper letter\n",
    "Output :- \n",
    "1. John \n",
    "2. Rohit Sharma\n",
    "3. Chris Gayle  \n",
    "\n",
    "=================================================================\t\t\t\t\tNaman’s Capgemini\n",
    "=================================================================\n",
    "Q1. Which optimization technique is used in spark?\n",
    "1) Broadcast join\n",
    "2) Cache/Persist as and when needed\n",
    "3) Filtering unnecessary data before doing joins\n",
    "4) Apart from 1,2 and 3, we use delta format which supports data skipping/optimize/z-order\n",
    "5) Enabled Adaptive Query Execution at cluster level.\n",
    "6) resource level optimization\n",
    "7) use kryo serializer\n",
    "Q2. Managed table vs external table?\n",
    "Q3. Write a program to find a number that will be odd or even?\n",
    "Q4. Write a query to find the max salary?\n",
    "Q5. write a query to show the 5 names whose salary will be highest?\n",
    "Q6. Difference between map vs flatmap?\n",
    "Q7. How many transformations are used in a project?\n",
    "Q8. What are shared variables?\n",
    "Q9. What is case class?\n",
    "\n",
    "==========================================================\n",
    "\t\t\t\t\tNaman’s Coforge\n",
    "==========================================================\n",
    "Q1. Write a command to edit a file in linux?\n",
    "Q2. Write a command to validate a file in linux?\n",
    "Q3. Difference between  external table and managed table?\n",
    "Q4. Write a query to find the 6th highest salary?\n",
    "Q5. Write a word count program in rdd?\n",
    "Q6. Difference between spark context and spark session ?\n",
    "Q7. Class vs case class?\n",
    "Q8. What is a higher order function ?\n",
    "Q9.  Differ between repetition and coalesce?\n",
    "\n",
    "========================Bhavesh - Publicis sapient=================\n",
    "\n",
    "1. Spark Context vs Spark Session?\n",
    "2. How Many Spark Context are Allowed in one Spark Session?\n",
    "3. Repartitioning vs Coalesce?\n",
    "4. Partitioning vs Bucketing?\n",
    "5. Sql query for second highest salary?\n",
    "6. Which version of spark are you using?\n",
    "7. What is case class, how is it different from Normal class and Object in scala?\n",
    "8. What happens internally when you do spark-submit?\n",
    "9. What is broadcast join?\n",
    "10. Can you run different spark jobs in the same cluster with different configurations?\n",
    "\n",
    "==========================Bhavesh - Cognizant ====================\n",
    "\n",
    "1. Project Overview?\n",
    "2. Given Spark cluster with 5 executors with 5 cores for each, how many tasks can run in parallel?\n",
    "3. Explain Spark Architecture?\n",
    "4. Word count program?\n",
    "5. Find the Highest salary for each department?\n",
    "\n",
    "=========================Bhavesh - Reliance Jio====================\n",
    "\n",
    "1. What are your day-to-day responsibilities in the current project?\n",
    "2. Explain about your project Architecture?\n",
    "3. Transactional vs Non-Transactional database?\n",
    "4. Managed table vs External Table?\n",
    "5. Singleton object vs singleton class?\n",
    "6. Window functions LEAD() vs LAG()?\n",
    "7. Dataset vs RDD?\n",
    "8. Partitioning vs Bucketing?\n",
    "9. Given a XML file with Name as a Column like: divide the Name column into two columns with Name    First_Name and Last_Name and store the result in another table?\n",
    "\n",
    "                        |        Name             |\n",
    "                    _________________________\n",
    "                   |    Bhavesh Patidar       |\n",
    "                   |    Ashutosh Singh        |\n",
    "                   |    Naman Sharma         |\n",
    "                   |    Neeraj Jaiswal          |\n",
    "\n",
    "\n",
    "========================= Bhavesh - Legato ===============================\n",
    "\n",
    "1.\tHow Data can be transferred from one cluster to another cluster?\n",
    "2.\tIf we perform partitioning on particular column values and we get a NULL value for that column, will it throw any Error?\n",
    "3.\tRepartitioning VS Coalesce?\n",
    "4.\tWhat does the MSCK repair command do?\n",
    "5.\tCan we change the location of the Managed Table?\n",
    "6.\tWhy is the Replication factor in Hadoop is 3 only?\n",
    "7.\tWhat is cost Based Optimization in Spark?\n",
    "8.\tIf we perform join on two DF, what will be default RDD Partition Internally?\n",
    "9.\tLinux Command create three Directories A,B,C inside a temp Directory?\n",
    "\n",
    "======================Bhavesh - Numeric Technologies ======================\n",
    "\n",
    "1.\tHow do you read data from s3 buckets and create Hive tables, write commands?\n",
    "\n",
    "create external table table_tmp (.....)\n",
    "row format delimited fields terminated by '',\"\n",
    "location 's3://...//.....//'\n",
    "\n",
    "2.\tWhat is the default table in Hive?\n",
    "Managed table\n",
    "\n",
    "3.\tCan we change the location of the managed table, write commands to change the location of the managed table, and where data is stored in the case of the managed table?\n",
    "\n",
    "Yes, you can do it by using the clause – LOCATION ‘<hdfs_path>’ we can change the default location of a managed table.\n",
    "\n",
    "4.\tWhat are the configuration files in Hadoop?\n",
    "Configuration Files are the files which are located in the extracted tar.gz file in the etc/hadoop/ directory.\n",
    "All Configuration Files in Hadoop are listed below,\n",
    "\n",
    "1) HADOOP-ENV.sh->> It specifies the environment variables that affect the JDK used by Hadoop Daemon (bin/hadoop). We know that the Hadoop framework is written in Java and uses JRE so one of the environment variables in Hadoop Daemons is $Java_Home in Hadoop-env.sh.\n",
    "\n",
    "2) CORE-SITE.XML->> It is one of the important configuration files which is required for runtime environment settings of a Hadoop cluster.It informs Hadoop daemons where the NAMENODE runs in the cluster. It also informs the Name Node as to which IP and ports it should bind.\n",
    "\n",
    "3) HDFS-SITE.XML->> It is one of the important configuration files which is required for runtime environment settings of a Hadoop. It contains the configuration settings for NAMENODE, DATANODE, SECONDARYNODE. It is used to specify default block replication. The actual number of replications can also be specified when the file is created,\n",
    "\n",
    "4) MAPRED-SITE.XML->> It is one of the important configuration files which is required for runtime environment settings of a Hadoop. It contains the configuration settings for MapReduce . In this file, we specify a framework name for MapReduce, by setting the MapReduce.framework.name.\n",
    "\n",
    "5) Masters->> It is used to determine the master Nodes in a Hadoop cluster. It will inform about the location of SECONDARY NAMENODE to Hadoop Daemon.\n",
    "The Mater File on Slave node is blank.\n",
    "\n",
    "6) Slave->> It is used to determine the slave Nodes in Hadoop cluster.\n",
    "The Slave file at Master Node contains a list of hosts, one per line.\n",
    "The Slave file at Slave server contains IP address of Slave nodes.\n",
    "\n",
    "5.\tHow will you install Hive on top of Hadoop?\n",
    "Pata hai but Nahi bataunga , Admin nahi banana hume\n",
    "\n",
    "6.\tWhat is dynamic partitioning, what is the property to set dynamic partitioning true?\n",
    "\n",
    "1.\tcreate a non-partitioned table t2 and insert data into it.\n",
    "2.\tnow create a table t1 partitioned on the intended column(say country).\n",
    "3.\tload data in t1 from t2 as below:\n",
    "hive> INSERT INTO TABLE t2 PARTITION(country) SELECT * from T1;\n",
    "\n",
    "In static partitioning we need to specify the partition column value in each and every LOAD statement.\n",
    "suppose we are having partition on column country for table t1(userid, name,occupation, country), so each time we need to provide country value\n",
    "hive>\n",
    "LOAD DATA INPATH '/hdfs path of the file' INTO TABLE t1 PARTITION(country=\"US\") \n",
    "hive>\n",
    "LOAD DATA INPATH '/hdfs path of the file' INTO TABLE t1 PARTITION(country=\"UK\")\n",
    "\n",
    "7.\tWhat is the default port No. for Hadoop?\n",
    "\n",
    "8.\tDifference b/w order by and sort by?\n",
    "Hive supports SORT BY which sorts the data per reducer. The difference between \"order by\" and \"sort by\" is that the former guarantees total order in the output while the later only guarantees ordering of the rows within a reducer. If there are more than one reducer, \"sort by\" may give partially ordered final results.\n",
    "Note: It may be confusing as to the difference between SORT BY alone of a single column and CLUSTER BY. The difference is that CLUSTER BY partitions by the field and SORT BY if there are multiple reducers partitions randomly in order to distribute data (and load) uniformly across the reducers.\n",
    "Basically, the data in each reducer will be sorted according to the order that the user specified. The following example shows\n",
    "SELECT key, value FROM src SORT BY key ASC, value DESC\n",
    "\n",
    "9.\tGiven a table with column “Field1 '', write sql query to Create usernames from given mail id?\n",
    "Ex:    \n",
    "                |     Field1      |                                                  output\n",
    "               |  —-------------  |\n",
    "                   Swapnil.abc@yahooo.in                           Swapnil.abc\n",
    "                  Bhavesh.patidar@google.com                 Bhavesh.patidar\n",
    "\n",
    "(mysql)\n",
    "select substring(t.field1,1,position(\"@\" in field1)-1) from (select \"antrikshkethwas@gmail.com\" as field1) t;\n",
    "\n",
    "\n",
    "10.\tWhat is Vectorization in Hive?\n",
    "Vectorization allows Hive to process a batch of rows together instead of processing one row at a time. Each batch is usually an array of primitive types. Operations are performed on the entire column vector, which improves the instruction pipelines and cache usage. HIVE-4160 has the design document for vectorization and tracks the implementation of many subtasks.\n",
    "\n",
    "Enable Vectorization in Hive\n",
    "\n",
    "To enable vectorization, set this configuration parameter:\n",
    "\n",
    "hive.vectorized.execution.enabled=true \n",
    "When vectorization is enabled, Hive examines the query and the data to determine whether vectorization can be supported. If it cannot be supported, Hive will execute the query with vectorization turned off.\n",
    "\n",
    "11.\tExplain Traits in Scala?\n",
    "Like abstract classes, they can not be instantiated and they can be either implemented or unimplemented. Traits are created so that they could be inherited by other classes or even other traits. Main property of a trait is that multiple traits can be inherited in a single class. Traits enable multiple inheritance in Scala. Pseudo code Example of multiple inheritance using trait is below:\n",
    "\n",
    "Trait A{\n",
    "\tVar a\n",
    "\tDef fun\n",
    "}\n",
    "Trait B\n",
    "{\n",
    "\tVar a = 30\n",
    "\tDef fun2:String{ “how are you” }\n",
    "}\n",
    "\n",
    "Class newcl extends A, B{}\n",
    "\n",
    "Newcl class above can inherit from multiple traits which would have been not possible if trait A and trait B were classes instead of traits\n",
    "\n",
    "12.\tCase Class in Scala?\n",
    "Case class A( name:String, id:int)\n",
    "Case class B(name:String, id:int)\n",
    "println(A.name)\n",
    "\n",
    "\t//A==B is possible\n",
    "\n",
    "\n",
    "13.\tHow will you measure the performance of Spark Job?\n",
    "\n",
    "Using spark web UI\n",
    "Checking task execution time of each stage. The difference between longest execution task and shortest execution should be minimum for a spark job to be performant.\n",
    "\n",
    "\n",
    "======================= Bhavesh - Coforge ============================\n",
    "\n",
    "1.\tProject Overview?\n",
    "2.\tRelational vs NoSql Database?\n",
    "\n",
    "Relational Database\tNoSQL\n",
    "It is used to handle data coming in low velocity.\tIt is used to handle data coming in at high velocity.\n",
    "It gives only read scalability.\tIt gives both read and write scalability.\n",
    "It manages structured data.\tIt manages all types of data.\n",
    "Data arrives from one or few locations.\tData arrives from many locations.\n",
    "It supports complex transactions.\tIt supports simple transactions.\n",
    "It has a single point of failure.\tNo single point of failure.\n",
    "It handles data in less volume.\tIt handles data in high volume.\n",
    "Transactions written in one location.\tTransactions written in many locations.\n",
    "Deployed in vertical fashion.\tDeployed in Horizontal fashion.\n",
    "\n",
    "3.\tWhat are different serde techniques?\n",
    "●\tSerDe is a Library which is built-in to the Hadoop API\n",
    "●\tHive uses Files systems like HDFS or any other storage (FTP) to store data, data here is in the form of tables (which has rows and columns).\n",
    "●\tSerDe - Serializer, Deserializer instructs hive on how to process a record (Row). Hive enables semi-structured (XML, Email, etc) or unstructured records (Audio, Video, etc) to be processed also. For Example If you have 1000 GB worth of RSS Feeds (RSS XMLs). You can ingest those to a location in HDFS. You would need to write a custom SerDe based on your XML structure so that Hive knows how to load XML files to Hive tables or other way around.\n",
    "\n",
    "4.\tCache vs Persist?\n",
    "5.\tWhat are the different lateral views?\n",
    "-----------------+---------------------+--+\n",
    "| employee.ename  | employee.dept_list  |\n",
    "+-----------------+---------------------+--+\n",
    "| Tom             | [20]               \t |\n",
    "| Jerry           | [10,20]      \t |\n",
    "| Riley           | [20,30,40]              |\n",
    "SELECT ename, dept_id FROM employee LATERAL VIEW explode(dept_list) depts AS dept_id;\n",
    "\n",
    "6.\tTraits in Scala?\n",
    "In scala, trait is a collection of abstract and non-abstract methods. You can create traits that can have all abstract methods or some abstract and some non-abstract methods. A variable that is declared either by using val or var keyword in a trait gets internally implemented in the class that implements the trait.\n",
    " \n",
    "======================= Antriksh - Deloitte ==============================\n",
    "\n",
    "1st part: explain your projects\n",
    "2nd part:questions on project:\t\n",
    "1.\tscheduling tools being used?\n",
    "a.\tWhat operators you used in airflow\n",
    "2.\tETL pipeline in your project?\n",
    "\t3rd part: Spark and sql questions\n",
    "\t\t\n",
    "Input table\n",
    "cid\tcname\tctype\n",
    "1\tabc\tstd\n",
    "2\tdef\tprem\n",
    "3\thij\tall\n",
    "\n",
    "Output table\n",
    "cid\tcname\tctype\n",
    "1\tabc\tstd\n",
    "2\tdef\tprem\n",
    "3\thij\tstd\n",
    "3\thij\tprem\n",
    "\n",
    "Write sql query to get above output from given input table.\n",
    "Write Spark code to get above output using given input dataframe\n",
    "\n",
    "\n",
    "Sql: (temporary solution)\n",
    "select d2.id,d2.name,d1.ctype\n",
    "\tfrom delloite d1\n",
    "    join delloite d2\n",
    "    on d1.id!=d2.id where d1.ctype!=\"all\";\n",
    "\n",
    "\n",
    "\n",
    "Spark:\n",
    "val data = List((1, \"abc\", \"std\"),\n",
    "        (2, \"def\", \"prem\"),\n",
    "        (3, \"hij\", \"all\"),\n",
    "        (4, \"klm\", \"std\"),\n",
    "        (5, \"opq\", \"all\")\n",
    "       )\n",
    "val rdd = sc.parallelize(data)\n",
    "\n",
    "//devendra\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val rdd2 = rdd.map(x=>if(x._3==\"all\"){ (x._1,x._2,(\"std prem\")) } else x)\n",
    "val df3 = rdd2.toDF(\"cid\",\"cname\",\"ctype\")\n",
    "val df4 = df3.withColumn(\"ctype2\", explode(split($\"ctype\", \" \"))).drop(\"ctype\")\n",
    "\n",
    "df4.show()\n",
    "\n",
    "================= dev coforge =====================\n",
    "1- hive meta store \n",
    "2- external/managed \n",
    "\n",
    "Question - 1 \n",
    "test1_a table : ID-> 10,20,30,40\n",
    "test1_b table : ID -> 10,30,50\n",
    "write a code to fetch values in test1_a that are not in test1_b without using NOT or MINUS keyword/operator?\n",
    "desired output : (20,40)\n",
    "\n",
    "val l1 = List(10,20,30,40)\n",
    "val l2 = List(10,30,50)\n",
    " \n",
    "val df1 = sc.parallelize(l1).toDF(\"id\")\n",
    "val df2 = sc.parallelize(l2).toDF(\"id2\")\n",
    "val df3 = df1.join(df2,df1(\"id\")===df2(\"id2\"),\"left\")\n",
    "val df4 = df3.filter(df3(\"id2\").isNull).drop(\"id2\")\n",
    "df3.show()\n",
    "df4.show()\n",
    "\n",
    "Question -2 \n",
    "The table Employee has 3 columns(EmpID, OfficeMobile, HomeMobile). \n",
    "Some Employees have given the same number for both places. write a code where output\n",
    "should be 2 columns only EmpID and Contact No. \n",
    "If number is same, put in 1 row else in 2 rows.\n",
    " \n",
    "Sample :\n",
    "EmpID, OfficeMobile,HomeMobile\n",
    "1,   \t 123,   \t 123\n",
    "2,   \t 456,   \t 789\n",
    "desired output :\n",
    "1,123\n",
    "2,456\n",
    "3,789\n",
    " \n",
    " \n",
    "import org.apache.spark.sql.functions._\n",
    "val tb = List((1,123,123),(2,456,789))\n",
    "val df = sc.parallelize(tb).toDF(\"EmpID\",\"OfficeMobile\",\"HomeMobile\")\n",
    " \n",
    "val df2 = df.withColumn(\"phone\",when(col(\"OfficeMobile\")===col(\"HomeMobile\"), col(\"OfficeMobile\"))\n",
    "                             .otherwise(concat(col(\"OfficeMobile\"),lit(\" \"),col(\"HomeMobile\")))\n",
    "                       ).drop(\"OfficeMobile\",\"HomeMobile\")\n",
    "val df3 = df2.withColumn(\"contact\", explode(   split(col(\"phone\"), \" \")   )   ).drop(\"phone\")\n",
    " \n",
    "// val df2 = df.withColumn(\"new_gender\", when(col(\"gender\") === \"M\",\"Male\")\n",
    "//       .when(col(\"gender\") === \"F\",\"Female\")\n",
    "//       .otherwise(\"Unknown\"))\n",
    " \n",
    "df3.show()\n",
    "\n",
    "======================================================================\n",
    "\n",
    "Q- What happens in the background when we fire spark-submit considering a job submitted in yarn cluster mode, right from sending the request to cluster manager and returning the output to destination.\n",
    "Answer = > When Spark submit is invoked with deploy mode as cluster, it will choose a node randomly to launch the driver. Once the driver is launched , Driver will take the help of Resource Manager(YARN in this case) to decide where to launch the executors considering availability of resources on node.\n",
    "Once executors gets spawned up , driver will send all of the artifacts ( which are specified in the spark-submit , it will be main artifact i.e. Jar/Py file..\n",
    "\n",
    "Then Spark Engine follows the lineage (Logical Plan) which will be converted to DAG(Physical Plan) during creation of stages. There could be multiple jobs (Each action corresponds to a job in spark) in a spark application. Each Shuffle operation will create a new stage, and inside the stage we will have multiple tasks (which is the smallest entity) and spark keeps working on each task(equivalent to partition). This happens lazily when action gets invoked.\n",
    "Once each action inside a spark application gets completed , the resources (Driver/executor) gets released.\n",
    "\n",
    "=====================================================================\n",
    "\n",
    "What to do if the processing data size is larger than the Spark memory size/ driver memory size or the spark cluster memory size?\n",
    "\n",
    "And, How to process 1TB of data with 64GB of memory in Apache Spark? How does Spark work with it internally?\n",
    "\n",
    "In Apache Spark, if the data does not fit into the memory then Spark simply persists that data to disk.\n",
    "It depends on whether you need the full terabyte to be in memory at once or not. If your 1TB of data is actually 1 million 1MB records that can be processed independently, then no problem. All you need to do is load your data into an RDD and repartition it such that each partition is less than 64GB in size. This is because Spark will load each partition into memory during processing.\n",
    "\n",
    "Spark is an In-Memory processing tool, but obviously, it can’t put everything in memory if the data is huge. So, what will it do?\n",
    "Discard it? That wouldn’t be ideal. Nobody wants that. So, the only other option left is to store it back on disk.\n",
    "And, so that is exactly what it does. Put it in the disk until it's needed. When the data is needed, it swaps it in, and so on.\n",
    "But, remember that this process can be costly as we see with MapReduce. Mapreduce stores every result onto the disk and takes input from the disk. So, It is really slow. Even though Spark does fast computations in-memory, It won’t be really useful if you have to keep shuffling the data around. So, you just have to be careful in those cases.\n",
    "\n",
    "Does my data need to fit in memory to use Spark?\n",
    "No. Spark's operators spill data to disk if it does not fit in memory, allowing it to run well on any sized data. Likewise, cached datasets that do not fit in memory are either spilled to disk or recomputed on the fly when needed, as determined by the RDD's storage level.\n",
    "\n",
    "A spark driver is a machine that obviously cannot hold TerraBytes' worth of data. Let's say you have a file or multiple files of 10TB and you want Spark to perform some computation of this file. The driver machine will just create a Logical plan on how to perform computation on this file, it will come up with the best possible and optimized plan (with the help of tungsten and catalyst optimizer), the file is partitioned and each partition is read by executors that are in machines closer to where the blocks exist (as per data locality in HDFS).\n",
    "So spark can handle data size much bigger than the driver since the read operation doesn't happen at the driver. Certain actions like collect can overwhelm the driver since it collects all the executor results to the driver before writing down the result.\n",
    "\n",
    "\n",
    "========================Antriksh’s LnT 2nd round========================\n",
    "\n",
    "Q.size the small table when we broadcast? Both in spark and hive\n",
    "Hive:25mb\n",
    "Spark:10mb\n",
    "\n",
    "Q. how many partitions does spark create when you read a file?\n",
    "Ans.\n",
    "As many number of blocks were there when the file was stored in HDFS, initial stage of our spark job will have same number of partitions \n",
    "(no. of partitions in stage0 = no. of blocks used to store file in hdfs)\n",
    "\n",
    "Q. How is group by key different performance wise then reduced by key?\n",
    "\n",
    "Q.what happens if you have a file of bigger size than the memory can accommodate, what would you use cache or persist and why?\n",
    "\n",
    "Q. where would you use cache/persist in your code? Scenario when you used it?\n",
    "\n",
    "Q. When and why does shuffling of data occur from partition to partition?\n",
    "\n",
    "=============================Avinash Volkswegan 1st round=================\n",
    "1)\tIntroduction\n",
    "2)\twhat project you are done\n",
    "3)\twhat is your task in daily\n",
    "4)\thow sqoop do execution internally\n",
    "5)\twhat is boundary val query \n",
    "--split-by For free-form query imports, you need to specify 'split-by' . When you are importing the result of any particular query, sqoop needs to know the column-name using which it will create splits. Whereas, while importing tables, if not specified, it uses the primary key of the table being imported for creating splits. In case your primary key is uneven and not consistent, you can also specify any other column using split-by.\n",
    "--boundary-query During sqoop import process, it uses this query to calculate the boundary for creating splits: select min(), max() from table_name.\n",
    "In some cases this query is not the most optimal so you can specify any arbitrary query returning two numeric columns using --boundary-query argument. This saves min(split-by) and max(split-by) operations and thereby is more efficient.\n",
    "\n",
    "6)\twhat if no primary key in table then how it will execute\n",
    "7)\tideally how many mapper should be there in sqoop while fetch join tables data\n",
    "8)\thow would you deal delta load or data change in daily\n",
    "9)\thow can give secure password in scoop\n",
    "10)\twhat are sorting algorithms\n",
    "11)\twhat is spark (exact meaning of general purpose , in memory , compute engine)\n",
    "12)\thow spark it better than map reduce\n",
    "13)\twhat is repartition and coalesce\n",
    "14)\twhat is catalyst optimizer and what optimization it do internally\n",
    "Catalyst optimizer\n",
    "Optimizer (aka Catalyst Optimizer) is the base of logical query plan optimizers that defines the rule batches of logical optimizations (i.e. logical optimizations that are the rules that transform the query plan of a structured query to produce the optimized logical plan).\n",
    "\n",
    "It consists of rules like\n",
    "=>filter pushdown\n",
    "=>combining of filters\n",
    "=>combining of projections\n",
    "=>we can also add our own rules.\n",
    "\n",
    "15)\twhat is stages \n",
    "16)\twhy still we required low level rdd while having df and ds\n",
    "17)\twhat is feature of spark 3 \n",
    "●\t2x performance improvement on TPC-DS over Spark 2.4, enabled by adaptive query execution, dynamic partition pruning and other optimizations\n",
    "●\tANSI SQL compliance\n",
    "●\tSignificant improvements in pandas APIs, including Python type hints and additional pandas UDFs\n",
    "○\tBetter Python error handling, simplifying PySpark exceptions\n",
    "○\tNew UI for structured streaming\n",
    "○\tUp to 40x speedups for calling R user-defined functions\n",
    "○\tOver 3,400 Jira tickets resolved\n",
    "○\t \n",
    "\n",
    "18)\twhat is feature of spark 2\n",
    "\n",
    "19)\tdiff bw spark 2 and spark3 \n",
    "Spark 3 has \n",
    "AQE(re-optimizing at each stage until last stage)\n",
    " spark3 has feature that it can dynamically coalesce shuffle partitions\n",
    "It can dynamically switch join strategies, choosing the most optimized one\n",
    "Dynamically optimizing skew joins\n",
    "\n",
    "20)\thow to search a word in the file , write a program\n",
    "21)\thow to fetch data from sqoop with out server down time\n",
    "22)\thow to fetch data from s3 with the sqoop import\n",
    "23)\tComplexity for binary search algorithm\n",
    "\n",
    "\n",
    "Result : Not selected : \n",
    "\n",
    "I  got very good comments from interviewer but consultancy person told your name is not shortlisted\n",
    "\n",
    "=============================Avinash PWC 1st round=================\n",
    "1.\tq: write sqoop import command\n",
    "sqoop import \\\n",
    "--connect jdbc:mysql://localhost/userdb \\\n",
    "--username root \\\n",
    "--table emp_add \\\n",
    "--m 1 \\\n",
    "--target-dir /queryresult\n",
    "\n",
    "2.\tq:how to handle delta load in sqoop\n",
    "(1) Incremental Append\n",
    "(2) Incremental Last Modified\n",
    "3.\tq:what is kind of file format(please tell me each in detail)\n",
    "4.\tq:what is partition and bucketing and when do we use what\n",
    "5.\tq:how much data you are reading into the spark\n",
    "6.\tq:what kind of complex data type we have in hive\n",
    "7.\tq: write a word count program (how can you make this more dynamic)\n",
    "8.\tq: how to you submit spark program , can you tell me end to end flow\n",
    "9.\tq: Can you tell me how to submit your jar file and once it is submitted what we do exactly ?\n",
    "10.\tq: what is a cluster machine and client machine ?\n",
    "11.\tq: can you write the data frame structure from parquet file format ?\n",
    "12.\tq: What is the lineage graph in spark ?\n",
    "13.\tq: what is difference between persistent and cache()\n",
    "14.\tq: Can you explain persistent storage levels ?\n",
    "15.\tQ: what is default partition in spark\n",
    "\n",
    "\n",
    "ABC     XYZ\n",
    "id\tnum\n",
    "1\t1\n",
    "2\t1\n",
    "3\t1\n",
    "4\t2\n",
    "5\t1\n",
    "6\t2\n",
    "7\t2\n",
    "\n",
    "consecutiveNums\n",
    "1\n",
    "Q: exactly needs to check for numbers appearing at least three times consecutively\n",
    "Q:tbl\n",
    "               \n",
    "SELECT DISTINCT\n",
    "       num\n",
    "       FROM (SELECT num,\n",
    "                    lag(num, 1) OVER (ORDER BY id) = num\n",
    "                    AND lag(num, 2) OVER (ORDER BY id) = num c3\n",
    "                    FROM logs) x\n",
    "       WHERE c3;\n",
    "\n",
    "\n",
    "name\tdept\tsalary\n",
    "james\tsales\t3000\n",
    "michael\tsales\t4600\n",
    "robert\tsales\t4100\n",
    "maria\tfinance\t3000\n",
    "james\tsales\t3000\n",
    "scott\tfinance\t3300\n",
    "jen\tfinance\t3900\n",
    "jeff\tmarketing\t3000\n",
    "kumar\tmarketing\t2000\n",
    "saif\tsales\t4100\n",
    "\n",
    "find the name of employees earning more than the average salary of their department ?\n",
    "Tell me about the last 5 records here  ?\n",
    " \n",
    "  select e.first_name, e.salary, e.department_id\n",
    "    from employees as e\n",
    "    where e.salary > (select avg(e2.salary)\n",
    "                  from employees e2\n",
    "                  where e2.department_id = e.department_id\n",
    "                 );\n",
    "\n",
    "Result : Pending\n",
    "\n",
    "I explained each theory concept well but got stuck in queries and interview time duration exact 1 hour with Aditi Singh\n",
    "\n",
    "\n",
    "=============================Avinash TAVANT 1st round=================\n",
    "1.\tq: how to migrate data from hdfs to s3\n",
    "2.\tq: what is surrogate key\n",
    "3.\tq: what is kafka streaming\n",
    "4.\tq: what is star schema & snowflake schema and which scenario use what\n",
    "5.\tq: what is primary key & foregin key\n",
    "6.\tq: what is diff between unique key and primary key\n",
    "7.\tq: what is emr & ec2\n",
    "8.\tq: how to load data in parques file\n",
    "9.\tq: what is azure ?\n",
    "10.\tq:why not Gcp your company is using ?\n",
    "11.\tq:tell me how google search internally works?\n",
    "12.\tq: what is AWS Glue and how do you use it in Etl activity?\n",
    "13.\tq:what is athena and how amazon takes charge from customers on running queries?\n",
    "\n",
    "Resule : Not Selected with Tanmay\n",
    "\n",
    "Tavnat is looking for a Kafka & cloud expert candidate.\n",
    "\n",
    "=============================Avinash Genpact 1st round=================\n",
    "\n",
    "Q: How to fetch data with sqoop\n",
    "Q: what is normalize and denormalize\n",
    "Q: how did you break a table into normalize form\n",
    "Q: what is LIT in data frame\n",
    "Q: what is partitioning\n",
    "Q: HOW would you load data manually in partition table\n",
    "Q: what is data frame\n",
    "Q: how did u join two table in data frame\n",
    "Q: what is bucketing\n",
    "Q: how your project architecture\n",
    "Q: what is athena and aws glue\n",
    "Q: how to normalize a table (can you do it right now )\n",
    "Q: how good are you in sql ?\n",
    "Q: tell me how many joins are there and diff between full outer and union \n",
    "Q: What exploded ?\n",
    "Q: Why do you use salting ?\n",
    "Q: Which file format did you use ?\n",
    "Q: Can you write your code for word count ?\n",
    "\n",
    "Result : Selected 1st round \n",
    "\n",
    "There were two panel members and one was observing you full time and another was asking only questions\n",
    "\n",
    "\n",
    "==========================Antriksh’s Cognizant=============================\n",
    "\n",
    "\n",
    "Q) When you submit your spark application, what cluster size do you give and why?\n",
    "\n",
    "Q) hadoop cluster vs spark execution cluster\n",
    "\t\n",
    "Q) generic questions like repartition coalesce, cache persist……..\n",
    "\n",
    "\n",
    "=========================Antriksh Impetus second round-=======================\n",
    "\n",
    "Q you have a variable string \"aaaaabbccaa\"\n",
    "o/p should be 5a 2b 2c 2a\n",
    "Write the code in any programming language\n",
    "\n",
    "val str = \"aaaaabbccaa\"\n",
    "\n",
    "var cnt = 0\n",
    "var elem = str(0)\n",
    "var ans = \"\"\n",
    "\n",
    "for(i <- str)\n",
    "{\n",
    "    if(i==elem)\n",
    "    {\n",
    "        cnt=cnt+1\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        ans=ans+elem+cnt\n",
    "        elem=i\n",
    "        cnt=1\n",
    "    }   \n",
    "}\n",
    "ans=ans+elem+cnt\n",
    "println(ans)\n",
    "\n",
    "Q) you have a file with each line having 40 characters, you need to write a code to create a new df with 4 columns. We do not have any delimiters in data. Each column has 5,10, 10,15 characters respectively.\n",
    "\n",
    "Ans.\n",
    "\n",
    "40 characters\n",
    "\n",
    "1)read\n",
    "\n",
    "val rdd = spark.sparkContext(\"path\")\n",
    "\n",
    "rdd.map(x=>\t{\n",
    "\t(substr(x._1,0,5)),(substr(x._1,5,15)),(substr(x._1,15,25)),(substr(x._1,25,40))\t)\n",
    "}\n",
    ")\n",
    "\t\n",
    "\n",
    "Q)\n",
    "\n",
    "table\n",
    "id,name,\tadd1,\tadd2\n",
    "1\tant\t\t ind\t ujj\n",
    "2\tsunil\tnull\tnoida\n",
    "3\tanil\tdelhi\tnull\n",
    "\n",
    "trgt tbl\n",
    "id,\tname,\taddress\n",
    "1\tant\t\t ind\n",
    "1\tant\t\t ujj\n",
    "2\tsunil\tnoida\n",
    "3\tanil\tdelhi\n",
    "\n",
    "\n",
    "var df = spark.read....(input table)\n",
    "\n",
    "//df.withColumn(\"address\",when(df(col(\"add1\").isNull and \n",
    "\n",
    "var df2 = df\n",
    "df.join(df2,df(id)===df2(id), \"inner\")\n",
    "\n",
    "id,name,\tadd1,\tadd2\n",
    "1\tant\t\t ind\t ujj\n",
    "1\tant\t\t ind\t ujj\n",
    "2\tsunil\tnull\tnoida\n",
    "2\tsunil\tnull\tnoida\n",
    "3\tanil\tdelhi\tnull\n",
    "3\tanil\tdelhi\tnull\n",
    "\n",
    "df.withColumn(\"address\",\n",
    "\n",
    "\n",
    "========================= DEV PAYTM TEST ==================================\n",
    "select  v.state, group_concat(v.votes) from \n",
    "\t( select t.state, concat(t.name, \" x \" , count(t.candidate_id) ) as votes , count(t.candidate_id) as cnt from \n",
    "    (\n",
    "    select concat(c.first_name,\" \" ,c.last_name) as name,r.state,r.candidate_id from candidates c \n",
    "\tjoin results r on c.id = r. candidate_id \n",
    "    ) t\n",
    "     group by t.state,t.name  order by count(t.candidate_id) desc \n",
    "     ) v group by v.state; \n",
    "\t \n",
    "\t \n",
    "candidate \n",
    "\n",
    "id  \tfirst_name\t\t last_name\n",
    "1\tavi \t\t\tporval\n",
    "2 \tdev \t\t\tkumar \n",
    "\n",
    "\n",
    "result \n",
    "\n",
    "cand_id   \tstate \n",
    "\n",
    "1\t\t\tUP\n",
    "2\t\t\tMP\t\n",
    "1\t\t\tUK\n",
    "1\t\t\tBH\n",
    "2\t\t\tPJ\n",
    "3\t\t\t\n",
    "4\n",
    "5\n",
    "1\n",
    "6\n",
    "\n",
    "\n",
    "output - \n",
    "\n",
    "state \t result \n",
    "\n",
    "UP \t\tavi porwal X 3, dev kumar X 2 \n",
    "MP      \t\tant kethwas X 10, avi porwal X 3 \n",
    "--------------------------------------------------------------------------\n",
    "\n",
    "input1 = 5\n",
    "input2 = 6 \n",
    "numbers = 7\n",
    "\n",
    "\n",
    "[5,6,55,56,66,555,556,566,666]\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "===========================Antriksh’s Reliance jio 1st round===========================\n",
    "\n",
    "\n",
    "\n",
    "Q) Write a program to write all those records in a file that have at least one column value as null and the rest of the records in other file or table\n",
    "\n",
    "schema\n",
    "aadhar, cname, cstate, cage, network_performance, timestamp\n",
    "\n",
    "my approach: \n",
    "\n",
    "//df = spark.read(path).option(\"mode\",\"dropmalformed\") interviewer doesn't want it like this\n",
    "\n",
    "df_all = spark.read()...\n",
    "\n",
    "var df_missing = df_all.select(when(col(\"aadhar\")==null),or(col(\"cname\")=null)\n",
    "\n",
    "var df_everythingelse = df_all.filter(df_all(col(\"aadhar\") == null or df_all(\n",
    "\n",
    "df_everythingelse.write.parquet(\"path\")\n",
    "\n",
    "\n",
    "Q)on the given schema, what columns would you use for bucketing and partitioning. explain with reason\n",
    "\n",
    "schema\n",
    "aadhar, cname, cstate, cage, network_performance, timestamp\n",
    "\n",
    "my ans:\n",
    "cstate and cage as partitioning columns\n",
    "timestamp or aadhar as bucketing column\n",
    "\n",
    "due to cardinality of cstate and cage is low, we can use them as partitioning columns\n",
    "and since cadrinality of timestamp//aadhar is low, we can use anyone as bucketing column\n",
    "\n",
    "\n",
    "Q)\n",
    "\n",
    "WITH T(StyleID, ID)\n",
    "     AS (SELECT 1,1 UNION ALL SELECT 1,1 UNION ALL SELECT 1,1 UNION ALL SELECT 1,2) T\n",
    "SELECT *,\n",
    "       RANK() OVER(PARTITION BY StyleID ORDER BY ID)       AS 'RANK',\n",
    "       ROW_NUMBER() OVER(PARTITION BY StyleID ORDER BY ID) AS 'ROW_NUMBER',\n",
    "       DENSE_RANK() OVER(PARTITION BY StyleID ORDER BY ID) AS 'DENSE_RANK'FROM   T;\n",
    "\n",
    "stleid, id \n",
    "1, 1\n",
    "1, 1\n",
    "1, 1\n",
    "1, 2\n",
    "\n",
    "Ans:\n",
    "\n",
    "stleid, id, rank, row_num, dense_rank\n",
    "1,\t 1, \t1,\t 1,\t\t 1\n",
    "1,\t 1, \t1,\t 2,\t\t 1\n",
    "1,\t 1, \t1,\t 3, \t\t 1\n",
    "1,\t 2, \t4,\t 4, \t\t 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65dc107f-5480-4097-a988-f5ee059522de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Q there are 5 ids (key) , 10 partitions in data. How many files will be there?\n",
    "ans.\t50 files\n",
    " val ddf2 = ddf1.repartition(10)\n",
    "ddf2.write.format(\"csv\").option(\"path\", \"partitioned_data2\").partitionBy(\"sid\").save\n",
    "ee\n",
    "Q Now I want 5 files only (as many number of keys are there)\n",
    "ans. coalesce()\n",
    "val ddf2 = ddf1.coalesce(1)\n",
    "ddf2.write.format(\"csv\").option(\"path\", \"partitioned_data3\").partitionBy(\"sid\").save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1454ccdc-813a-49ed-b57a-3dc703305ebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [1,2,3,4,5]\n",
    "schema =[\"id\"]\n",
    "\n",
    "df = spark.createDataFrame(data,schema)\n",
    "df.show()\n",
    "\n",
    "ddf2 = df.repartition(10)\n",
    "ddf2.write.format(\"csv\").partitionBy(\"id\").option(\"path\", \"/Volumes/bigdata/assign/assign_file\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0cd69d4-cd8e-43d1-af2f-13c791201120",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "schema = [\"id\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "display(df)\n",
    "\n",
    "ddf2 = df.repartition(10)\n",
    "ddf2.write.partitionBy(\"id\").format(\"delta\").mode(\"overwrite\").option(\n",
    "    \"path\", \"/Volumes/bigdata/assign/assign_file\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca778951-892f-4b90-b6d1-4b4ed32f79e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "schema = [\"id\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "display(df)\n",
    "\n",
    "# Write to a managed Delta table in Unity Catalog\n",
    "df.write.partitionBy('id').mode(\"overwrite\").save(\"/Volumes/bigdata/assign/assign_file/part_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dbc267f-b8ec-489f-9f0d-f501c1d3ed1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY delta.`/Volumes/bigdata/assign/assign_file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9661eb9-ffa3-4325-8da4-4e2478b2066f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RepartitionWrite\").getOrCreate()\n",
    "\n",
    "# Sample data with 5 keys\n",
    "data = [\n",
    "    (\"C001\", \"UP\"), (\"C002\", \"MP\"), (\"C003\", \"MH\"),\n",
    "    (\"C004\", \"UP\"), (\"C005\", \"MP\"), (\"C006\", \"MH\"),\n",
    "    (\"C007\", \"UP\"), (\"C008\", \"MP\"), (\"C009\", \"MH\"),\n",
    "    (\"C010\", \"UP\")\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"customer_id\", \"sid\"])\n",
    "\n",
    "# Repartition to 10\n",
    "df_repartitioned = df.repartition(10)\n",
    "\n",
    "# Write with partitionBy → creates 5 folders × 10 files = ~50 files\n",
    "df_repartitioned.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"sid\") \\\n",
    "    .save(\"/Volumes/bigdata/assign/assign_file/part_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92d87601-2caf-48ba-9396-7dde30e75cca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8917545070241352,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "INTERVIEW_QTNS",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
