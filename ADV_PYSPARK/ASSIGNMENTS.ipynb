{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94e46039-f98f-4989-b6ea-cce50130c3e4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758552682386}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1 Sales Total Revenue, Avg Units Sold Revenue ≥ 1000 Date, Product, Units_Sold, Revenue\n",
    "\n",
    "\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "df_sale = spark.read.format(\"parquet\").load(\"/Volumes/bigdata/bdata/sales/sales.parquet\")\n",
    "df_sale.display()\n",
    "\n",
    "#total sale\n",
    "\n",
    "df_sale_sum = df_sale.agg(sum(\"Revenue\").alias(\"Total_Revenue\"))\n",
    "df_sale_sum.display()\n",
    "\n",
    "#avg units_sold\n",
    "df_sale_sum = df_sale.agg(sum(\"Revenue\").alias(\"Total_Revenue\"), avg(\"Units_Sold\").alias(\"Avg_Units_Sold\"))\n",
    "df_sale_sum.display()\n",
    "\n",
    "#filter revenue >= 1000\n",
    "\n",
    "df_sale_filtered = df_sale.filter(col(\"Revenue\") >= 1000).select(\"Date\", \"Product\", \"Units_Sold\", \"Revenue\")\n",
    "df_sale_filtered.display()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2969a4e3-30cb-40ee-b98f-e2e88b252dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2 Web Analytics Avg Page Views per Visitor Visitors ≥ 5000 Date, Visitors, PageViews, Country\n",
    "\n",
    "\n",
    "df_web = spark.read.format(\"parquet\").load(\"/Volumes/bigdata/bdata/web_analytics/web_analytics.parquet\")\n",
    "df_web.display()\n",
    "\n",
    "#Avg Page Views per Visitor\n",
    "\n",
    "df_web_avg = df_web.groupBy(\"Country\").agg(sum(\"Visitors\").alias(\"Visitors\"),avg(\"PageViews\").alias(\"Avg_PageViews\"))\n",
    "df_web_avg.display()\n",
    "\n",
    "#filter Visitors >= 5000\n",
    "\n",
    "df_web_filtered = df_web.filter(\"Visitors\") >= 5000).select(\"Date\", \"Visitors\", \"PageViews\", \"Country\")\n",
    "df_web_filtered.display()\n",
    "\n",
    "#join\n",
    "\n",
    "df_final = df_web_filtered.join(df_web_avg, df_web_filtered.Visitors == df_web_avg.Visitors)\n",
    "df_final.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f92c8bcb-7a67-4701-b6fb-3418a96a611d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3 Employee Performance Avg Performance Score, Productivity Hours_Worked ≥ 150 Emp_ID, Projects_Completed, Hours_Worked, Performance_Score\n",
    "\n",
    "\n",
    "df_emp = spark.read.format(\"parquet\").load(\"/Volumes/bigdata/bdata/employee/employee_performance.parquet\")\n",
    "df_emp.display()\n",
    "\n",
    "#Employee Performance Avg Performance Score\n",
    "\n",
    "df_emp_avg = df_emp.groupBy(\"Emp_ID\").agg(round(avg(\"Performance_Score\"),2).alias(\"Avg_Performance_Score\"))\n",
    "df_emp_avg.display()\n",
    "\n",
    "#filter Hours_Worked >= 150\n",
    "\n",
    "df_emp_filtered = df_emp.filter(col(\"Hours_Worked\") >= 150).select(\"Emp_ID\", \"Projects_Completed\", \"Hours_Worked\", \"Performance_Score\")\n",
    "df_emp_filtered.display()\n",
    "\n",
    "#join   \n",
    "df_emp_final = df_emp_filtered.join(df_emp_avg, df_emp_filtered.Emp_ID == df_emp_avg.Emp_ID)\n",
    "df_emp_final.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bc9e76d-9846-4159-905d-8ad9557262d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#4 Customer Feedback Avg Rating Rating ≥ 4 Feedback_ID, Product, Rating, Comment\n",
    "\n",
    "df_cust = spark.read.format(\"parquet\").load(\"/Volumes/bigdata/bdata/customer/customer_feedback.parquet\")\n",
    "df_cust.display()\n",
    "\n",
    "#Avg Rating \n",
    "df_filter = df_cust.filter(col(\"Rating\") >= 4).select(\"Feedback_ID\", \"Product\", \"Rating\", \"Comment\")\n",
    "df_filter.display()\n",
    "\n",
    "df_cust_avg = df_cust.agg(round(avg(\"Rating\"),2).alias(\"Avg_Rating\"))\n",
    "df_cust_avg.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c1c439f-a6ff-4179-b21e-7f41fca388a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#5 Project Tracking Task Completion Rate Status = \"On Track\" Project_ID, Task_Count, Completed_Tasks, Status\n",
    "\n",
    "df_proj_trk = spark.read.format(\"parquet\").load(\"/Volumes/bigdata/bdata/project_tracking/project_tracking.parquet\")\n",
    "df_proj_trk.display()\n",
    "\n",
    "#Task Completion Rate\n",
    "df_proj_trk_filt = df_proj_trk.filter(col(\"Status\") == \"On Track\").select(\"Project_ID\", \"Task_Count\", \"Completed_Tasks\", \"Status\")\n",
    "df_proj_trk_filt.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c682c6bd-9d10-4418-88e2-32852fd452d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#6 Finance Report Profit Margin (%) Profit ≥ 50_000 Month, Revenue, Expenses, Profit\n",
    "\n",
    "df_fin = spark.read.format(\"parquet\").load(\"/Volumes/bigdata/bdata/finance_report/finance_report.parquet\")\n",
    "df_fin.display()\n",
    "\n",
    "#filtered profit\n",
    "\n",
    "df_fin_fil = df_fin.filter(col(\"Profit\") >= 50000).select(\"Month\", \"Revenue\", \"Expenses\", \"Profit\")\n",
    "df_fin_fil.display()\n",
    "\n",
    "df_fin_margin = df_fin_fil.withColumn(\"profit_margin\", concat(round((col(\"Profit\") / col(\"Revenue\")) * 100,2),lit(\"%\")))\n",
    "\n",
    "df_fin_margin.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da4dd05c-ae45-458f-8b09-57567c935f61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e90c04a9-88ea-4171-9566-d9b40b8421bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "and also share these question for practice to every one Implement Word Count using RDD API.\n",
    " Implement Word Count using DataFrame API. Filter customers with age > 30. \n",
    " Remove duplicates based on customer_id.\n",
    " Replace null values with default in salary column. \n",
    " Drop rows with nulls in specific columns. \n",
    " Cast salary column from string to integer. \n",
    " Add a derived column total = salary + bonus.\n",
    " Split full_name into first_name and last_name. \n",
    " Concatenate two columns into one (city + state). \n",
    " Select distinct job titles from employee data. S\n",
    " ort DataFrame by multiple columns (salary desc, age asc). \n",
    " Sample 5% of rows from DataFrame randomly. Count number of rows in DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "792ed448-667b-48e4-9047-27c4a1acd4e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "1:-  You have to read this CSV file and create proper data frame. at least 2 different way. \n",
    "1:-  id,name|age;country \n",
    "1,a|12;usa id name age country 1 a   12 usa 2:-  id||name||age||country 1||a||12||usa\n",
    "\n",
    "You have to read this CSV file and create proper data frame.\n",
    "\n",
    "at least 2 different way.\n",
    "\n",
    "\n",
    "1:- \n",
    "id,name|age;country\n",
    "1,a|12;usa\n",
    "\n",
    "id name age country\n",
    "1 a   12 usa\n",
    "\n",
    "\n",
    "2:- \n",
    "id||name||age||country\n",
    "\n",
    "1||a||12||usa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a44b206-e7af-492c-a9d5-dfe8dcdb1de9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d4a04f-7e08-49bb-bee9-bb2b8c46f056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Multiple Delimiters\").getOrCreate()\n",
    "\n",
    "\n",
    "df_multi = spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).option(\"delimiter\",'|').load(\"/Volumes/bigdata/assign/assign_file/pipe_deli.csv\")\n",
    "\n",
    "df_multi.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14093c93-f75e-4d09-8409-5999139e9e1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "df_multi = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"inferSchema\", True)\\\n",
    "    .option(\"delimiter\", replace(\"||\", \"|\"))\\\n",
    "    .load(\"/Volumes/bigdata/assign/assign_file/pipe_deli.csv\")\n",
    "\n",
    "\n",
    "display(df_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2adb1fd9-021d-408e-84e1-fb2dd4770075",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "works"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "df_multi = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", True)\\\n",
    "    .option(\"inferSchema\", True)\\\n",
    "    .option(\"delimiter\",'||')\\\n",
    "    .load(\"/Volumes/bigdata/assign/assign_file/pipe_deli.csv\")\n",
    "\n",
    "\n",
    "display(df_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b9e37d6-9f1d-4249-9726-2d94b8a225ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1:- \n",
    "# id,name|age;country\n",
    "# 1,a|12;usa\n",
    "\n",
    "# id name age country\n",
    "# 1 a   12 usa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74055316-ea4b-44c9-a196-258a55cafb8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_diff_deli = spark.read.format(\"csv\").option(\"headwe\", True). option(\"inferSchema\", True).option(\"delimiter\", \";,|\").load(\"/Volumes/bigdata/assign/assign_file/multi_delimiter.csv\")\n",
    "df_diff_deli.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3653e9d-2e94-4e66-a17a-e49f80a3c2d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_diff_deli = spark.read.format(\"csv\").option(\"header\", True)\\\n",
    "    .load(\"/Volumes/bigdata/assign/assign_file/multi_delimiter.csv\")\n",
    "\n",
    "df_diff_deli.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a107055-0dd1-4342-afc6-3fda7c1af742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace as re\n",
    "\n",
    "df_diff_deli = spark.read.format(\"csv\")\\\n",
    "    .load(\"/Volumes/bigdata/assign/assign_file/multi_delimiter.csv\")\n",
    "\n",
    "\n",
    "df_del_new = df_diff_deli.withcolumn(\"_c1\", regexp_replace(\"_c1\", \";\",\",\"))\n",
    "\n",
    "df_diff_deli.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70a2b07c-9887-44f8-870e-7cf3eb14fc5a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "works"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace as re\n",
    "\n",
    "df_diff_deli = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .load(\"/Volumes/bigdata/assign/assign_file/multi_delimiter.csv\")\n",
    ")\n",
    "\n",
    "df_del_new = df_diff_deli.withColumn(\"_c1\", re(\"_c1\", \"[;|]\", \",\"))\n",
    "df_split = df_del_new.withColumn(\"_c1\", split(\"_c1\",\",\"))\\\n",
    "    .withColumn(\"name\", col(\"_c1\")[0])\\\n",
    "    .withColumn(\"age\", col(\"_c1\")[1])\\\n",
    "    .withColumn(\"country\", col(\"_c1\")[2])\\\n",
    "    .drop(\"_c1\")\n",
    "\n",
    "first_row = df_split.first()\n",
    "df_no_first = df_split.filter(\n",
    "    (col(\"name\") != first_row[\"name\"]) |\n",
    "    (col(\"age\") != first_row[\"age\"]) |\n",
    "    (col(\"country\") != first_row[\"country\"])\n",
    ")\n",
    "\n",
    "display(df_no_first)\n",
    "\n",
    "\n",
    "\n",
    "display(df_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a2f9e52-5834-40ea-a666-1bfea7c3a53f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "works"
    }
   },
   "outputs": [],
   "source": [
    "df_diff_deli = spark.read.format(\"csv\").option(\"header\", True)\\\n",
    "    .load(\"/Volumes/bigdata/assign/assign_file/multi_delimiter.csv\")\n",
    "\n",
    "df_del_new = df_diff_deli.withColumn(\"name|age;country\", re(\"name|age;country\", \"[;|]\", \",\"))\n",
    "\n",
    "df_split = df_del_new.withColumn(\"name|age;country\", split(\"name|age;country\",\",\"))\\\n",
    "    .withColumn(\"name\", col(\"name|age;country\")[0])\\\n",
    "    .withColumn(\"age\", col(\"name|age;country\")[1])\\\n",
    "    .withColumn(\"country\", col(\"name|age;country\")[2])\\\n",
    "    .drop(\"name|age;country\")\n",
    "\n",
    "df_diff_deli.display()\n",
    "\n",
    "df_split.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af789883-df25-458d-8d3a-207af7e5eb40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1e3dcb4-b65f-4d0f-a394-a07d2ae0bc63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_deli =  spark.read.format(\"csv\")\\\n",
    "    .load(\"/Volumes/bigdata/assign/assign_file/multi_delimiter.csv\")\n",
    "\n",
    "df_deli.select(\"_c0\", split(\"_c1\",\"[,!;]\").alias(\"parts\"))\n",
    "df_deli.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f901f2c1-5287-4cf8-8705-bd49045a7531",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_h=df_split.filter(~(col(\"parts\")[0] == \"Date\")&(col(\"parts\")[1] != \"product\")&(col(\"parts\")[2] != \"Units_Sold\")&(col(\"parts\")[3] != \"Revenue\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ASSIGNMENTS",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
