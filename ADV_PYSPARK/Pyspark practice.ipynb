{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "386903fa-7fea-4fbf-8199-28ac4b133e42",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760544307695}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "df_emp = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option('sep','|').load(\"/Volumes/workspace/default/emp/EMP_FF_SRC.txt\")\n",
    "\n",
    "df_emp.display()\n",
    "\n",
    "#Row_number\n",
    "\n",
    "win_spec = Window.orderBy(col(\"SALARY\").desc())\n",
    "emp_rank = df_emp.withColumn(\"Rank\", rank().over(win_spec))\n",
    "emp_res = emp_rank.filter(col(\"Rank\") == 1)\n",
    "emp_res.display()\n",
    "\n",
    "\n",
    "#Dense Rank\n",
    "\n",
    "win_spec = Window.orderBy(col(\"SALARY\").desc())\n",
    "emp_rank = df_emp.withColumn(\"Dense_Rank\", dense_rank().over(win_spec))\n",
    "emp_res = emp_rank.filter(\"Dense_Rank == 1\")\n",
    "emp_res.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a44e0fe1-672e-44a6-af4a-5634639302a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da9c06b5-3cc1-464f-bb9f-9e5fa3e65766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#collect all the data as List\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa117c0c-892e-450f-b557-fdc88c96da3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "second_highest_salary = (\n",
    "    df_emp\n",
    "    .select(\"SALARY\")\n",
    "    .distinct()\n",
    "    .orderBy(col(\"SALARY\").desc())\n",
    "    .collect()[1][0]\n",
    ")\n",
    "\n",
    "display(second_highest_salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eb6091e-f00d-442c-ac8b-ac09ad710969",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_emp.select(\"SALARY\").distinct().orderBy(col(\"SALARY\").desc()).collect()[1][[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5251dcfa-17cf-4528-840c-b3c03c6a565d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_emp.collect()\n",
    "\n",
    "df_emp.collect()[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9433e3b-7a79-4cd2-be88-5bba230a8c4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "a= [1,2,3]\n",
    "print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92ca0ff1-5631-46ab-8f45-4a80da412b69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp = df_emp.alias(\"emp\")\n",
    "manager = df_emp.alias(\"manager\")\n",
    "\n",
    "res_emp_sal = emp.join(manager, emp.MANAGER_ID == manager.EMPLOYEE_ID, \"inner\")\\\n",
    "    .filter(col(\"emp.SALARY\") > col(\"manager.SALARY\"))\\\n",
    "    .selectExpr(\"emp.EMPLOYEE_ID\", \"emp.FIRST_NAME\", \"emp.SALARY\", \"manager.FIRST_NAME as manager_FIRST_NAME\", \"manager.SALARY as manager_SALARY\")\\\n",
    "    .distinct()\\\n",
    "    .orderBy(col(\"manager.SALARY\")\\\n",
    "    .desc())\n",
    "\n",
    "display(res_emp_sal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c966c703-6123-4bdb-b671-cffb335bba63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_emp =df_emp.groupBy(\"DEPARTMENT_ID\").count().orderBy(col(\"count\").desc()).limit(1).display()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fb2a4f1-7775-4b35-9da7-c4aeedfe47dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_emp = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option('sep','|').load(\"/Volumes/workspace/default/emp/EMP_FF_SRC.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b43ef1b-d6cf-4324-af3f-9db998b429d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_emp.groupBy(\"FIRST_NAME\").agg(count(\"FIRST_NAME\").alias(\"Name_count\")).filter(col(\"Name_count\") == 2).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "685a177e-15d1-4719-ba0e-5eaf55f926db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_emp.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "198aecdb-ed80-4014-ab34-52514b9e4cfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(current_date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc8847a0-ff04-4611-bcdf-9f6963b5ae56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "current_date()\n",
    "df_emp.withColumn(\"DATE\", current_date()).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1795c6c-f438-4d6f-8eff-73f960caf6fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# date = time.current_date()\n",
    "# print(date)\n",
    "\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "current_date = date.today()\n",
    "print(current_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f1d7ec-5bb1-45ec-9bf2-b795a5ad3778",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "display(spark.range(1).select(current_date().alias(\"current_date\")))\n",
    "\n",
    "\n",
    "print(current_date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "069d3e96-2ae6-4d2c-8538-ed4a65b64259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6 Find employees whose salary is in the top 10%\n",
    "# 7.Display employees who have same department and same salary\n",
    "# 8.Find employees who don’t have a manager\n",
    "# 9.Show employee(s) who joined first in each department\n",
    "# 10.Find department with the highest average salary.\n",
    "# 11.Find employees who joined before their manager.\n",
    "# 12.Find all employees along with their manager names\n",
    "# 13.Find employees who have the same manager\n",
    "# 14.List employees working under a specific manager (e.g., “John”).\n",
    "# 15.Find manager(s) who manage more than 5 employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "195055c0-65c3-4fcf-a00e-32b8b72d28b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6 Find employees whose salary is in the top 10%\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "salary_threshold = df_emp.approxQuantile(\"SALARY\", [0.9], 0.01)[0]df_emp.filter(col(\"SALARY\") >= salary_threshold).display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17cbc61e-78e4-4c58-a17b-3935a841d2f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#7.Display employees who have same department and same salary\n",
    "\n",
    "\n",
    "df_emp_sal = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option('sep','|').load(\"/Volumes/workspace/default/emp/EMP_FF_SRC.txt\")\n",
    "\n",
    "emp_sal = df_emp_sal.alias(\"emp_sal\")\n",
    "emp_dept = df_emp_sal.alias(\"emp_dept\")\n",
    "\n",
    "df_emp_res =emp_sal.join(emp_dept,  ((emp_sal.DEPARTMENT_ID == emp_dept.DEPARTMENT_ID) & (emp_sal.SALARY == emp_dept.SALARY) & (emp_sal.EMPLOYEE_ID != emp_dept.EMPLOYEE_ID)), \"inner\")\\\n",
    "    .selectExpr(\"emp_sal.EMPLOYEE_ID\", \"emp_sal.FIRST_NAME\", \"emp_sal.SALARY\", \"emp_sal.DEPARTMENT_ID\",\"emp_dept.FIRST_NAME\", \"emp_dept.SALARY\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ad0943-b58a-4084-a745-6ce909e4adde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#7.Display employees who have same department and same salary\n",
    "\n",
    "\n",
    "df_emp_sal = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option('sep','|').load(\"/Volumes/workspace/default/emp/EMP_FF_SRC.txt\")\n",
    "\n",
    "emp_sal = df_emp_sal.alias(\"emp_sal\")\n",
    "emp_dept = df_emp_sal.alias(\"emp_dept\")\n",
    "\n",
    "df_emp_res =emp_sal.join(emp_dept,  (emp_sal.DEPARTMENT_ID == emp_dept.DEPARTMENT_ID), \"inner\")\\\n",
    "    .selectExpr(\"emp_sal.EMPLOYEE_ID\", \"emp_sal.FIRST_NAME\", \"emp_sal.SALARY\", \"emp_dept.FIRST_NAME\", \"emp_dept.SALARY\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebb0dbaf-58c8-4607-a165-65276646525a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a04a57e2-c23f-43cd-893d-d0f6652eea41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp = df_emp.alias(\"emp\")\n",
    "manager = df_emp.alias(\"manager\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de57761c-c55b-48db-955b-70b5d123e8b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 8.Find employees who don’t have a manager\n",
    "\n",
    "df_emp_man = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option('sep','|').load(\"/Volumes/workspace/default/emp/EMP_FF_SRC.txt\")\n",
    "\n",
    "emp = df_emp_man.alias(\"emp\")\n",
    "mgr = df_emp_man.alias(\"mgr\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark practice",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
