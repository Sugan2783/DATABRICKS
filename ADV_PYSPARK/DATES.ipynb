{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79bbc5ba-12ad-458f-9cce-e85e8c05f0d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import *\n",
    "from pyspark import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime\n",
    "df = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b642c06f-c89e-4180-a2ce-c93fbbb94cd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "a = datetime.datetime.now()\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8aa64a26-5b0d-4aa8-adef-811c5d4a5244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "current_date - datetime.now()\n",
    "date_format -- 18/02/2025 -- 2025-02-18\n",
    "to_date --convert to string to date\n",
    "add_months() \n",
    "date-add - adding to the current days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6a79741-674b-4922-bdc5-5439ed7a6ff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = datetime.now()\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebda04af-66b1-4029-8e80-705a550e0b56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
    "df=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e422c14-6230-4f1d-b7e2-a69b499666ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#next_day\n",
    "from pyspark.sql.types import to_date\n",
    "\n",
    "df = df.withColumn(\"next_day\", to_date(col(\"input\"), \"yyyy-MM-dd\").cast(\"timestamp\").cast(\"date\").cast(\"string\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "983d87f7-8c48-47fb-b108-cc7ea8679612",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"next_day\",\n",
    "    to_date(\n",
    "        col(\"input\"),\n",
    "        \"yyyy-MM-dd\"\n",
    "    ).cast(\"string\")\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50701f42-030f-4882-b2dc-e7e232cf5c1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumn(\"next_day\", next_day(df.input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08040417-ad8b-4023-ba17-d1fe8561b498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select(col('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "777d50a6-a9af-4d47-883d-43c7268c7c10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#trunc and date_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6db7931-513b-4fc9-a0c3-16c23b7e304d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df.select(col('date'),trunc(col('date'),\"Month\").alias(\"month_t\"),date_trunc(\"Year\",col(\"date\")).alias(\"year_t\"),date_trunc(\"month\",col(\"date\")).alias(\"month\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e594b927-7a1d-4542-8ad7-e5256df0f653",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#try_to_date--- for dif formats\n",
    "\n",
    "\n",
    "df.withColumn(\"date\", try_to_date(col('input'), \"yyyy-MM-dd\").rlike(r\"^[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "754da80a-839d-440f-ba26-049d08e48b9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    df.withColumn(i,tyr_to_date(col(i),\"yyyy-MM-dd\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b09ad0ec-bb5d-4e24-a5ae-48302449f454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "formats = [\"yyyy-MM-dd\",\"MM-dd-yyyy\",\"dd-MM-yyyy\",\"MM/dd/yyyy\",\"dd/MM/yyyy\",\"yyyy/MM/dd\",\"MM/dd/yyyy\",\"dd/MM/yyyy\",\"yyyy/MM/dd\",\"MM/dd/yyyy\"]\n",
    "df_new = df.withcolumn(\"f_date\",coalesce([try_to_date(col(\"date_str\"),a)for a in formats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5270ce95-be2a-45a8-86ab-af21ca07ed7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df5.withColumn(“new_column”, when((col(“code”) == “a”) | (col(“code”) == “d”), “A”)\n",
    ".when((col(“code”) == “b”) & (col(“amt”) == “4”), “B”)\n",
    ".otherwise(“A1”)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1b3da75-03b1-44e7-9d29-2980b668cf88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca26adc1-f269-4a4c-92fb-1c8f55740cf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def convert_multiple_columns_to_date(df, columns, date_format=\"dd-MMM-yy\"):\n",
    "    \"\"\"\n",
    "    Converts multiple columns in a DataFrame from string type to date type.\n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame.\n",
    "    columns (list of str): A list of column names to convert.\n",
    "    date_format (str): The format of the date in the string columns. Default is \"dd-MMM-yy\".\n",
    "    Returns:\n",
    "    DataFrame: A new DataFrame with the specified columns converted to date type.\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        df = df.withColumn(column, to_date(df[column], date_format))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd51b626-97ad-4791-9b47-99e8f1ad4c9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=[[\"1\",\"2020/02/01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
    "df1=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51298164-01d2-4a07-9c94-4006b9b2af4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Columns to convert\n",
    "my_cols_01 = ['input']\n",
    "\n",
    "# Convert columns\n",
    "df_converted = convert_multiple_columns_to_date(df1, my_cols_01)\n",
    "\n",
    "# Show result\n",
    "display(df_converted.limit(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "072e2e61-b0da-4c98-bf1f-7ba8602b254c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, try_to_date\n",
    "\n",
    "def convert_multiple_columns_to_date(df, columns, fmt= [\"yyyy-MM-dd\",\"MM-dd-yyyy\",\"dd-MM-yyyy\",\"MM/dd/yyyy\",\"dd/MM/yyyy\",\"yyyy/MM/dd\",\"MM/dd/yyyy\",\"dd/MM/yyyy\",\"yyyy/MM/dd\",\"MM/dd/yyyy\"]):\n",
    "    for c in columns:\n",
    "        df = df.withColumn(\n",
    "            c,\n",
    "            try_to_date(col(c), fmt)\n",
    "        )\n",
    "    return df\n",
    "\n",
    "# Columns to convert\n",
    "my_cols_01 = ['input']\n",
    "\n",
    "# Convert columns\n",
    "df_converted = convert_multiple_columns_to_date(df1, my_cols_01)\n",
    "\n",
    "# Show result\n",
    "display(df_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b26ee46-f096-45dd-9001-031090113455",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data=[[\"1\",\"2020/02/01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"],[\"4\",\"27/OCT/2022\"],[\"5\",\"October 27, 2025\"], [\"6\",\"27 October 2025\"],[\"7\",\"27 Oct 2025\"]]\n",
    "df1=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13afa5ac-a7df-423a-99e1-41b4107a293f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, try_to_date, coalesce\n",
    "\n",
    "def con_mul_date_fmat_todate(\n",
    "    df,\n",
    "    columns,\n",
    "    fmts=[\n",
    "        \"yyyy-MM-dd\",\n",
    "        \"MM-dd-yyyy\",\n",
    "        \"dd-MM-yyyy\",\n",
    "        \"MM/dd/yyyy\",\n",
    "        \"dd/MM/yyyy\",\n",
    "        \"yyyy/MM/dd\",\n",
    "        \"dd/MMM/yyyy\",\n",
    "        \"MMMM dd, yyyy\",\n",
    "        \"dd MMMM yyyy\",\n",
    "        \"dd MMM yyyy\"]\n",
    "):\n",
    "    for c in columns:\n",
    "        # Chain try_to_date for each format using coalesce\n",
    "        date_exprs = [try_to_date(col(c), fmt) for fmt in fmts]\n",
    "        df = df.withColumn(c, coalesce(*date_exprs))\n",
    "    return df\n",
    "\n",
    "# Columns to convert\n",
    "my_cols_01 = ['input']\n",
    "\n",
    "# Convert columns\n",
    "df_converted = con_mul_date_fmat_todate(df1, my_cols_01)\n",
    "\n",
    "# Show result\n",
    "display(df_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "042e6a0a-4c05-4014-b605-7c36d0adabd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def con_mul_date_fmat_todate(\n",
    "    df,\n",
    "    columns,\n",
    "    fmts=[\n",
    "        \"yyyy-MM-dd\",\n",
    "        \"MM-dd-yyyy\",\n",
    "        \"dd-MM-yyyy\",\n",
    "        \"MM/dd/yyyy\",\n",
    "        \"dd/MM/yyyy\",\n",
    "        \"yyyy/MM/dd\",\n",
    "        \"dd/MMM/yyyy\",\n",
    "        \"MMMM dd, yyyy\",\n",
    "        \"dd MMMM yyyy\",\n",
    "        \"dd MMM yyyy\"]\n",
    "):\n",
    "    for c in columns:\n",
    "        date_exprs=[]\n",
    "        for fmt in fmts:\n",
    "        # Chain try_to_date for each format using coalesce\n",
    "            date_exprs.append(try_to_date(col(c), fmt) )\n",
    "            df = df.withColumn(c, coalesce(*date_exprs))\n",
    "    return df\n",
    "\n",
    "\n",
    "# Columns to convert\n",
    "my_cols_01 = ['input']\n",
    "\n",
    "# Convert columns\n",
    "df_converted = con_mul_date_fmat_todate(df1, my_cols_01)\n",
    "\n",
    "# Show result\n",
    "display(df_converted)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DATES",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
