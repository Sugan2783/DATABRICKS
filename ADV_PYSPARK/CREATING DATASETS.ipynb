{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8980efa-48b1-415b-ab18-06ee5ad6594a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install fastavro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eda286b-89c8-4703-9487-a9c385385864",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15a95821-0a6f-486e-9f26-71115b6f1a16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from fastavro import writer, parse_schema\n",
    "\n",
    "# # ðŸ”§ Change this path to your local drive folder\n",
    "output_dir = \"/tmp/dataset\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Total rows per file (~500MB target)\n",
    "ROWS = 20_000_000\n",
    "CHUNK_SIZE = 1_000_000  # generate 1M rows at a time\n",
    "\n",
    "# -----------------------------\n",
    "# Utility writers for multiple formats\n",
    "# -----------------------------\n",
    "def write_json(df, filename):\n",
    "    df.to_json(filename, orient=\"records\", lines=True)\n",
    "\n",
    "def write_parquet(df, filename):\n",
    "    df.to_parquet(filename, engine=\"pyarrow\", index=False)\n",
    "\n",
    "def write_orc(df, filename):\n",
    "    df.to_orc(filename, engine=\"pyarrow\", index=False)\n",
    "\n",
    "def write_avro(df, filename):\n",
    "    schema = {\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"Dataset\",\n",
    "        \"fields\": [{\"name\": col, \"type\": [\"null\", \"string\"]} for col in df.columns]\n",
    "    }\n",
    "    parsed_schema = parse_schema(schema)\n",
    "    records = df.astype(str).to_dict(\"records\")\n",
    "    with open(filename, \"wb\") as out:\n",
    "        writer(out, parsed_schema, records)\n",
    "\n",
    "def write_txt(df, filename):\n",
    "    df.to_csv(filename, sep=\"\\t\", index=False, header=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Main function to generate dataset in multiple formats\n",
    "# -----------------------------\n",
    "def generate_dataset(base_name, delimiter, schema_func):\n",
    "    csv_path = os.path.join(output_dir, base_name + \".csv\")\n",
    "    json_path = os.path.join(output_dir, base_name + \".json\")\n",
    "    parquet_path = os.path.join(output_dir, base_name + \".parquet\")\n",
    "    orc_path = os.path.join(output_dir, base_name + \".orc\")\n",
    "    avro_path = os.path.join(output_dir, base_name + \".avro\")\n",
    "    txt_path = os.path.join(output_dir, base_name + \".txt\")\n",
    "\n",
    "    print(f\"ðŸš€ Generating dataset: {base_name}\")\n",
    "\n",
    "    if os.path.exists(csv_path):\n",
    "        os.remove(csv_path)\n",
    "\n",
    "    written_rows = 0\n",
    "    while written_rows < ROWS:\n",
    "        rows_to_write = min(CHUNK_SIZE, ROWS - written_rows)\n",
    "        df = schema_func(rows_to_write)\n",
    "\n",
    "        # Append CSV in chunks\n",
    "        df.to_csv(csv_path, mode=\"a\", index=False, sep=delimiter, header=(written_rows == 0))\n",
    "\n",
    "        # Write other formats once (first chunk)\n",
    "        if written_rows == 0:\n",
    "            write_json(df, json_path)\n",
    "            write_parquet(df, parquet_path)\n",
    "            write_orc(df, orc_path)\n",
    "            write_avro(df, avro_path)\n",
    "            write_txt(df, txt_path)\n",
    "\n",
    "        written_rows += rows_to_write\n",
    "        size_mb = os.path.getsize(csv_path) / (1024 * 1024)\n",
    "        print(f\"   âž¡ {written_rows:,} rows written, current CSV size = {size_mb:.2f} MB\")\n",
    "\n",
    "    print(f\"âœ… {base_name} ready in all formats!\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset schemas (datetime as string)\n",
    "# -----------------------------\n",
    "def sales_schema(n):\n",
    "    products = [\"Laptop\", \"Mouse\", \"Keyboard\", \"Monitor\", \"Printer\"]\n",
    "    df = pd.DataFrame({\n",
    "        \"Date\": pd.date_range(\"2023-01-01\", periods=n, freq=\"S\").astype(str),\n",
    "        \"Product\": np.random.choice(products, size=n),\n",
    "        \"Units_Sold\": np.random.randint(1, 100, size=n),\n",
    "        \"Revenue\": np.random.randint(100, 5000, size=n)\n",
    "    })\n",
    "    return df\n",
    "\n",
    "def web_schema(n):\n",
    "    countries = [\"India\", \"USA\", \"UK\", \"Germany\", \"Canada\"]\n",
    "    df = pd.DataFrame({\n",
    "        \"Date\": pd.date_range(\"2023-01-01\", periods=n, freq=\"S\").astype(str),\n",
    "        \"Visitors\": np.random.randint(1000, 10000, size=n),\n",
    "        \"PageViews\": np.random.randint(2000, 20000, size=n),\n",
    "        \"Country\": np.random.choice(countries, size=n)\n",
    "    })\n",
    "    return df\n",
    "\n",
    "def employee_schema(n):\n",
    "    emp_ids = [f\"E{i:05}\" for i in range(1, 1001)]\n",
    "    df = pd.DataFrame({\n",
    "        \"Emp_ID\": np.random.choice(emp_ids, size=n),\n",
    "        \"Projects_Completed\": np.random.randint(0, 50, size=n),\n",
    "        \"Hours_Worked\": np.random.randint(100, 300, size=n),\n",
    "        \"Performance_Score\": np.random.randint(1, 10, size=n)\n",
    "    })\n",
    "    return df\n",
    "\n",
    "def feedback_schema(n):\n",
    "    products = [\"Laptop\", \"Mouse\", \"Keyboard\", \"Monitor\", \"Printer\"]\n",
    "    df = pd.DataFrame({\n",
    "        \"Feedback_ID\": np.arange(1, n+1),\n",
    "        \"Product\": np.random.choice(products, size=n),\n",
    "        \"Rating\": np.random.randint(1, 6, size=n),\n",
    "        \"Comment\": np.random.choice([\"Good\", \"Average\", \"Poor\", \"Excellent\"], size=n)\n",
    "    })\n",
    "    return df\n",
    "\n",
    "def project_schema(n):\n",
    "    projects = [f\"P{i:04}\" for i in range(1, 500)]\n",
    "    status = [\"On Track\", \"Delayed\", \"Completed\"]\n",
    "    df = pd.DataFrame({\n",
    "        \"Project_ID\": np.random.choice(projects, size=n),\n",
    "        \"Task_Count\": np.random.randint(5, 100, size=n),\n",
    "        \"Completed_Tasks\": np.random.randint(0, 100, size=n),\n",
    "        \"Status\": np.random.choice(status, size=n)\n",
    "    })\n",
    "    return df\n",
    "\n",
    "def finance_schema(n):\n",
    "    dates = pd.date_range(\"2020-01-01\", \"2025-12-31\", freq=\"D\").astype(str)\n",
    "    df = pd.DataFrame({\n",
    "        \"Month\": np.random.choice(dates, size=n),\n",
    "        \"Revenue\": np.random.randint(10000, 500000, size=n),\n",
    "        \"Expenses\": np.random.randint(5000, 300000, size=n),\n",
    "        \"Profit\": np.random.randint(1000, 200000, size=n)\n",
    "    })\n",
    "    return df\n",
    "\n",
    "def inventory_schema(n):\n",
    "    items = [\"ItemA\", \"ItemB\", \"ItemC\", \"ItemD\"]\n",
    "    df = pd.DataFrame({\n",
    "        \"Item_ID\": np.random.choice(items, size=n),\n",
    "        \"Stock_Level\": np.random.randint(0, 1000, size=n),\n",
    "        \"Reorder_Level\": np.random.randint(50, 200, size=n),\n",
    "        \"Warehouse\": np.random.choice([\"WH1\", \"WH2\", \"WH3\"], size=n)\n",
    "    })\n",
    "    return df\n",
    "\n",
    "def marketing_schema(n):\n",
    "    campaigns = [f\"CMP{i:04}\" for i in range(1, 200)]\n",
    "    df = pd.DataFrame({\n",
    "        \"Campaign_ID\": np.random.choice(campaigns, size=n),\n",
    "        \"Leads_Generated\": np.random.randint(50, 5000, size=n),\n",
    "        \"Conversions\": np.random.randint(0, 500, size=n),\n",
    "        \"Spend\": np.random.randint(1000, 50000, size=n)\n",
    "    })\n",
    "    return df\n",
    "\n",
    "def healthcare_schema(n):\n",
    "    patients = [f\"PT{i:06}\" for i in range(1, 5000)]\n",
    "    df = pd.DataFrame({\n",
    "        \"Patient_ID\": np.random.choice(patients, size=n),\n",
    "        \"Visits\": np.random.randint(1, 20, size=n),\n",
    "        \"Recovery_Days\": np.random.randint(1, 100, size=n),\n",
    "        \"Treatment_Success\": np.random.choice([0, 1], size=n)\n",
    "    })\n",
    "    return df\n",
    "\n",
    "def education_schema(n):\n",
    "    students = [f\"S{i:06}\" for i in range(1, 5000)]\n",
    "    df = pd.DataFrame({\n",
    "        \"Student_ID\": np.random.choice(students, size=n),\n",
    "        \"Subject\": np.random.choice([\"Math\", \"Science\", \"History\", \"English\"], size=n),\n",
    "        \"Score\": np.random.randint(0, 100, size=n),\n",
    "        \"Attendance\": np.random.randint(50, 100, size=n)\n",
    "    })\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# List of datasets\n",
    "# -----------------------------\n",
    "datasets = [\n",
    "    (\"sales\", \",\", sales_schema),\n",
    "    (\"web_analytics\", \";\", web_schema),\n",
    "    (\"employee_performance\", \"|\", employee_schema),\n",
    "    (\"customer_feedback\", \":\", feedback_schema),\n",
    "    (\"project_tracking\", \"\\t\", project_schema),\n",
    "    (\"finance_report\", \"~\", finance_schema),\n",
    "    (\"inventory\", \"^\", inventory_schema),\n",
    "    (\"marketing_campaign\", \"@\", marketing_schema),\n",
    "    (\"healthcare\", \"#\", healthcare_schema),\n",
    "    (\"education\", \" \", education_schema),\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Generate all datasets\n",
    "# -----------------------------\n",
    "for name, delim, schema in datasets:\n",
    "    generate_dataset(name, delim, schema)\n",
    "\n",
    "print(\"ðŸŽ‰ All datasets generated in CSV, JSON, Parquet, ORC, Avro, TXT!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c84592a2-62a7-4ba6-819d-d28f8dba481b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files = os.listdir(\"/tmp/dataset\")\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e1a1106-b0ee-4292-817d-8ce144156950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\n",
    "    \"/tmp/sales.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CREATING DATASETS",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
