{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1eadc007-f55a-4be5-a56b-8381f12e9505",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c6bfe06-20c5-4784-9e85-2c7dc6440756",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls /Volumes/workspace/default/emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40b4b857-de8d-4de9-98ab-b37c37e6f8a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "815f194c-9300-4da0-b68e-3e44939ce228",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6835264a-c013-4370-9ef9-b25ac3e6564d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://jsonplaceholder.typicode.com/posts/1\"\n",
    "payload = {\"id\" : [1,2,3], \"userId\" : 1, \"title\" : \"Hello World\", \"body\" : \"This is a test\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cc9de66-cc3a-4404-a848-7b2d138851b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "response = requests.get(url,payload)\n",
    "print(response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5fea5d8-7a88-4bff-8c1f-e24c32eb9510",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "response_json = response.json()\n",
    "print(response_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ccd66f3-e5f9-480f-9e1d-a5ced40255ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jsondata = json.loads(json.dumps(response_json))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86392a7e-e5d2-4c87-b792-f60b869eda6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(sc.parallelize([jsondata]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77e09d0a-134e-48c7-a1b7-e2b3acdcea8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Assuming spark is already initialized\n",
    "jsondata = '{\"name\": \"John\", \"age\": 30}'  # Example JSON data\n",
    "rdd = spark.sparkContext.parallelize([jsondata])\n",
    "df = spark.read.json(rdd)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77be01fc-705d-4044-b011-be211c4ed3d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Assuming spark is already initialized\n",
    "jsondata = '{\"name\": \"John\", \"age\": 30}'  # Example JSON data\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame from JSON data\n",
    "df = spark.read.schema(schema).json(spark.sparkContext.parallelize([jsondata]))\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba3f986-1db7-42d2-acbe-0bd27bd20bc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, FloatType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"OBJECT_ID\", IntegerType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"Customer_ID\", IntegerType(), True),\n",
    "    StructField(\"Change_Date\", StringType(), True),\n",
    "    StructField(\"Load_Date\", StringType(), True),\n",
    "    StructField(\"Status\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"start_date_source\", StringType(), True),\n",
    "    StructField(\"start_date_bronze\", StringType(), True)\n",
    "])\n",
    "\n",
    "data = [(583069, \"Harish\", None, 13681832, None, '2025-06-02', None, 'E-Mail', 1724256609000, None),\n",
    "        (510102, \"\", \"HR\", 40685884, '2025-04-02T04:15:05Z', '2025-06-02', 'Finished', 'Internet', 1724256609000, None),\n",
    "        (506654, \"Basha\", \"\", None, '2025-04-02T04:15:05Z', '2025-06-02', 'Not Relevant', 'Social Media', 1724256609000, None),\n",
    "        (583195, None, \"Finance\", 12619703, None, '2025-06-02', 'Started', 'Messaging', 1724256609000, None),\n",
    "        (470450, \"Venky\", \"IT\", 8541938, '2025-04-02T07:59:14Z', '2025-06-02', 'Not Relevant', 'IoT', 1724256609000, None),\n",
    "        (558253, \"\", None, 2269299, None, '2025-06-02', 'Open', None, 1724256609000, None),\n",
    "        (None, \"Krishna\", \"Sales\", None, '2025-04-02T06:12:18Z', '2025-06-02', None, 'Manual data entry', 1724256609000, None),\n",
    "        (583181, \"Kiran\", \"Marketing\", 39714449, None, '2025-06-02', 'Finished', 'Other', 1724256609000, None),\n",
    "        (583119, \"Hitesh\", None, 10183510, '2025-04-02T04:15:13Z', None, 'Open', 'Telephony', 1724256609000, None),\n",
    "        (577519, \"\", \"Accounts\", None, '2025-04-02T08:27:50Z', '2025-06-02', 'Not Relevant', None, 1724256609000, None),\n",
    "        (583151, \"Sushma\", \"Accounts\", 40442877, None, '2025-06-02', 'Open', 'Fax', 1724256609000, None),\n",
    "        (583167, None, \"Admin\", 16474490, '2025-04-02T09:07:27Z', None, 'Not Relevant', 'Feedback', 1724256609000, None),\n",
    "        (583162, \"Buvan\", \"IT\", 7447339, '2025-04-02T16:46:07Z', None, 'Finished', 'WorkZone', 1724256609000, None),\n",
    "        (575216, \"Mohan\", \"Admin\", 17258071, '2025-04-02T01:51:03Z', '2025-06-02', 'Open', 'IOT', 1724256609000, None),\n",
    "        (None, None, None, None, None, None, None, None, None, None),\n",
    "        (583173, \"Lohith\", \"Finance\", 15113750, None, '2025-06-02', 'Finished', None, 1724256609000, None),\n",
    "        (583099, \"Loba\", \"Testing\", 40505376, '2025-04-02T19:54:50Z', None, 'Started', None, 1724256609000, None)\n",
    "       ]\n",
    "\n",
    "df_dev = spark.createDataFrame(data, schema)\n",
    "df_dev.withColumn(\"Customer_ID\", col(\"Customer_ID\").cast(StringType()))\n",
    "display(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "695e78a0-405f-42c4-9b3f-a4c504076c8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, when, lit\n",
    "def count_values(df, column):\n",
    "    return df_dev.select(\n",
    "        count(when(col(column).isNull(),column)).alias(\"Null columns\"),\n",
    "        count(when(col(column).isNotNull(),column)).alias(\"Valid_columns\"),\n",
    "        count(when(col(column) == '',column)).alias(\"Empty columns\"),\n",
    "        count(lit(1).alias('Total'))\n",
    "    )    \n",
    "\n",
    "\n",
    "df_dev.withColumn(\"Customer_ID\", col(\"Customer_ID\").cast(StringType()))\n",
    "display(df_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "614a38bc-bc35-4268-9be7-ae8753172a0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(count_values(df_dev, 'Name'))\n",
    "display(count_values(df_dev, 'department'))\n",
    "display(count_values(df_dev, 'Customer_ID'))\n",
    "display(count_values(df_dev, 'Change_Date'))\n",
    "display(count_values(df_dev, 'Load_Date'))\n",
    "display(count_values(df_dev, 'Status'))\n",
    "display(count_values(df_dev, 'description'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c995db72-40b8-4587-8db4-8b28f1bcace0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def count_values(df, column_name):\n",
    "    return df.groupBy(column_name).count()\n",
    "\n",
    "display(count_values(df_dev, 'Name'))\n",
    "display(count_values(df_dev, 'department'))\n",
    "display(count_values(df_dev.withColumn('Customer_ID', try_cast('Customer_ID' as 'BIGINT')), 'Customer_ID'))\n",
    "display(count_values(df_dev, 'Change_Date'))\n",
    "display(count_values(df_dev, 'Load_Date'))\n",
    "display(count_values(df_dev, 'Status'))\n",
    "display(count_values(df_dev, 'description'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59391601-eb59-4b24-934b-a5025ff9ed6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def count_values(df, column_name):\n",
    "    return df.groupBy(column_name).count()\n",
    "\n",
    "display(count_values(df_dev, 'Name'))\n",
    "display(count_values(df_dev, 'department'))\n",
    "display(count_values(df_dev.withColumn('Customer_ID', df_dev['Customer_ID'].cast('BIGINT')), 'Customer_ID'))\n",
    "display(count_values(df_dev, 'Change_Date'))\n",
    "display(count_values(df_dev, 'Load_Date'))\n",
    "display(count_values(df_dev, 'Status'))\n",
    "display(count_values(df_dev, 'description'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaa66104-933b-400f-a8e0-743d9bb22505",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60959085-0d0b-49cd-aaa7-ee6b4279e9d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import locate,expr\n",
    "\n",
    "# Sample data\n",
    "data = [(\"Azure data engineer (ADE)\", \"suman@gmail.com\"),\n",
    "        (\"AWS data engineer (AWS)\", \"kiranrathod@gmail.com\"),\n",
    "        (\"data warehouse\", \"rameshwaran@gmail.com\"),\n",
    "        (\"GCP engineer\", \"krishnamurthy@gmail.com\"),\n",
    "        (\"PySpark engineer\", \"vishweswarrao@gmail.com\")]\n",
    "\n",
    "columns = [\"text\", \"mail\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60906598-d309-4b49-9b9c-ae4c03ef3c7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_loc = df.withColumn(\"position\",locate((\"AWS\"),col(\"text\"))).filter(col(\"position\") == 1)\n",
    "df_loc.display()\n",
    "\n",
    "df_dom = df.withColumn(\"domain\",locate((\"@\"),col(\"mail\"))).filter(col(\"domain\") > 0)\n",
    "df_mail = df_dom.withColumn(\"domain_name\", expr(\"substring(col(\"mail\"), col(\"domain\")+1, length(col(\"mail\"))-col(\"domain\"))\"))\n",
    "df_mail.display()\n",
    "df_dom.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3841b3a5-a7d2-4392-8803-fc54eab71dab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_loc = df.withColumn(\"position\", locate(\"AWS\", col(\"text\"))).filter(col(\"position\") == 1)\n",
    "display(df_loc)\n",
    "\n",
    "df_dom = df.withColumn(\"domain\", locate(\"@\", col(\"mail\"))).filter(col(\"domain\") > 0)\n",
    "df_mail = df_dom.withColumn(\"domain_name\", expr(\"substring(col('mail'),'@',length(col('mail'))-col('domain'))\"))\n",
    "display(df_mail)\n",
    "display(df_dom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1c841d7-c3b6-4311-993e-b86c29a3037a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Sample Data\n",
    "data = [\n",
    "    (1, \"ITC\", 59000, \"2024-01-15\"),\n",
    "    (2, \"BEML\", 68000, \"2023-12-10\"),\n",
    "    (3, \"HCL\", 53500, \"2022-06-25\"),\n",
    "    (4, \"AIRTEL\", 77800, \"2021-09-30\"),\n",
    "    (5, \"ACT\", 5550, \"2024-05-15\"),\n",
    "    (6, \"TATA\", 95600, \"2023-09-15\"),\n",
    "    (7, \"BEML\", 87500, \"2025-02-05\"),\n",
    "    (8, \"AIRTEL\", 95600, \"2021-06-20\"),\n",
    "    (9, \"ACT\", 65000, \"2024-02-04\"),\n",
    "    (10, \"ITC\", 36700, \"2022-09-08\"),\n",
    "    (11, \"TATA\", 175600, \"2023-06-15\"),\n",
    "    (12, \"ITC\", 98700, \"2022-12-18\"),\n",
    "    (13, \"BEML\", 99550, \"2023-01-22\"),\n",
    "    (14, \"AIRTEL\", 395800, \"2020-02-23\")\n",
    "]\n",
    "\n",
    "# Define Schema\n",
    "columns = [\"ID\", \"Company\", \"Salary\", \"JoiningDate\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = df.withColumn(\"JoiningDate\", col(\"JoiningDate\").cast(DateType()))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "003c97ea-0727-4b43-baf9-03bc9789159f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import min, max, col\n",
    "from pyspark.sql.types import DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8bbe5b6-f379-485d-9a85-7a701c0accbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Sample Data\n",
    "data = [\n",
    "    (1, \"ITC\", 59000, \"2024-01-15\"),\n",
    "    (2, \"BEML\", 68000, \"2023-12-10\"),\n",
    "    (3, \"HCL\", 53500, \"2022-06-25\"),\n",
    "    (4, \"AIRTEL\", 77800, \"2021-09-30\"),\n",
    "    (5, \"ACT\", 5550, \"2024-05-15\"),\n",
    "    (6, \"TATA\", 95600, \"2023-09-15\"),\n",
    "    (7, \"BEML\", 87500, \"2025-02-05\"),\n",
    "    (8, \"AIRTEL\", 95600, \"2021-06-20\"),\n",
    "    (9, \"ACT\", 65000, \"2024-02-04\"),\n",
    "    (10, \"ITC\", 36700, \"2022-09-08\"),\n",
    "    (11, \"TATA\", 175600, \"2023-06-15\"),\n",
    "    (12, \"ITC\", 98700, \"2022-12-18\"),\n",
    "    (13, \"BEML\", 99550, \"2023-01-22\"),\n",
    "    (14, \"AIRTEL\", 395800, \"2020-02-23\")\n",
    "]\n",
    "\n",
    "# Define Schema\n",
    "columns = [\"ID\", \"Company\", \"Salary\", \"JoiningDate\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = df.withColumn(\"JoiningDate\", col(\"JoiningDate\").cast(DateType()))\n",
    "display(df)\")).max(col(\"\n",
    "# Sample Data\n",
    "data = [\n",
    "    (1, \"ITC\", 59000, \"2024-01-15\"),\n",
    "    (2, \"BEML\", 68000, \"2023-12-10\"),\n",
    "    (3, \"HCL\", 53500, \"2022-06-25\"),\n",
    "    (4, \"AIRTEL\", 77800, \"2021-09-30\"),\n",
    "    (5, \"ACT\", 5550, \"2024-05-15\"),\n",
    "    (6, \"TATA\", 95600, \"2023-09-15\"),\n",
    "    (7, \"BEML\", 87500, \"2025-02-05\"),\n",
    "    (8, \"AIRTEL\", 95600, \"2021-06-20\"),\n",
    "    (9, \"ACT\", 65000, \"2024-02-04\"),\n",
    "    (10, \"ITC\", 36700, \"2022-09-08\"),\n",
    "    (11, \"TATA\", 175600, \"2023-06-15\"),\n",
    "    (12, \"ITC\", 98700, \"2022-12-18\"),\n",
    "    (13, \"BEML\", 99550, \"2023-01-22\"),\n",
    "    (14, \"AIRTEL\", 395800, \"2020-02-23\")\n",
    "]\n",
    "\n",
    "# Define Schema\n",
    "columns = [\"ID\", \"Company\", \"Salary\", \"JoiningDate\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = df.withColumn(\"JoiningDate\", col(\"JoiningDate\").cast(DateType()))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb5edca2-c41d-46aa-93ac-9af2c7699447",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_min = df.agg(min(col(\"Salary\")), max(col(\"salary\"))).display()\n",
    "df_date = df.agg(min(col(\"joiningDate\")), max(col(\"joiningDate\"))).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "905d04e1-5b7a-4f20-8a34-c64160f5489c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#convert rdd to data frame\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RDD_to_DF_Example\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Convert RDD to DataFrame with default column names\n",
    "df_default = rdd.toDF()\n",
    "df_default.show()\n",
    "\n",
    "# Convert RDD to DataFrame with custom column names\n",
    "columns = [\"Name\", \"ID\"]\n",
    "df_custom = rdd.toDF(columns)\n",
    "df_custom.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "737a3525-16e3-495d-a44c-ed890a905269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataFrame to RDD Conversion\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [\n",
    "    Row(name=\"Alice\", age=30),\n",
    "    Row(name=\"Bob\", age=25),\n",
    "    Row(name=\"Charlie\", age=35)\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Convert the DataFrame to an RDD\n",
    "rdd = df.rdd\n",
    "\n",
    "# Perform an action to see the content of the RDD (e.g., collect)\n",
    "collected_rdd = rdd.collect()\n",
    "\n",
    "# Print the collected RDD\n",
    "print(collected_rdd)\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8019491706881161,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Configurations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
