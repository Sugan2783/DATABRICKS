{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8f9dd2a-9e44-412d-be94-b805df0dcdbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Customer Data\n",
    "customer_id: string\n",
    "first_name: string\n",
    "last_name: string\n",
    "date_of_birth: date\n",
    "email: string\n",
    "phone_number: string\n",
    "address: string\n",
    "city: string\n",
    "state: string\n",
    "zip_code: string\n",
    "country: string\n",
    "customer_since: date\n",
    "credit_score: integer\n",
    "risk_segment: string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ac494da-8f6e-4163-9141-f568f85b9b42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "1. Customer Data\n",
    "customer_id: string\n",
    "first_name: string\n",
    "last_name: string\n",
    "date_of_birth: date\n",
    "email: string\n",
    "phone_number: string\n",
    "address: string\n",
    "city: string\n",
    "state: string\n",
    "zip_code: string\n",
    "country: string\n",
    "customer_since: date\n",
    "credit_score: integer\n",
    "risk_segment: string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a8389ef-0e61-4fdc-876b-3ee34e81e48f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "2. Account Data\n",
    "account_id: string\n",
    "customer_id: string\n",
    "account_type: string (checking, savings, investment)\n",
    "account_status: string (active, closed, suspended)\n",
    "open_date: date\n",
    "close_date: date\n",
    "currency: string\n",
    "branch_id: string\n",
    "interest_rate: float\n",
    "balance: decimal\n",
    "last_activity_date: date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1d52109-b28e-4810-9806-4538784b8068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "3. Transaction Data\n",
    "transaction_id: string\n",
    "account_id: string\n",
    "transaction_date: timestamp\n",
    "transaction_type: string (deposit, withdrawal, transfer, payment)\n",
    "amount: decimal\n",
    "currency: string\n",
    "description: string\n",
    "merchant_name: string\n",
    "merchant_category: string\n",
    "transaction_status: string (completed, pending, failed, reversed)\n",
    "channel: string (online, mobile, branch, atm)\n",
    "location: string\n",
    "is_international: boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "958698aa-0552-4722-a5ae-3c57b61d8918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "4. Credit Card Data\n",
    "card_id: string\n",
    "customer_id: string\n",
    "account_id: string\n",
    "card_type: string (visa, mastercard, amex)\n",
    "card_status: string (active, blocked, expired)\n",
    "issue_date: date\n",
    "expiry_date: date\n",
    "credit_limit: decimal\n",
    "current_balance: decimal\n",
    "available_credit: decimal\n",
    "last_payment_date: date\n",
    "last_payment_amount: decimal\n",
    "interest_rate: float\n",
    "reward_points: integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fe14644-8ced-4d7e-9a31-218d23155b7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "5. Loan Data\n",
    "loan_id: string\n",
    "customer_id: string\n",
    "loan_type: string (personal, mortgage, auto, business)\n",
    "loan_status: string (active, paid, defaulted)\n",
    "loan_amount: decimal\n",
    "interest_rate: float\n",
    "term_months: integer\n",
    "start_date: date\n",
    "end_date: date\n",
    "monthly_payment: decimal\n",
    "remaining_balance: decimal\n",
    "next_payment_date: date\n",
    "collateral_value: decimal\n",
    "collateral_type: string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0e4f2d1-d161-444b-9ec7-a9238a86d225",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "1. Spark Session Initialization\n",
    "# src/utils/spark_session.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "\n",
    "def create_spark_session(app_name=\"Banking ETL Pipeline\"):\n",
    "    \"\"\"\n",
    "    Create and configure a Spark session for the ETL pipeline.\n",
    "    \n",
    "    Args:\n",
    "        app_name (str): Name of the Spark application\n",
    "        \n",
    "    Returns:\n",
    "        SparkSession: Configured Spark session\n",
    "    \"\"\"\n",
    "    # For Databricks, we can use the existing spark session\n",
    "    if \"DATABRICKS_RUNTIME_VERSION\" in os.environ:\n",
    "        return SparkSession.builder.getOrCreate()\n",
    "    \n",
    "    # For local or AWS EMR execution\n",
    "    return (SparkSession.builder\n",
    "            .appName(app_name)\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "            .config(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "            .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "            .config(\"spark.sql.legacy.parquet.datetimeRebaseModeInWrite\", \"LEGACY\")\n",
    "            .config(\"spark.sql.warehouse.dir\", \"s3://your-data-lake-bucket/warehouse\")\n",
    "            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "            .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \n",
    "                   \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\")\n",
    "            .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"100\")\n",
    "            .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "            .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "            .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "            .config(\"spark.default.parallelism\", \"200\")\n",
    "            .enableHiveSupport()\n",
    "            .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d6cb2b6-12ba-4ed5-8961-bf5c9fda5441",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "2. Data Ingestion from S3\n",
    "# src/ingestion/s3_connector.py\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class S3Connector:\n",
    "    \"\"\"Class to handle data ingestion from AWS S3.\"\"\"\n",
    "    \n",
    "    def __init__(self, spark: SparkSession, bucket_name: str):\n",
    "        \"\"\"\n",
    "        Initialize S3 connector.\n",
    "        \n",
    "        Args:\n",
    "            spark (SparkSession): Spark session\n",
    "            bucket_name (str): S3 bucket name\n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "        self.bucket_name = bucket_name\n",
    "        \n",
    "    def read_csv(self, file_path: str, header: bool = True, infer_schema: bool = True) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Read CSV file from S3.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the CSV file in S3\n",
    "            header (bool): Whether the CSV has a header\n",
    "            infer_schema (bool): Whether to infer the schema\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Spark DataFrame containing the data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            full_path = f\"s3a://{self.bucket_name}/{file_path}\"\n",
    "            logger.info(f\"Reading CSV file from {full_path}\")\n",
    "            \n",
    "            return (self.spark.read\n",
    "                   .option(\"header\", header)\n",
    "                   .option(\"inferSchema\", infer_schema)\n",
    "                   .csv(full_path))\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading CSV file from {full_path}: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def read_parquet(self, file_path: str) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Read Parquet file from S3.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the Parquet file in S3\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Spark DataFrame containing the data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            full_path = f\"s3a://{self.bucket_name}/{file_path}\"\n",
    "            logger.info(f\"Reading Parquet file from {full_path}\")\n",
    "            \n",
    "            return self.spark.read.parquet(full_path)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading Parquet file from {full_path}: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def read_delta(self, file_path: str) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Read Delta table from S3.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the Delta table in S3\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Spark DataFrame containing the data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            full_path = f\"s3a://{self.bucket_name}/{file_path}\"\n",
    "            logger.info(f\"Reading Delta table from {full_path}\")\n",
    "            \n",
    "            return self.spark.read.format(\"delta\").load(full_path)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error reading Delta table from {full_path}: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78851a8c-46b1-4e16-8e14-4facf7c19fd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "3. Data Transformation for Transactions\n",
    "# src/transformation/transaction_transform.py\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TransactionTransformer:\n",
    "    \"\"\"Class to handle transaction data transformations.\"\"\"\n",
    "    \n",
    "    def __init__(self, spark: SparkSession):\n",
    "        \"\"\"\n",
    "        Initialize transaction transformer.\n",
    "        \n",
    "        Args:\n",
    "            spark (SparkSession): Spark session\n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "        \n",
    "    def clean_transaction_data(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Clean transaction data by handling missing values and data type conversions.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): Raw transaction data\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Cleaned transaction data\n",
    "        \"\"\"\n",
    "        logger.info(\"Cleaning transaction data\")\n",
    "        \n",
    "        # Convert date strings to timestamp\n",
    "        df = df.withColumn(\"transaction_date\", \n",
    "                          to_timestamp(col(\"transaction_date\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "        \n",
    "        # Handle missing values\n",
    "        df = df.na.fill(\"Unknown\", [\"merchant_name\", \"merchant_category\", \"description\"])\n",
    "        \n",
    "        # Filter out invalid transactions (e.g., negative amounts for deposits)\n",
    "        df = df.filter(~((col(\"transaction_type\") == \"deposit\") & (col(\"amount\") < 0)))\n",
    "        \n",
    "        # Standardize transaction types\n",
    "        df = df.withColumn(\"transaction_type\", \n",
    "                          when(col(\"transaction_type\").isin(\"deposit\", \"DEPOSIT\", \"Deposit\"), \"deposit\")\n",
    "                          .when(col(\"transaction_type\").isin(\"withdrawal\", \"WITHDRAWAL\", \"Withdrawal\"), \"withdrawal\")\n",
    "                          .when(col(\"transaction_type\").isin(\"transfer\", \"TRANSFER\", \"Transfer\"), \"transfer\")\n",
    "                          .when(col(\"transaction_type\").isin(\"payment\", \"PAYMENT\", \"Payment\"), \"payment\")\n",
    "                          .otherwise(col(\"transaction_type\")))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def enrich_transaction_data(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Enrich transaction data with additional features.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): Cleaned transaction data\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Enriched transaction data\n",
    "        \"\"\"\n",
    "        logger.info(\"Enriching transaction data\")\n",
    "        \n",
    "        # Extract date components\n",
    "        df = df.withColumn(\"transaction_year\", year(col(\"transaction_date\")))\n",
    "        df = df.withColumn(\"transaction_month\", month(col(\"transaction_date\")))\n",
    "        df = df.withColumn(\"transaction_day\", dayofmonth(col(\"transaction_date\")))\n",
    "        df = df.withColumn(\"transaction_hour\", hour(col(\"transaction_date\")))\n",
    "        df = df.withColumn(\"transaction_dayofweek\", dayofweek(col(\"transaction_date\")))\n",
    "        \n",
    "        # Flag for weekend transactions\n",
    "        df = df.withColumn(\"is_weekend\", \n",
    "                          when(col(\"transaction_dayofweek\").isin(1, 7), True)\n",
    "                          .otherwise(False))\n",
    "        \n",
    "        # Calculate transaction amount in USD (assuming currency conversion)\n",
    "        df = df.withColumn(\"amount_usd\", \n",
    "                          when(col(\"currency\") == \"USD\", col(\"amount\"))\n",
    "                          .when(col(\"currency\") == \"EUR\", col(\"amount\") * 1.1)\n",
    "                          .when(col(\"currency\") == \"GBP\", col(\"amount\") * 1.3)\n",
    "                          .otherwise(col(\"amount\")))\n",
    "        \n",
    "        # Add transaction category based on merchant category\n",
    "        df = df.withColumn(\"transaction_category\",\n",
    "                          when(col(\"merchant_category\").isin(\"grocery\", \"supermarket\"), \"Groceries\")\n",
    "                          .when(col(\"merchant_category\").isin(\"restaurant\", \"fast food\"), \"Dining\")\n",
    "                          .when(col(\"merchant_category\").isin(\"gas\", \"fuel\"), \"Transportation\")\n",
    "                          .when(col(\"merchant_category\").isin(\"utility\", \"electricity\", \"water\"), \"Utilities\")\n",
    "                          .otherwise(\"Other\"))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def calculate_transaction_metrics(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate transaction metrics like running balances and spending patterns.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): Enriched transaction data\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Transaction data with metrics\n",
    "        \"\"\"\n",
    "        logger.info(\"Calculating transaction metrics\")\n",
    "        \n",
    "        # Define window for running calculations by account\n",
    "        window_spec = Window.partitionBy(\"account_id\").orderBy(\"transaction_date\")\n",
    "        \n",
    "        # Calculate running balance\n",
    "        df = df.withColumn(\"amount_signed\", \n",
    "                          when(col(\"transaction_type\").isin(\"deposit\", \"transfer_in\"), col(\"amount_usd\"))\n",
    "                          .otherwise(-col(\"amount_usd\")))\n",
    "        \n",
    "        df = df.withColumn(\"running_balance\", sum(\"amount_signed\").over(window_spec))\n",
    "        \n",
    "        # Calculate days since last transaction\n",
    "        df = df.withColumn(\"prev_transaction_date\", \n",
    "                          lag(\"transaction_date\", 1).over(window_spec))\n",
    "        \n",
    "        df = df.withColumn(\"days_since_last_transaction\", \n",
    "                          when(col(\"prev_transaction_date\").isNull(), 0)\n",
    "                          .otherwise(datediff(col(\"transaction_date\"), col(\"prev_transaction_date\"))))\n",
    "        \n",
    "        # Calculate transaction frequency metrics\n",
    "        window_30d = Window.partitionBy(\"account_id\")\\\n",
    "                           .orderBy(\"transaction_date\")\\\n",
    "                           .rangeBetween(-30 * 86400, 0)  # 30 days in seconds\n",
    "        \n",
    "        df = df.withColumn(\"transaction_count_30d\", count(\"transaction_id\").over(window_30d))\n",
    "        df = df.withColumn(\"total_spend_30d\", \n",
    "                          sum(when(col(\"transaction_type\").isin(\"withdrawal\", \"payment\"), col(\"amount_usd\"))\n",
    "                             .otherwise(0)).over(window_30d))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def detect_anomalies(self, df: DataFrame) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Detect anomalous transactions based on various rules.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): Transaction data with metrics\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Transaction data with anomaly flags\n",
    "        \"\"\"\n",
    "        logger.info(\"Detecting anomalous transactions\")\n",
    "        \n",
    "        # Calculate account-level statistics\n",
    "        account_stats = df.groupBy(\"account_id\").agg(\n",
    "            stddev(\"amount_usd\").alias(\"amount_stddev\"),\n",
    "            avg(\"amount_usd\").alias(\"amount_avg\"),\n",
    "            max(\"amount_usd\").alias(\"amount_max\")\n",
    "        )\n",
    "        \n",
    "        # Join with transaction data\n",
    "        df = df.join(account_stats, on=\"account_id\", how=\"left\")\n",
    "        \n",
    "        # Flag large transactions (> 3 standard deviations from mean)\n",
    "        df = df.withColumn(\"is_large_transaction\", \n",
    "                          (col(\"amount_usd\") > (col(\"amount_avg\") + 3 * col(\"amount_stddev\"))) &\n",
    "                          (col(\"amount_usd\") > 1000))\n",
    "        \n",
    "        # Flag transactions in unusual locations\n",
    "        df = df.withColumn(\"is_unusual_location\", \n",
    "                          col(\"is_international\") & \n",
    "                          ~col(\"location\").isin(\"Canada\", \"Mexico\", \"United Kingdom\", \"France\", \"Germany\"))\n",
    "        \n",
    "        # Flag high-frequency transactions\n",
    "        df = df.withColumn(\"is_high_frequency\", \n",
    "                          col(\"transaction_count_30d\") > 100)\n",
    "        \n",
    "        # Flag potential fraud based on combined factors\n",
    "        df = df.withColumn(\"potential_fraud\", \n",
    "                          col(\"is_large_transaction\") | \n",
    "                          col(\"is_unusual_location\") |\n",
    "                          (col(\"days_since_last_transaction\") < 0.01))  # Multiple transactions in seconds\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff27f60e-552e-4669-b616-73b34450f8ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Explanation: The TransactionTransformer class handles the transformation of transaction data through several key methods:\n",
    "\n",
    "clean_transaction_data(): Performs basic data cleaning operations:\n",
    "- Converts string dates to proper timestamp format\n",
    "- Fills missing values with defaults for text fields\n",
    "- Removes invalid transactions (like negative deposit amounts)\n",
    "- Standardizes transaction type values for consistency\n",
    "enrich_transaction_data(): Adds valuable derived features:\n",
    "- Extracts date components (year, month, day, hour, day of week)\n",
    "- Adds a weekend flag for time-based analysis\n",
    "- Converts transaction amounts to USD for consistent analysis\n",
    "- Categorizes transactions based on merchant category\n",
    "calculate_transaction_metrics(): Computes advanced metrics using window functions:\n",
    "- Calculates running balance for each account\n",
    "- Determines days between transactions\n",
    "- Computes 30-day transaction counts and spending totals\n",
    "detect_anomalies(): Identifies potentially suspicious transactions:\n",
    "- Calculates statistical metrics for each account\n",
    "- Flags unusually large transactions (statistical outliers)\n",
    "- Identifies transactions in unusual locations\n",
    "- Detects high-frequency transaction patterns\n",
    "- Creates a composite fraud indicator based on multiple factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec5c0f9f-4344-413d-a51b-d094640f24c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "4. Data Quality Checks\n",
    "# src/transformation/data_quality.py\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from typing import Dict, List, Tuple\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataQualityChecker:\n",
    "    \"\"\"Class to perform data quality checks on datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, spark: SparkSession):\n",
    "        \"\"\"\n",
    "        Initialize data quality checker.\n",
    "        \n",
    "        Args:\n",
    "            spark (SparkSession): Spark session\n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "        \n",
    "    def check_nulls(self, df: DataFrame, required_columns: List[str]) -> Tuple[bool, Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Check for null values in required columns.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): DataFrame to check\n",
    "            required_columns (List[str]): List of columns that should not have nulls\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[bool, Dict[str, int]]: (passed/failed, dict of null counts by column)\n",
    "        \"\"\"\n",
    "        logger.info(f\"Checking for nulls in columns: {required_columns}\")\n",
    "        \n",
    "        # Count nulls in each column\n",
    "        null_counts = {}\n",
    "        for column in required_columns:\n",
    "            if column in df.columns:\n",
    "                null_count = df.filter(col(column).isNull()).count()\n",
    "                null_counts[column] = null_count\n",
    "            else:\n",
    "                logger.warning(f\"Column {column} not found in DataFrame\")\n",
    "                null_counts[column] = \"Column not found\"\n",
    "        \n",
    "        # Check if any required column has nulls\n",
    "        has_nulls = any(isinstance(count, int) and count > 0 for count in null_counts.values())\n",
    "        \n",
    "        if has_nulls:\n",
    "            logger.warning(f\"Null check failed. Null counts: {null_counts}\")\n",
    "            return False, null_counts\n",
    "        else:\n",
    "            logger.info(\"Null check passed\")\n",
    "            return True, null_counts\n",
    "    \n",
    "    def check_duplicates(self, df: DataFrame, key_columns: List[str]) -> Tuple[bool, int]:\n",
    "        \"\"\"\n",
    "        Check for duplicate records based on key columns.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): DataFrame to check\n",
    "            key_columns (List[str]): Columns that should form a unique key\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[bool, int]: (passed/failed, count of duplicate records)\n",
    "        \"\"\"\n",
    "        logger.info(f\"Checking for duplicates on key columns: {key_columns}\")\n",
    "        \n",
    "        # Count total rows\n",
    "        total_rows = df.count()\n",
    "        \n",
    "        # Count distinct rows based on key columns\n",
    "        distinct_rows = df.select(key_columns).distinct().count()\n",
    "        \n",
    "        # Calculate duplicates\n",
    "        duplicate_count = total_rows - distinct_rows\n",
    "        \n",
    "        if duplicate_count > 0:\n",
    "            logger.warning(f\"Duplicate check failed. Found {duplicate_count} duplicates\")\n",
    "            return False, duplicate_count\n",
    "        else:\n",
    "            logger.info(\"Duplicate check passed\")\n",
    "            return True, 0\n",
    "    \n",
    "    def check_data_ranges(self, df: DataFrame, range_checks: Dict[str, Tuple]) -> Tuple[bool, Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Check if values in columns fall within expected ranges.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): DataFrame to check\n",
    "            range_checks (Dict[str, Tuple]): Dictionary mapping column names to (min, max) tuples\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[bool, Dict[str, int]]: (passed/failed, dict of out-of-range counts by column)\n",
    "        \"\"\"\n",
    "        logger.info(f\"Checking data ranges for columns: {list(range_checks.keys())}\")\n",
    "        \n",
    "        out_of_range_counts = {}\n",
    "        \n",
    "        for column, (min_val, max_val) in range_checks.items():\n",
    "            if column in df.columns:\n",
    "                # Count values outside the expected range\n",
    "                out_of_range_count = df.filter(\n",
    "                    (col(column) < min_val) | (col(column) > max_val)\n",
    "                ).count()\n",
    "                \n",
    "                out_of_range_counts[column] = out_of_range_count\n",
    "            else:\n",
    "                logger.warning(f\"Column {column} not found in DataFrame\")\n",
    "                out_of_range_counts[column] = \"Column not found\"\n",
    "        \n",
    "        # Check if any column has out-of-range values\n",
    "        has_out_of_range = any(isinstance(count, int) and count > 0 for count in out_of_range_counts.values())\n",
    "        \n",
    "        if has_out_of_range:\n",
    "            logger.warning(f\"Range check failed. Out-of-range counts: {out_of_range_counts}\")\n",
    "            return False, out_of_range_counts\n",
    "        else:\n",
    "            logger.info(\"Range check passed\")\n",
    "            return True, out_of_range_counts\n",
    "    \n",
    "    def check_referential_integrity(self, df: DataFrame, ref_df: DataFrame, \n",
    "                                   fk_column: str, pk_column: str) -> Tuple[bool, int]:\n",
    "        \"\"\"\n",
    "        Check referential integrity between two DataFrames.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): DataFrame with foreign key\n",
    "            ref_df (DataFrame): Reference DataFrame with primary key\n",
    "            fk_column (str): Foreign key column in df\n",
    "            pk_column (str): Primary key column in ref_df\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[bool, int]: (passed/failed, count of orphaned records)\n",
    "        \"\"\"\n",
    "        logger.info(f\"Checking referential integrity: {fk_column} -> {pk_column}\")\n",
    "        \n",
    "        # Get distinct foreign keys\n",
    "        fk_values = df.select(fk_column).distinct()\n",
    "        \n",
    "        # Get distinct primary keys\n",
    "        pk_values = ref_df.select(pk_column).distinct()\n",
    "        \n",
    "        # Find orphaned records (foreign keys without matching primary keys)\n",
    "        orphaned_records = fk_values.join(\n",
    "            pk_values,\n",
    "            fk_values[fk_column] == pk_values[pk_column],\n",
    "            \"left_anti\"\n",
    "        )\n",
    "        \n",
    "        orphaned_count = orphaned_records.count()\n",
    "        \n",
    "        if orphaned_count > 0:\n",
    "            logger.warning(f\"Referential integrity check failed. Found {orphaned_count} orphaned records\")\n",
    "            return False, orphaned_count\n",
    "        else:\n",
    "            logger.info(\"Referential integrity check passed\")\n",
    "            return True, 0\n",
    "    \n",
    "    def run_all_checks(self, df: DataFrame, check_config: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Run all configured data quality checks on a DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): DataFrame to check\n",
    "            check_config (Dict): Configuration for checks to run\n",
    "            \n",
    "        Returns:\n",
    "            Dict: Results of all checks\n",
    "        \"\"\"\n",
    "        logger.info(f\"Running all data quality checks for table: {check_config.get('table_name', 'unknown')}\")\n",
    "        \n",
    "        results = {\n",
    "            \"table_name\": check_config.get(\"table_name\", \"unknown\"),\n",
    "            \"record_count\": df.count(),\n",
    "            \"checks\": {}\n",
    "        }\n",
    "        \n",
    "        # Run null checks\n",
    "        if \"required_columns\" in check_config:\n",
    "            null_check_passed, null_counts = self.check_nulls(df, check_config[\"required_columns\"])\n",
    "            results[\"checks\"][\"null_check\"] = {\n",
    "                \"passed\": null_check_passed,\n",
    "                \"details\": null_counts\n",
    "            }\n",
    "        \n",
    "        # Run duplicate checks\n",
    "        if \"key_columns\" in check_config:\n",
    "            dup_check_passed, dup_count = self.check_duplicates(df, check_config[\"key_columns\"])\n",
    "            results[\"checks\"][\"duplicate_check\"] = {\n",
    "                \"passed\": dup_check_passed,\n",
    "                \"details\": {\"duplicate_count\": dup_count}\n",
    "            }\n",
    "        \n",
    "        # Run range checks\n",
    "        if \"range_checks\" in check_config:\n",
    "            range_check_passed, range_counts = self.check_data_ranges(df, check_config[\"range_checks\"])\n",
    "            results[\"checks\"][\"range_check\"] = {\n",
    "                \"passed\": range_check_passed,\n",
    "                \"details\": range_counts\n",
    "            }\n",
    "        \n",
    "        # Overall check result\n",
    "        results[\"overall_passed\"] = all(check[\"passed\"] for check in results[\"checks\"].values())\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c99d2dff-3234-4d16-82c9-c798d246ec8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Explanation: The DataQualityChecker class implements comprehensive data quality validation, which is crucial for financial data processing. It provides several key methods:\n",
    "\n",
    "check_nulls(): Validates that required columns don’t contain null values:\n",
    "- Counts null values in each specified column\n",
    "- Returns both a pass/fail status and detailed counts\n",
    "- Handles missing columns gracefully\n",
    "check_duplicates(): Ensures uniqueness of key columns:\n",
    "- Compares total row count with distinct row count\n",
    "- Identifies duplicate records based on specified key columns\n",
    "- Returns both status and count of duplicates\n",
    "check_data_ranges(): Verifies that values fall within expected ranges:\n",
    "- Takes a dictionary mapping columns to min/max value ranges\n",
    "- Counts out-of-range values for each column\n",
    "- Returns detailed results by column\n",
    "check_referential_integrity(): Validates foreign key relationships:\n",
    "- Ensures all foreign keys have corresponding primary keys\n",
    "- Uses anti-join to find orphaned records\n",
    "- Returns count of integrity violations\n",
    "run_all_checks(): Orchestrates multiple quality checks:\n",
    "- Runs configured checks based on provided configuration\n",
    "- Aggregates results into a structured report\n",
    "- Determines overall pass/fail status\n",
    "This class is essential for ensuring data reliability in a banking environment where data accuracy is critical for regulatory compliance and business operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95153259-8409-4a9b-98c5-f837bec8a80b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "5. Data Loading to Redshift\n",
    "# src/loading/redshift_loader.py\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import logging\n",
    "from typing import Dict, Optional\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class RedshiftLoader:\n",
    "    \"\"\"Class to handle data loading to AWS Redshift.\"\"\"\n",
    "    \n",
    "    def __init__(self, spark: SparkSession, jdbc_url: str, username: str, password: str):\n",
    "        \"\"\"\n",
    "        Initialize Redshift loader.\n",
    "        \n",
    "        Args:\n",
    "            spark (SparkSession): Spark session\n",
    "            jdbc_url (str): JDBC URL for Redshift\n",
    "            username (str): Redshift username\n",
    "            password (str): Redshift password\n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "        self.jdbc_url = jdbc_url\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        \n",
    "    def write_to_redshift(self, df: DataFrame, table_name: str, write_mode: str = \"append\",\n",
    "                         preactions: Optional[str] = None, postactions: Optional[str] = None) -> None:\n",
    "        \"\"\"\n",
    "        Write DataFrame to Redshift table.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): DataFrame to write\n",
    "            table_name (str): Target table name\n",
    "            write_mode (str): Write mode (append, overwrite, error)\n",
    "            preactions (Optional[str]): SQL to execute before writing\n",
    "            postactions (Optional[str]): SQL to execute after writing\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Writing data to Redshift table: {table_name}\")\n",
    "            \n",
    "            # Connection properties\n",
    "            connection_properties = {\n",
    "                \"url\": self.jdbc_url,\n",
    "                \"user\": self.username,\n",
    "                \"password\": self.password,\n",
    "                \"driver\": \"com.amazon.redshift.jdbc42.Driver\",\n",
    "                \"dbtable\": table_name\n",
    "            }\n",
    "            \n",
    "            # Add pre/post actions if provided\n",
    "            if preactions:\n",
    "                connection_properties[\"preactions\"] = preactions\n",
    "            if postactions:\n",
    "                connection_properties[\"postactions\"] = postactions\n",
    "                \n",
    "            # Write to Redshift\n",
    "            df.write.format(\"jdbc\") \\\n",
    "                .mode(write_mode) \\\n",
    "                .options(**connection_properties) \\\n",
    "                .save()\n",
    "                \n",
    "            logger.info(f\"Successfully wrote data to Redshift table: {table_name}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error writing to Redshift table {table_name}: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def load_with_staging(self, df: DataFrame, target_table: str, staging_table: str = None,\n",
    "                         key_columns: list = None) -> None:\n",
    "        \"\"\"\n",
    "        Load data to Redshift using a staging table for better performance.\n",
    "        \n",
    "        Args:\n",
    "            df (DataFrame): DataFrame to write\n",
    "            target_table (str): Target table name\n",
    "            staging_table (str): Staging table name (defaults to target_table + '_staging')\n",
    "            key_columns (list): Primary key columns for merging\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not staging_table:\n",
    "                staging_table = f\"{target_table}_staging\"\n",
    "                \n",
    "            logger.info(f\"Loading data to Redshift using staging table: {staging_table}\")\n",
    "            \n",
    "            # Create staging table with the same structure as target table\n",
    "            create_staging_sql = f\"\"\"\n",
    "            DROP TABLE IF EXISTS {staging_table};\n",
    "            CREATE TABLE {staging_table} (LIKE {target_table});\n",
    "            \"\"\"\n",
    "            \n",
    "            # Write data to staging table\n",
    "            self.write_to_redshift(df, staging_table, \"overwrite\", preactions=create_staging_sql)\n",
    "            \n",
    "            # Merge data from staging to target table\n",
    "            if key_columns and len(key_columns) > 0:\n",
    "                # For upsert operation\n",
    "                key_conditions = \" AND \".join([f\"target.{col} = source.{col}\" for col in key_columns])\n",
    "                non_key_columns = [col for col in df.columns if col not in key_columns]\n",
    "                update_statements = \", \".join([f\"target.{col} = source.{col}\" for col in non_key_columns])\n",
    "                insert_columns = \", \".join(df.columns)\n",
    "                insert_values = \", \".join([f\"source.{col}\" for col in df.columns])\n",
    "                \n",
    "                merge_sql = f\"\"\"\n",
    "                BEGIN TRANSACTION;\n",
    "                \n",
    "                -- Update existing records\n",
    "                UPDATE {target_table} AS target\n",
    "                SET {update_statements}\n",
    "                FROM {staging_table} AS source\n",
    "                WHERE {key_conditions};\n",
    "                \n",
    "                -- Insert new records\n",
    "                INSERT INTO {target_table} ({insert_columns})\n",
    "                SELECT {insert_values}\n",
    "                FROM {staging_table} AS source\n",
    "                LEFT JOIN {target_table} AS target\n",
    "                ON {key_conditions}\n",
    "                WHERE target.{key_columns[0]} IS NULL;\n",
    "                \n",
    "                -- Clean up staging table\n",
    "                DROP TABLE IF EXISTS {staging_table};\n",
    "                \n",
    "                END TRANSACTION;\n",
    "                \"\"\"\n",
    "                \n",
    "                # Execute the merge SQL\n",
    "                self.execute_sql(merge_sql)\n",
    "            else:\n",
    "                # For full load/truncate and load\n",
    "                truncate_and_load_sql = f\"\"\"\n",
    "                BEGIN TRANSACTION;\n",
    "                \n",
    "                TRUNCATE TABLE {target_table};\n",
    "                \n",
    "                INSERT INTO {target_table}\n",
    "                SELECT * FROM {staging_table};\n",
    "                \n",
    "                DROP TABLE IF EXISTS {staging_table};\n",
    "                \n",
    "                END TRANSACTION;\n",
    "                \"\"\"\n",
    "                \n",
    "                # Execute the truncate and load SQL\n",
    "                self.execute_sql(truncate_and_load_sql)\n",
    "                \n",
    "            logger.info(f\"Successfully loaded data to Redshift table: {target_table}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading data to Redshift table {target_table}: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "    def execute_sql(self, sql: str) -> None:\n",
    "        \"\"\"\n",
    "        Execute SQL statement in Redshift.\n",
    "        \n",
    "        Args:\n",
    "            sql (str): SQL statement to execute\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Executing SQL in Redshift\")\n",
    "            \n",
    "            # Create a temporary DataFrame to execute SQL\n",
    "            temp_df = self.spark.createDataFrame([(\"dummy\",)], [\"dummy\"])\n",
    "            \n",
    "            # Execute SQL as a postaction\n",
    "            connection_properties = {\n",
    "                \"url\": self.jdbc_url,\n",
    "                \"user\": self.username,\n",
    "                \"password\": self.password,\n",
    "                \"driver\": \"com.amazon.redshift.jdbc42.Driver\",\n",
    "                \"dbtable\": \"(SELECT 1) AS dummy\",\n",
    "                \"postactions\": sql\n",
    "            }\n",
    "            \n",
    "            temp_df.write.format(\"jdbc\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .options(**connection_properties) \\\n",
    "                .save()\n",
    "                \n",
    "            logger.info(\"Successfully executed SQL in Redshift\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error executing SQL in Redshift: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d31f4c1-329b-4ebc-bdad-4a968c0c2aad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Explanation: The RedshiftLoader class handles loading processed data into AWS Redshift, a popular data warehouse solution. It provides several key methods:\n",
    "\n",
    "write_to_redshift(): Basic method to write data directly to a Redshift table:\n",
    "- Sets up JDBC connection properties\n",
    "- Supports different write modes (append, overwrite)\n",
    "- Allows pre and post SQL actions\n",
    "- Includes comprehensive error handling\n",
    "load_with_staging(): Implements a more sophisticated loading pattern using staging tables:\n",
    "- Creates a temporary staging table\n",
    "- Loads data to the staging table\n",
    "- Performs either an upsert operation (for incremental loads) or a full replacement\n",
    "- Uses transactions to ensure atomicity\n",
    "- Cleans up staging tables after successful load\n",
    "execute_sql(): Utility method to execute arbitrary SQL in Redshift:\n",
    "- Creates a dummy DataFrame as a vehicle for SQL execution\n",
    "- Uses Spark’s JDBC connector to run the SQL\n",
    "- Provides error handling and logging\n",
    "This class implements best practices for loading data to Redshift, including:\n",
    "\n",
    "Using staging tables to minimize impact on production tables\n",
    "Supporting both full and incremental loading patterns\n",
    "Using transactions to ensure data consistency\n",
    "Providing detailed logging for troubleshooting\n",
    "These patterns are particularly important in banking applications where data integrity is critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e34f6c04-3d05-4735-ae85-7374f495692c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "6. Main Pipeline Orchestration\n",
    "# src/orchestration/main_pipeline.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Import project modules\n",
    "from src.utils.spark_session import create_spark_session\n",
    "from src.ingestion.s3_connector import S3Connector\n",
    "from src.ingestion.rds_connector import RDSConnector\n",
    "from src.transformation.customer_transform import CustomerTransformer\n",
    "from src.transformation.transaction_transform import TransactionTransformer\n",
    "from src.transformation.account_transform import AccountTransformer\n",
    "from src.transformation.data_quality import DataQualityChecker\n",
    "from src.loading.redshift_loader import RedshiftLoader\n",
    "from src.loading.s3_loader import S3Loader\n",
    "from src.utils.logging_utils import setup_logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BankingETLPipeline:\n",
    "    \"\"\"Main class to orchestrate the banking ETL pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: str):\n",
    "        \"\"\"\n",
    "        Initialize the ETL pipeline.\n",
    "        \n",
    "        Args:\n",
    "            config_path (str): Path to the configuration file\n",
    "        \"\"\"\n",
    "        # Set up logging\n",
    "        setup_logging()\n",
    "        \n",
    "        # Load configuration\n",
    "        logger.info(f\"Loading configuration from {config_path}\")\n",
    "        with open(config_path, 'r') as config_file:\n",
    "            self.config = json.load(config_file)\n",
    "        \n",
    "        # Initialize Spark session\n",
    "        logger.info(\"Initializing Spark session\")\n",
    "        self.spark = create_spark_session(app_name=self.config.get(\"app_name\", \"Banking ETL Pipeline\"))\n",
    "        \n",
    "        # Initialize components\n",
    "        self._init_components()\n",
    "        \n",
    "        # Set execution date\n",
    "        self.execution_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        \n",
    "    def _init_components(self):\n",
    "        \"\"\"Initialize pipeline components based on configuration.\"\"\"\n",
    "        logger.info(\"Initializing pipeline components\")\n",
    "        \n",
    "        # Initialize data connectors\n",
    "        s3_config = self.config.get(\"s3\", {})\n",
    "        self.s3_connector = S3Connector(\n",
    "            self.spark, \n",
    "            s3_config.get(\"bucket_name\", \"banking-data-lake\")\n",
    "        )\n",
    "        \n",
    "        rds_config = self.config.get(\"rds\", {})\n",
    "        self.rds_connector = RDSConnector(\n",
    "            self.spark,\n",
    "            rds_config.get(\"jdbc_url\"),\n",
    "            rds_config.get(\"username\"),\n",
    "            rds_config.get(\"password\")\n",
    "        )\n",
    "        \n",
    "        # Initialize transformers\n",
    "        self.customer_transformer = CustomerTransformer(self.spark)\n",
    "        self.transaction_transformer = TransactionTransformer(self.spark)\n",
    "        self.account_transformer = AccountTransformer(self.spark)\n",
    "        \n",
    "        # Initialize data quality checker\n",
    "        self.data_quality_checker = DataQualityChecker(self.spark)\n",
    "        \n",
    "        # Initialize data loaders\n",
    "        redshift_config = self.config.get(\"redshift\", {})\n",
    "        self.redshift_loader = RedshiftLoader(\n",
    "            self.spark,\n",
    "            redshift_config.get(\"jdbc_url\"),\n",
    "            redshift_config.get(\"username\"),\n",
    "            redshift_config.get(\"password\")\n",
    "        )\n",
    "        \n",
    "        self.s3_loader = S3Loader(\n",
    "            self.spark,\n",
    "            s3_config.get(\"bucket_name\", \"banking-data-lake\")\n",
    "        )\n",
    "        \n",
    "    def run_customer_pipeline(self):\n",
    "        \"\"\"Run the customer data pipeline.\"\"\"\n",
    "        logger.info(\"Running customer data pipeline\")\n",
    "        \n",
    "        try:\n",
    "            # Extract customer data\n",
    "            customer_config = self.config.get(\"pipelines\", {}).get(\"customer\", {})\n",
    "            source_type = customer_config.get(\"source_type\")\n",
    "            \n",
    "            if source_type == \"s3\":\n",
    "                raw_customers = self.s3_connector.read_csv(\n",
    "                    customer_config.get(\"source_path\")\n",
    "                )\n",
    "            elif source_type == \"rds\":\n",
    "                raw_customers = self.rds_connector.read_table(\n",
    "                    customer_config.get(\"source_table\")\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported source type: {source_type}\")\n",
    "            \n",
    "            # Transform customer data\n",
    "            cleaned_customers = self.customer_transformer.clean_customer_data(raw_customers)\n",
    "            enriched_customers = self.customer_transformer.enrich_customer_data(cleaned_customers)\n",
    "            \n",
    "            # Run data quality checks\n",
    "            quality_results = self.data_quality_checker.run_all_checks(\n",
    "                enriched_customers,\n",
    "                customer_config.get(\"data_quality\", {})\n",
    "            )\n",
    "            \n",
    "            if not quality_results.get(\"overall_passed\", False):\n",
    "                logger.warning(\"Data quality checks failed for customer data\")\n",
    "                # Depending on configuration, we might still proceed\n",
    "                if customer_config.get(\"fail_on_quality_check\", True):\n",
    "                    raise Exception(\"Data quality checks failed for customer data\")\n",
    "            \n",
    "            # Load customer data\n",
    "            target_type = customer_config.get(\"target_type\")\n",
    "            \n",
    "            if target_type == \"redshift\":\n",
    "                self.redshift_loader.load_with_staging(\n",
    "                    enriched_customers,\n",
    "                    customer_config.get(\"target_table\"),\n",
    "                    key_columns=customer_config.get(\"key_columns\", [\"customer_id\"])\n",
    "                )\n",
    "            elif target_type == \"s3\":\n",
    "                self.s3_loader.write_delta(\n",
    "                    enriched_customers,\n",
    "                    customer_config.get(\"target_path\"),\n",
    "                    mode=customer_config.get(\"write_mode\", \"overwrite\"),\n",
    "                    partition_cols=customer_config.get(\"partition_cols\", [])\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported target type: {target_type}\")\n",
    "            \n",
    "            logger.info(\"Customer data pipeline completed successfully\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in customer data pipeline: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def run_transaction_pipeline(self):\n",
    "        \"\"\"Run the transaction data pipeline.\"\"\"\n",
    "        logger.info(\"Running transaction data pipeline\")\n",
    "        \n",
    "        try:\n",
    "            # Extract transaction data\n",
    "            transaction_config = self.config.get(\"pipelines\", {}).get(\"transaction\", {})\n",
    "            source_type = transaction_config.get(\"source_type\")\n",
    "            \n",
    "            if source_type == \"s3\":\n",
    "                raw_transactions = self.s3_connector.read_csv(\n",
    "                    transaction_config.get(\"source_path\")\n",
    "                )\n",
    "            elif source_type == \"rds\":\n",
    "                raw_transactions = self.rds_connector.read_table(\n",
    "                    transaction_config.get(\"source_table\")\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported source type: {source_type}\")\n",
    "            \n",
    "            # Transform transaction data\n",
    "            cleaned_transactions = self.transaction_transformer.clean_transaction_data(raw_transactions)\n",
    "            enriched_transactions = self.transaction_transformer.enrich_transaction_data(cleaned_transactions)\n",
    "            transactions_with_metrics = self.transaction_transformer.calculate_transaction_metrics(enriched_transactions)\n",
    "            final_transactions = self.transaction_transformer.detect_anomalies(transactions_with_metrics)\n",
    "            \n",
    "            # Run data quality checks\n",
    "            quality_results = self.data_quality_checker.run_all_checks(\n",
    "                final_transactions,\n",
    "                transaction_config.get(\"data_quality\", {})\n",
    "            )\n",
    "            \n",
    "            if not quality_results.get(\"overall_passed\", False):\n",
    "                logger.warning(\"Data quality checks failed for transaction data\")\n",
    "                # Depending on configuration, we might still proceed\n",
    "                if transaction_config.get(\"fail_on_quality_check\", True):\n",
    "                    raise Exception(\"Data quality checks failed for transaction data\")\n",
    "            \n",
    "            # Load transaction data\n",
    "            target_type = transaction_config.get(\"target_type\")\n",
    "            \n",
    "            if target_type == \"redshift\":\n",
    "                self.redshift_loader.load_with_staging(\n",
    "                    final_transactions,\n",
    "                    transaction_config.get(\"target_table\"),\n",
    "                    key_columns=transaction_config.get(\"key_columns\", [\"transaction_id\"])\n",
    "                )\n",
    "            elif target_type == \"s3\":\n",
    "                self.s3_loader.write_delta(\n",
    "                    final_transactions,\n",
    "                    transaction_config.get(\"target_path\"),\n",
    "                    mode=transaction_config.get(\"write_mode\", \"append\"),\n",
    "                    partition_cols=transaction_config.get(\"partition_cols\", [\"transaction_year\", \"transaction_month\"])\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported target type: {target_type}\")\n",
    "            \n",
    "            logger.info(\"Transaction data pipeline completed successfully\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in transaction data pipeline: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def run_account_pipeline(self):\n",
    "        \"\"\"Run the account data pipeline.\"\"\"\n",
    "        logger.info(\"Running account data pipeline\")\n",
    "        \n",
    "        try:\n",
    "            # Extract account data\n",
    "            account_config = self.config.get(\"pipelines\", {}).get(\"account\", {})\n",
    "            source_type = account_config.get(\"source_type\")\n",
    "            \n",
    "            if source_type == \"s3\":\n",
    "                raw_accounts = self.s3_connector.read_csv(\n",
    "                    account_config.get(\"source_path\")\n",
    "                )\n",
    "            elif source_type == \"rds\":\n",
    "                raw_accounts = self.rds_connector.read_table(\n",
    "                    account_config.get(\"source_table\")\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported source type: {source_type}\")\n",
    "            \n",
    "            # Transform account data\n",
    "            cleaned_accounts = self.account_transformer.clean_account_data(raw_accounts)\n",
    "            enriched_accounts = self.account_transformer.enrich_account_data(cleaned_accounts)\n",
    "            \n",
    "            # Run data quality checks\n",
    "            quality_results = self.data_quality_checker.run_all_checks(\n",
    "                enriched_accounts,\n",
    "                account_config.get(\"data_quality\", {})\n",
    "            )\n",
    "            \n",
    "            if not quality_results.get(\"overall_passed\", False):\n",
    "                logger.warning(\"Data quality checks failed for account data\")\n",
    "                # Depending on configuration, we might still proceed\n",
    "                if account_config.get(\"fail_on_quality_check\", True):\n",
    "                    raise Exception(\"Data quality checks failed for account data\")\n",
    "            \n",
    "            # Load account data\n",
    "            target_type = account_config.get(\"target_type\")\n",
    "            \n",
    "            if target_type == \"redshift\":\n",
    "                self.redshift_loader.load_with_staging(\n",
    "                    enriched_accounts,\n",
    "                    account_config.get(\"target_table\"),\n",
    "                    key_columns=account_config.get(\"key_columns\", [\"account_id\"])\n",
    "                )\n",
    "            elif target_type == \"s3\":\n",
    "                self.s3_loader.write_delta(\n",
    "                    enriched_accounts,\n",
    "                    account_config.get(\"target_path\"),\n",
    "                    mode=account_config.get(\"write_mode\", \"overwrite\"),\n",
    "                    partition_cols=account_config.get(\"partition_cols\", [])\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported target type: {target_type}\")\n",
    "            \n",
    "            logger.info(\"Account data pipeline completed successfully\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in account data pipeline: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"Run the complete ETL pipeline.\"\"\"\n",
    "        logger.info(\"Starting the banking ETL pipeline\")\n",
    "        \n",
    "        try:\n",
    "            # Run individual pipelines based on configuration\n",
    "            pipelines_to_run = self.config.get(\"pipelines_to_run\", [])\n",
    "            \n",
    "            if \"customer\" in pipelines_to_run:\n",
    "                self.run_customer_pipeline()\n",
    "            \n",
    "            if \"account\" in pipelines_to_run:\n",
    "                self.run_account_pipeline()\n",
    "            \n",
    "            if \"transaction\" in pipelines_to_run:\n",
    "                self.run_transaction_pipeline()\n",
    "            \n",
    "            logger.info(\"Banking ETL pipeline completed successfully\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in banking ETL pipeline: {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            # Clean up resources\n",
    "            logger.info(\"Cleaning up resources\")\n",
    "            self.spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Get configuration path from environment variable or use default\n",
    "    config_path = os.environ.get(\"ETL_CONFIG_PATH\", \"config/config.json\")\n",
    "    \n",
    "    # Run the pipeline\n",
    "    pipeline = BankingETLPipeline(config_path)\n",
    "    pipeline.run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c091cc0-26f4-4045-948b-d91bb3c5152e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Explanation: The BankingETLPipeline class serves as the main orchestrator for the entire ETL process, tying together all the components we’ve built. It provides:\n",
    "\n",
    "Initialization and Configuration:\n",
    "- Loads configuration from a JSON file\n",
    "- Sets up logging\n",
    "- Initializes a Spark session\n",
    "- Creates instances of all required components (connectors, transformers, loaders)\n",
    "Pipeline Execution Methods:\n",
    "- run_customer_pipeline(): Processes customer data through extraction, transformation, quality checks, and loading\n",
    "- run_transaction_pipeline(): Handles transaction data with specialized transformations and anomaly detection\n",
    "- run_account_pipeline(): Processes account data with appropriate transformations\n",
    "- run_pipeline(): Orchestrates the execution of all sub-pipelines based on configuration\n",
    "Error Handling and Resource Management:\n",
    "- Implements comprehensive try/except blocks for error handling\n",
    "- Ensures proper cleanup of resources in the finally block\n",
    "- Provides detailed logging throughout the process\n",
    "The class follows a modular design pattern, making it easy to:\n",
    "\n",
    "Add new data pipelines\n",
    "Modify existing pipelines\n",
    "Configure which pipelines to run\n",
    "Handle different source and target types\n",
    "This orchestration layer is crucial for managing the complexity of a multi-domain ETL pipeline in a banking environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "254d7fb5-43ab-48fa-974d-7dd22d1a0fee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Sample Configuration File\n",
    "{\n",
    "  \"app_name\": \"Banking ETL Pipeline\",\n",
    "  \"environment\": \"production\",\n",
    "  \"pipelines_to_run\": [\"customer\", \"account\", \"transaction\"],\n",
    "  \n",
    "  \"s3\": {\n",
    "    \"bucket_name\": \"banking-data-lake\",\n",
    "    \"region\": \"us-east-1\"\n",
    "  },\n",
    "  \n",
    "  \"rds\": {\n",
    "    \"jdbc_url\": \"jdbc:mysql://banking-db.cluster-xyz.us-east-1.rds.amazonaws.com:3306/banking\",\n",
    "    \"username\": \"${RDS_USERNAME}\",\n",
    "    \"password\": \"${RDS_PASSWORD}\"\n",
    "  },\n",
    "  \n",
    "  \"redshift\": {\n",
    "    \"jdbc_url\": \"jdbc:redshift://banking-warehouse.xyz.us-east-1.redshift.amazonaws.com:5439/banking\",\n",
    "    \"username\": \"${REDSHIFT_USERNAME}\",\n",
    "    \"password\": \"${REDSHIFT_PASSWORD}\"\n",
    "  },\n",
    "  \n",
    "  \"pipelines\": {\n",
    "    \"customer\": {\n",
    "      \"source_type\": \"s3\",\n",
    "      \"source_path\": \"raw/customers/\",\n",
    "      \"target_type\": \"redshift\",\n",
    "      \"target_table\": \"dim_customer\",\n",
    "      \"key_columns\": [\"customer_id\"],\n",
    "      \"fail_on_quality_check\": true,\n",
    "      \"data_quality\": {\n",
    "        \"table_name\": \"dim_customer\",\n",
    "        \"required_columns\": [\"customer_id\", \"first_name\", \"last_name\", \"email\"],\n",
    "        \"key_columns\": [\"customer_id\"],\n",
    "        \"range_checks\": {\n",
    "          \"credit_score\": [300, 850]\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \n",
    "    \"account\": {\n",
    "      \"source_type\": \"rds\",\n",
    "      \"source_table\": \"accounts\",\n",
    "      \"target_type\": \"redshift\",\n",
    "      \"target_table\": \"dim_account\",\n",
    "      \"key_columns\": [\"account_id\"],\n",
    "      \"fail_on_quality_check\": true,\n",
    "      \"data_quality\": {\n",
    "        \"table_name\": \"dim_account\",\n",
    "        \"required_columns\": [\"account_id\", \"customer_id\", \"account_type\", \"open_date\"],\n",
    "        \"key_columns\": [\"account_id\"],\n",
    "        \"range_checks\": {\n",
    "          \"balance\": [0, 10000000],\n",
    "          \"interest_rate\": [0, 30]\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \n",
    "    \"transaction\": {\n",
    "      \"source_type\": \"s3\",\n",
    "      \"source_path\": \"raw/transactions/\",\n",
    "      \"target_type\": \"s3\",\n",
    "      \"target_path\": \"processed/transactions/\",\n",
    "      \"write_mode\": \"append\",\n",
    "      \"partition_cols\": [\"transaction_year\", \"transaction_month\"],\n",
    "      \"fail_on_quality_check\": false,\n",
    "      \"data_quality\": {\n",
    "        \"table_name\": \"fact_transaction\",\n",
    "        \"required_columns\": [\"transaction_id\", \"account_id\", \"transaction_date\", \"amount\"],\n",
    "        \"key_columns\": [\"transaction_id\"],\n",
    "        \"range_checks\": {\n",
    "          \"amount\": [0, 1000000]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a4ebd3c-a345-4823-9498-a913a71efc77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Explanation: This configuration file provides a centralized way to control the ETL pipeline’s behavior without changing code. Key sections include:\n",
    "\n",
    "General Settings:\n",
    "- Application name and environment\n",
    "- List of pipelines to run\n",
    "Connection Information:\n",
    "- S3 bucket details for data lake storage\n",
    "- RDS connection parameters for relational database access\n",
    "- Redshift connection parameters for data warehouse access\n",
    "- Note the use of environment variable placeholders for sensitive credentials\n",
    "Pipeline-Specific Configurations:\n",
    "- Customer Pipeline: Reads from S3, loads to Redshift, with specific data quality checks\n",
    "- Account Pipeline: Reads from RDS, loads to Redshift, with account-specific validations\n",
    "- Transaction Pipeline: Reads from S3, writes to S3 Delta Lake with time-based partitioning\n",
    "Each pipeline configuration includes:\n",
    "\n",
    "Source and target specifications\n",
    "Data quality requirements\n",
    "Failure handling policies\n",
    "Key columns for deduplication and merging\n",
    "This configuration-driven approach allows for flexible deployment across environments and easy modification of pipeline behavior without code changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6438291-41d2-421f-9458-452275b7b865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Generating Sample Data\n",
    "# Create a notebook to generate sample data:\n",
    "\n",
    "# Databricks notebook: Generate Sample Data\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import uuid\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define schemas\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"date_of_birth\", DateType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"phone_number\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"zip_code\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"customer_since\", DateType(), True),\n",
    "    StructField(\"credit_score\", IntegerType(), True),\n",
    "    StructField(\"risk_segment\", StringType(), True)\n",
    "])\n",
    "\n",
    "account_schema = StructType([\n",
    "    StructField(\"account_id\", StringType(), False),\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"account_type\", StringType(), True),\n",
    "    StructField(\"account_status\", StringType(), True),\n",
    "    StructField(\"open_date\", DateType(), True),\n",
    "    StructField(\"close_date\", DateType(), True),\n",
    "    StructField(\"currency\", StringType(), True),\n",
    "    StructField(\"branch_id\", StringType(), True),\n",
    "    StructField(\"interest_rate\", FloatType(), True),\n",
    "    StructField(\"balance\", DecimalType(18, 2), True),\n",
    "    StructField(\"last_activity_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), False),\n",
    "    StructField(\"account_id\", StringType(), False),\n",
    "    StructField(\"transaction_date\", TimestampType(), False),\n",
    "    StructField(\"transaction_type\", StringType(), True),\n",
    "    StructField(\"amount\", DecimalType(18, 2), True),\n",
    "    StructField(\"currency\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"merchant_name\", StringType(), True),\n",
    "    StructField(\"merchant_category\", StringType(), True),\n",
    "    StructField(\"transaction_status\", StringType(), True),\n",
    "    StructField(\"channel\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"is_international\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "# Generate customer data\n",
    "def generate_customers(num_customers=1000):\n",
    "    first_names = [\"James\", \"Mary\", \"John\", \"Patricia\", \"Robert\", \"Jennifer\", \"Michael\", \"Linda\", \"William\", \"Elizabeth\"]\n",
    "    last_names = [\"Smith\", \"Johnson\", \"Williams\", \"Jones\", \"Brown\", \"Davis\", \"Miller\", \"Wilson\", \"Moore\", \"Taylor\"]\n",
    "    states = [\"CA\", \"NY\", \"TX\", \"FL\", \"IL\", \"PA\", \"OH\", \"GA\", \"NC\", \"MI\"]\n",
    "    cities = [\"Los Angeles\", \"New York\", \"Houston\", \"Miami\", \"Chicago\", \"Philadelphia\", \"Columbus\", \"Atlanta\", \"Charlotte\", \"Detroit\"]\n",
    "    risk_segments = [\"Low\", \"Medium\", \"High\"]\n",
    "    \n",
    "    customers = []\n",
    "    \n",
    "    for i in range(num_customers):\n",
    "        customer_id = f\"CUST{i:06d}\"\n",
    "        first_name = random.choice(first_names)\n",
    "        last_name = random.choice(last_names)\n",
    "        \n",
    "        # Generate date of birth (21-80 years old)\n",
    "        years_ago = random.randint(21, 80)\n",
    "        dob = datetime.now() - timedelta(days=365 * years_ago)\n",
    "        \n",
    "        # Generate customer since date (0-10 years ago)\n",
    "        years_customer = random.randint(0, 10)\n",
    "        customer_since = datetime.now() - timedelta(days=365 * years_customer)\n",
    "        \n",
    "        state = random.choice(states)\n",
    "        city = random.choice(cities)\n",
    "        \n",
    "        customers.append((\n",
    "            customer_id,\n",
    "            first_name,\n",
    "            last_name,\n",
    "            dob.date(),\n",
    "            f\"{first_name.lower()}.{last_name.lower()}@example.com\",\n",
    "            f\"555-{random.randint(100, 999)}-{random.randint(1000, 9999)}\",\n",
    "            f\"{random.randint(100, 9999)} Main St\",\n",
    "            city,\n",
    "            state,\n",
    "            f\"{random.randint(10000, 99999)}\",\n",
    "            \"USA\",\n",
    "            customer_since.date(),\n",
    "            random.randint(300, 850),\n",
    "            random.choice(risk_segments)\n",
    "        ))\n",
    "    \n",
    "    return spark.createDataFrame(customers, customer_schema)\n",
    "\n",
    "import decimal\n",
    "\n",
    "def generate_accounts(customers_df, num_accounts=1500):\n",
    "    account_types = [\"checking\", \"savings\", \"investment\"]\n",
    "    account_statuses = [\"active\", \"closed\", \"suspended\"]\n",
    "    currencies = [\"USD\", \"EUR\", \"GBP\"]\n",
    "    \n",
    "    accounts = []\n",
    "    customer_ids = [row.customer_id for row in customers_df.select(\"customer_id\").collect()]\n",
    "    \n",
    "    for i in range(num_accounts):\n",
    "        account_id = f\"ACC{i:08d}\"\n",
    "        customer_id = random.choice(customer_ids)\n",
    "        account_type = random.choice(account_types)\n",
    "        account_status = random.choice(account_statuses)\n",
    "        years_ago = random.randint(0, 5)\n",
    "        open_date = datetime.now() - timedelta(days=365 * years_ago)\n",
    "        close_date = None\n",
    "        if account_status == \"closed\":\n",
    "            days_ago = random.randint(0, 365)\n",
    "            close_date = datetime.now() - timedelta(days=days_ago)\n",
    "        days_ago = random.randint(0, 30)\n",
    "        last_activity_date = datetime.now() - timedelta(days=days_ago)\n",
    "        \n",
    "        # Fix: Convert balance to decimal.Decimal\n",
    "        balance = decimal.Decimal(str(round(random.uniform(0, 100000), 2)))\n",
    "        \n",
    "        accounts.append((\n",
    "            account_id,\n",
    "            customer_id,\n",
    "            account_type,\n",
    "            account_status,\n",
    "            open_date.date(),\n",
    "            close_date,\n",
    "            random.choice(currencies),\n",
    "            f\"BR{random.randint(100, 999)}\",\n",
    "            random.uniform(0.01, 5.0),\n",
    "            balance,\n",
    "            last_activity_date.date()\n",
    "        ))\n",
    "    \n",
    "    return spark.createDataFrame(accounts, account_schema)\n",
    "\n",
    "# Generate transaction data\n",
    "def generate_transactions(accounts_df, num_transactions=10000):\n",
    "    transaction_types = [\"deposit\", \"withdrawal\", \"transfer\", \"payment\"]\n",
    "    currencies = [\"USD\", \"EUR\", \"GBP\"]\n",
    "    merchant_categories = [\"grocery\", \"restaurant\", \"retail\", \"travel\", \"utility\", \"entertainment\"]\n",
    "    transaction_statuses = [\"completed\", \"pending\", \"failed\", \"reversed\"]\n",
    "    channels = [\"online\", \"mobile\", \"branch\", \"atm\"]\n",
    "    locations = [\"USA\", \"Canada\", \"UK\", \"France\", \"Germany\", \"Japan\", \"Australia\", \"Brazil\", \"Mexico\", \"China\"]\n",
    "    \n",
    "    transactions = []\n",
    "    \n",
    "    # Get active account IDs\n",
    "    account_ids = [row.account_id for row in accounts_df.filter(col(\"account_status\") == \"active\").select(\"account_id\").collect()]\n",
    "    \n",
    "    for i in range(num_transactions):\n",
    "        transaction_id = str(uuid.uuid4())\n",
    "        account_id = random.choice(account_ids)\n",
    "        \n",
    "        # Generate transaction date (last 90 days)\n",
    "        days_ago = random.randint(0, 90)\n",
    "        hours_ago = random.randint(0, 24)\n",
    "        minutes_ago = random.randint(0, 60)\n",
    "        transaction_date = datetime.now() - timedelta(days=days_ago, hours=hours_ago, minutes=minutes_ago)\n",
    "        \n",
    "        transaction_type = random.choice(transaction_types)\n",
    "        \n",
    "        # Amount based on transaction type\n",
    "        if transaction_type == \"deposit\":\n",
    "            amount = random.uniform(10, 5000)\n",
    "        elif transaction_type == \"withdrawal\":\n",
    "            amount = random.uniform(10, 1000)\n",
    "        elif transaction_type == \"transfer\":\n",
    "            amount = random.uniform(10, 3000)\n",
    "        else:  # payment\n",
    "            amount = random.uniform(10, 2000)\n",
    "        \n",
    "        currency = random.choice(currencies)\n",
    "        merchant_category = random.choice(merchant_categories)\n",
    "        \n",
    "        # Generate merchant name based on category\n",
    "        if merchant_category == \"grocery\":\n",
    "            merchant_name = random.choice([\"Whole Foods\", \"Safeway\", \"Kroger\", \"Trader Joe's\"])\n",
    "        elif merchant_category == \"restaurant\":\n",
    "            merchant_name = random.choice([\"McDonald's\", \"Starbucks\", \"Chipotle\", \"Olive Garden\"])\n",
    "        elif merchant_category == \"retail\":\n",
    "            merchant_name = random.choice([\"Amazon\", \"Walmart\", \"Target\", \"Best Buy\"])\n",
    "        elif merchant_category == \"travel\":\n",
    "            merchant_name = random.choice([\"Delta Airlines\", \"Marriott\", \"Expedia\", \"Uber\"])\n",
    "        elif merchant_category == \"utility\":\n",
    "            merchant_name = random.choice([\"AT&T\", \"PG&E\", \"Comcast\", \"Verizon\"])\n",
    "        else:  # entertainment\n",
    "            merchant_name = random.choice([\"Netflix\", \"AMC Theaters\", \"Spotify\", \"Disney+\"])\n",
    "        \n",
    "        location = random.choice(locations)\n",
    "        is_international = location != \"USA\"\n",
    "        \n",
    "        transactions.append((\n",
    "            transaction_id,\n",
    "            account_id,\n",
    "            transaction_date,\n",
    "            transaction_type,\n",
    "            amount,\n",
    "            currency,\n",
    "            f\"{transaction_type.capitalize()} at {merchant_name}\",\n",
    "            merchant_name,\n",
    "            merchant_category,\n",
    "            random.choice(transaction_statuses),\n",
    "            random.choice(channels),\n",
    "            location,\n",
    "            is_international\n",
    "        ))\n",
    "    \n",
    "    return spark.createDataFrame(transactions, transaction_schema)\n",
    "\n",
    "# Generate the data\n",
    "customers_df = generate_customers(1000)\n",
    "accounts_df = generate_accounts(customers_df, 1500)\n",
    "transactions_df = generate_transactions(accounts_df, 10000)\n",
    "\n",
    "# Write data to S3\n",
    "customers_df.write.mode(\"overwrite\").csv(\"s3://banking-data-lake/raw/customers/\", header=True)\n",
    "accounts_df.write.mode(\"overwrite\").csv(\"s3://banking-data-lake/raw/accounts/\", header=True)\n",
    "transactions_df.write.mode(\"overwrite\").csv(\"s3://banking-data-lake/raw/transactions/\", header=True)\n",
    "\n",
    "print(\"Sample data generation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7c34286-7e87-4540-9f30-152b6b83bfe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook: Run ETL Pipeline\n",
    "\n",
    "# Import required modules\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project directory to Python path\n",
    "project_path = \"/dbfs/FileStore/banking-etl-pipeline\"\n",
    "sys.path.append(project_path)\n",
    "\n",
    "# Import the main pipeline class\n",
    "from src.orchestration.main_pipeline import BankingETLPipeline\n",
    "\n",
    "# Set configuration path\n",
    "config_path = os.path.join(project_path, \"config/config.json\")\n",
    "\n",
    "# Initialize and run the pipeline\n",
    "pipeline = BankingETLPipeline(config_path)\n",
    "pipeline.run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ebe4fa8-8e08-42ca-823d-027db5509534",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "4. Monitoring and Troubleshooting\n",
    "Create a notebook for monitoring and troubleshooting:\n",
    "\n",
    "# Databricks notebook: Monitor ETL Pipeline\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Check data quality results\n",
    "def check_data_quality_results():\n",
    "    try:\n",
    "        quality_results = spark.read.format(\"delta\").load(\"s3://banking-data-lake/monitoring/data_quality_results/\")\n",
    "        \n",
    "        # Show the latest results\n",
    "        latest_results = quality_results.orderBy(col(\"execution_date\").desc()).limit(10)\n",
    "        display(latest_results)\n",
    "        \n",
    "        # Show failed checks\n",
    "        failed_checks = quality_results.filter(col(\"overall_passed\") == False)\n",
    "        print(f\"Number of failed quality checks: {failed_checks.count()}\")\n",
    "        display(failed_checks)\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking data quality results: {str(e)}\")\n",
    "\n",
    "# Check pipeline execution logs\n",
    "def check_pipeline_logs():\n",
    "    try:\n",
    "        logs = spark.read.text(\"s3://banking-data-lake/logs/\")\n",
    "        \n",
    "        # Filter for errors\n",
    "        error_logs = logs.filter(col(\"value\").contains(\"ERROR\"))\n",
    "        print(f\"Number of error logs: {error_logs.count()}\")\n",
    "        display(error_logs)\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking pipeline logs: {str(e)}\")\n",
    "\n",
    "# Check data counts\n",
    "def check_data_counts():\n",
    "    try:\n",
    "        # Check customer counts\n",
    "        customer_count = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", \"jdbc:redshift://banking-warehouse.xyz.us-east-1.redshift.amazonaws.com:5439/banking\") \\\n",
    "            .option(\"dbtable\", \"dim_customer\") \\\n",
    "            .option(\"user\", dbutils.secrets.get(\"redshift\", \"username\")) \\\n",
    "            .option(\"password\", dbutils.secrets.get(\"redshift\", \"password\")) \\\n",
    "            .option(\"driver\", \"com.amazon.redshift.jdbc42.Driver\") \\\n",
    "            .load() \\\n",
    "            .count()\n",
    "        \n",
    "        print(f\"Customer count in Redshift: {customer_count}\")\n",
    "        \n",
    "        # Check account counts\n",
    "        account_count = spark.read.format(\"jdbc\") \\\n",
    "            .option(\"url\", \"jdbc:redshift://banking-warehouse.xyz.us-east-1.redshift.amazonaws.com:5439/banking\") \\\n",
    "            .option(\"dbtable\", \"dim_account\") \\\n",
    "            .option(\"user\", dbutils.secrets.get(\"redshift\", \"username\")) \\\n",
    "            .option(\"password\", dbutils.secrets.get(\"redshift\", \"password\")) \\\n",
    "            .option(\"driver\", \"com.amazon.redshift.jdbc42.Driver\") \\\n",
    "            .load() \\\n",
    "            .count()\n",
    "        \n",
    "        print(f\"Account count in Redshift: {account_count}\")\n",
    "        \n",
    "        # Check transaction counts\n",
    "        transaction_count = spark.read.format(\"delta\") \\\n",
    "            .load(\"s3://banking-data-lake/processed/transactions/\") \\\n",
    "            .count()\n",
    "        \n",
    "        print(f\"Transaction count in Delta Lake: {transaction_count}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking data counts: {str(e)}\")\n",
    "\n",
    "# Run the monitoring functions\n",
    "print(\"=== Data Quality Results ===\")\n",
    "check_data_quality_results()\n",
    "\n",
    "print(\"\\n=== Pipeline Logs ===\")\n",
    "check_pipeline_logs()\n",
    "\n",
    "print(\"\\n=== Data Counts ===\")\n",
    "check_data_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57ccd780-a189-4fb1-a566-885a53bb798c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Explanation: This monitoring notebook provides essential visibility into the ETL pipeline’s operation:\n",
    "\n",
    "Data Quality Monitoring:\n",
    "- Reads data quality check results from the monitoring location\n",
    "- Shows the most recent results\n",
    "- Highlights any failed quality checks\n",
    "Log Analysis:\n",
    "- Reads pipeline execution logs\n",
    "- Filters for error messages\n",
    "- Displays count and details of errors\n",
    "Data Reconciliation:\n",
    "- Counts records in target systems (Redshift and Delta Lake)\n",
    "- Helps identify potential data loss or duplication issues\n",
    "- Provides a quick sanity check on pipeline results\n",
    "This monitoring approach is crucial for:\n",
    "\n",
    "Detecting issues early\n",
    "Ensuring data quality\n",
    "Validating successful data processing\n",
    "Troubleshooting pipeline failures\n",
    "For a production environment, these checks could be automated and integrated with alerting systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "896e3148-d98a-4b58-9d5a-8304cce7666b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    ". Scheduling the Pipeline\n",
    "To schedule the ETL pipeline in Databricks:\n",
    "\n",
    "Create a Databricks job:\n",
    "- Go to the Databricks workspace and click on “Jobs”\n",
    "- Click “Create Job”\n",
    "- Add the “Run ETL Pipeline” notebook as a task\n",
    "- Configure the cluster to use\n",
    "- Set up a schedule (e.g., daily at 2 AM)\n",
    "- Configure email notifications for success/failure\n",
    "Set up monitoring alerts:\n",
    "- Add the “Monitor ETL Pipeline” notebook as a separate task\n",
    "- Configure it to run after the main pipeline task\n",
    "- Set up email notifications for critical issues\n",
    "Explanation: Scheduling ensures that our ETL pipeline runs automatically at the appropriate times:\n",
    "\n",
    "Job Configuration:\n",
    "- The main pipeline task processes the data\n",
    "- The monitoring task validates results and checks for issues\n",
    "- Email notifications alert the team to any problems\n",
    "Scheduling Considerations:\n",
    "- Schedule during off-peak hours to minimize impact on source systems\n",
    "- Allow sufficient time for the pipeline to complete before business hours\n",
    "- Consider dependencies on other data processes\n",
    "Advanced Scheduling:\n",
    "- For more complex workflows, consider using Databricks Workflows\n",
    "- Implement retry logic for transient failures\n",
    "- Set up SLA monitoring for timely completion\n",
    "Proper scheduling is essential for maintaining up-to-date data in the banking environment where timely information is critical for decision-making and reporting.\n",
    "\n",
    "Conclusion\n",
    "In this comprehensive guide, we’ve built an end-to-end ETL pipeline for a banking domain using AWS, PySpark, and Databricks. The pipeline includes:\n",
    "\n",
    "Data Ingestion: Reading data from various sources like S3 and RDS\n",
    "Data Transformation: Cleaning, enriching, and validating data\n",
    "Data Quality: Implementing robust data quality checks\n",
    "Data Loading: Loading processed data to Redshift and S3\n",
    "Monitoring and Alerting: Setting up monitoring for the pipeline\n",
    "This architecture provides a scalable, maintainable, and production-ready solution for processing banking data. The modular design allows for easy extension and modification as business requirements evolve.\n",
    "\n",
    "Key benefits of this implementation:\n",
    "\n",
    "Scalability: Leverages Spark’s distributed computing capabilities to handle growing data volumes\n",
    "Reliability: Includes error handling, data quality checks, and monitoring to ensure dependable operation\n",
    "Maintainability: Modular design with clear separation of concerns makes the codebase easier to maintain\n",
    "Security: Implements secure data handling practices essential for sensitive banking information\n",
    "Flexibility: Supports multiple data sources and targets with a configuration-driven approach\n",
    "The pipeline we’ve built addresses common challenges in banking data processing:\n",
    "\n",
    "Handling diverse data types and sources\n",
    "Ensuring data quality and consistency\n",
    "Detecting anomalous transactions\n",
    "Maintaining historical data with proper partitioning\n",
    "Providing a foundation for analytics and reporting\n",
    "By following this guide, you can implement a robust ETL pipeline for your banking data processing needs, enabling advanced analytics, regulatory reporting, and data-driven decision making that are essential in today’s competitive banking industry."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DOMAIN BANK",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
