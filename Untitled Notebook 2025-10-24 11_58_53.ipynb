{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "935ab31c-995d-4735-9508-3ba88505cade",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_id|activity_date|year_month|\n",
    "+-------+-------------+----------+\n",
    "|   1|  2025-08-05|  2025-08| 2025-08-01 2025-09-01\n",
    "|   1|  2025-09-10|  2025-09|\n",
    "|   2|  2025-08-15|  2025-08|\n",
    "|   3|  2025-09-01|  2025-09|\n",
    "|   2|  2025-09-20|  2025-09|\n",
    "|   3|  2025-10-05|  2025-10|\n",
    "|   4|  2025-09-12|  2025-09|\n",
    "|   4|  2025-10-02|  2025-10|\n",
    "|   5|  2025-10-02|  2025-10|\n",
    "|   5|  2025-12-10|  2025-12|\n",
    "+-------+-------------+----------+\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "df=spark.read.csv(\"/Volumes/pyspark_classes/fintec_dev/22-10-2025/activity_df.csv\",\n",
    "header=True,inferSchema=True)\n",
    "\n",
    "#df.orderBy(col(\"user_id\").desc(),col(\"activity_date\").desc()).show()\n",
    "\n",
    "\n",
    "=======================================================\n",
    "\n",
    "df=df.withColumn(\"a_d\",to_date(\"activity_date\",'yyyy-MM-dd')).\\\n",
    "  withColumn(\"month_start\",to_date(concat(date_format(col(\"a_d\"),\"yyyy-MM\"),lit(\"-01\"))))\n",
    "\n",
    "This does 2 things:\n",
    "\n",
    "Converts the text column activity_date → proper date format and stores it as a_d.\n",
    "\n",
    "Extracts the first day of that month (e.g., 2025-09-10 → 2025-09-01) and stores it as month_start.\n",
    "\n",
    "| user_id | activity_date | a_d    | month_start |\n",
    "| ------- | ------------- | ---------- | ----------- |\n",
    "| 1    | 2025-09-10  | 2025-09-10 | 2025-09-01 |\n",
    "| 2    | 2025-08-25  | 2025-08-25 | 2025-08-01 |\n",
    "\n",
    "====================================================================================================\n",
    "\n",
    "df_pre_month=df.select(\"user_id\",add_months(col('month_start'),1).alias('month_start1')).distinct()\n",
    "\n",
    "\n",
    "For each user, it creates a record where month_start1 = month_start + 1 month.\n",
    "(e.g., if user was active in August → month_start1 = September 1st)\n",
    "\n",
    "✅ Example:\n",
    "\n",
    "user_id\tmonth_start1\n",
    "1\t2025-09-01\n",
    "2\t2025-09-01\n",
    "3\t2025-10-01\n",
    "\n",
    "So now, we can later check — if a user active in August was also active in September.\n",
    "\n",
    "=====================================================================================\n",
    "\n",
    "\n",
    "jd=df_pre_month.alias('a').join(df.alias('b'),(col('a.user_id')==col('b.user_id')) \n",
    "& (col('a.month_start1')==col('b.month_start')) ,'left')\n",
    "\n",
    "\n",
    "\n",
    "(col('a.user_id') == col('b.user_id'))\n",
    "\n",
    "➡️ Same user.\n",
    "\n",
    "(col('a.month_start1') == col('b.month_start'))\n",
    " 1|  2025-08-05|  2025-08| 2025-08-01 2025-09-01\n",
    "\n",
    "➡️ Check if their next month (from a) matches a month where they were actually active (from b).\n",
    "\n",
    "So basically:\n",
    "\n",
    "“For each user who was active in month M,\n",
    "did we find an activity record for month M + 1?”\n",
    "\n",
    "⚙️ Example of how the join works\n",
    "a.user_id\ta.month_start1\tb.user_id\tb.month_start\tMatched?\n",
    "1\t2025-09-01\t1\t2025-09-01\t✅ Yes → returned\n",
    "1\t2025-10-01\t1\t2025-09-01\t❌ No\n",
    "2\t2025-09-01\t2\t2025-09-01\t✅ Yes → returned\n",
    "2\t2025-10-01\t2\t2025-09-01\t❌ No\n",
    "3\t2025-10-01\t3\t2025-10-01\t✅ Yes → returned\n",
    "\n",
    "🧮 After join:\n",
    "\n",
    "You now have something like:\n",
    "\n",
    "a.user_id\ta.month_start1\tb.user_id\tb.month_start\n",
    "1\t2025-09-01\t1\t2025-09-01\n",
    "2\t2025-09-01\t2\t2025-09-01\n",
    "3\t2025-10-01\t3\t2025-10-01\n",
    "\n",
    "Now you can:\n",
    "\n",
    "Count all users in column a.user_id → total active in previous month.\n",
    "\n",
    "Count all users in b.user_id (non-null) → how many returned this month.\n",
    "\n",
    "🧠 Why LEFT JOIN?\n",
    "\n",
    "Because we want to keep all previous month users (a) —\n",
    "even if they didn’t come back in the next month.\n",
    "\n",
    "If a user didn’t return, b.user_id will be NULL.\n",
    "That helps when we count returning users (non-null ones only).\n",
    "\n",
    "\n",
    "So after this join, you can easily calculate:\n",
    "\n",
    "Active users (total from previous month)\n",
    "\n",
    "Returning users (those who matched next month)\n",
    "\n",
    "And then retention % = returning / active × 100\n",
    "\n",
    "\n",
    "=======================================================================\n",
    ".groupBy(\"a.month_start1\").agg(count(\"a.user_id\").alias(\"active_user\"),count(\"b.user_id\")\n",
    ".alias(\"return_user\"))\n",
    "\n",
    "For each month:\n",
    "\n",
    "active_user: how many users were active in the previous month.\n",
    "\n",
    "return_user: how many of them came back in this month (found in the join).\n",
    "\n",
    "\n",
    "==================================================================\n",
    ".withColumn(\"per\",(col(\"return_user\")/col(\"active_user\"))*100)\n",
    ".withColumn(\"month\",date_format(col(\"a.month_start1\"),\"MMM-yyyy\")).select('month','per')\n",
    "\n",
    "\n",
    "per: calculates retention rate → (returning / total active) × 100\n",
    "\n",
    "Formats month like Sep-2025 or Oct-2025\n",
    "\n",
    "✅ Final output:\n",
    "\n",
    "month\tper\n",
    "Sep-2025\t100.0\n",
    "Oct-2025  50.\n",
    "\n",
    "------------------------------------------------------\n",
    "Priyanka\n",
    "Unverified\n",
    "11:58 AM\n",
    "SELECT \n",
    "    emp_id,\n",
    "    COUNT(*) AS days\n",
    "FROM grouped\n",
    "GROUP BY emp_id, grp_key\n",
    "HAVING COUNT(*) >= 10\n",
    "ORDER BY emp_id;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3d64553-da79-44a7-bc68-d24403e88ff3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "AB\n",
    "Unverified\n",
    "12:38 PM\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/Volumes/pyspark_classes/fintec_dev/22-10-2025/employee_leaves_1.csv\")\n",
    "\n",
    "windowSpec = Window.partitionBy(\"emp_id\").orderBy(\"leave_date\")\n",
    "\n",
    "df_row=df.withColumn(\"rn\",row_number().over(windowSpec))\n",
    "#df_row.show()\n",
    "\n",
    "to group to identiy consecutive days\n",
    "df_row1=df_row.withColumn(\"group\",date_add(col(\"leave_date\"),-col(\"rn\"))).\n",
    "    groupBy(\"emp_id\",\"group\").agg(count(\"*\").alias(\"con_leave_days\")).\n",
    "    select(\"emp_id\",\"con_leave_days\").filter(col(\"con_leave_days\")>=10)\n",
    "#df_row1=df_row.withColumn(\"group\",date_sub(col(\"leave_date\"),col(\"rn\")))\n",
    "\n",
    "df_row1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76b80721-901b-47a6-9103-925c8b5f9ac4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferSchema\",\"true\").load(\"/Volumes/pyspark_classes/fintec_dev/22-10-2025/employee_leaves_1.csv\")\n",
    "df.createOrReplaceTempView(\"df_tbl\")\n",
    "df1=spark.sql(\"\"\"\n",
    "Select\n",
    "  emp_id,\n",
    "  leave_date,\n",
    "  dateadd(day, -rn, leave_date) as group\n",
    "from\n",
    "  (\n",
    "    Select\n",
    "      *,\n",
    "      row_number() over (partition by emp_id order by leave_date) as rn\n",
    "    from\n",
    "      df_tbl\n",
    "  ) \"\"\")\n",
    "df1.createOrReplaceTempView(\"df_tbl1\")  \n",
    "df1.display()\n",
    "%sql Select emp_id,group,count() from df_tbl1 group by emp_id,group  having count()>=10\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf3da3fd-9562-4ede-b52a-8f3c97bb6ad3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#find wmp who were late for 5 or more consecutive days\n",
    "\n",
    "df1=spark.sql(\"\"\"\n",
    "Select\n",
    "  empoyee_id,\n",
    "  employee_name,date,arrival_time,status,\n",
    "  dateadd(day, -rn, date) as group\n",
    "from\n",
    "  (\n",
    "    Select\n",
    "      *,\n",
    "      row_number() over(partition by employee_id order by date) as rn, \n",
    "      lag(arrival_time,1) over(partition by employee_id order by date) as time_sub\n",
    "    from\n",
    "      df_tbl\n",
    "  ) \"\"\")\n",
    "df1.createOrReplaceTempView(\"df_tbl1\")  \n",
    "df1.display()\n",
    "%sql Select emp_id,group,count(*),from df_tbl1 group by emp_id,group  having count()>=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "073a3cfd-3e7c-498c-b4ed-d60d9e38d971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql select employee_id,late_count from (Select employee_id,group,count(1) as late_count from (Select employee_id,date,dateadd(day,-rn,date) as group from (Select employee_id,date,row_number() over(partition by employee_id order by date) as rn from df_tbl where status='Late')  ) group by employee_id,group having count(1)>=5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-10-24 11_58_53",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
